{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd14014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, FancyArrowPatch, Wedge, Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple, Optional, Dict, Any, List, Callable\n",
    "\n",
    "# Local imports\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "from src.colors import COLORS\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (10, 6),\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'axes.labelsize': 11,\n",
    "    'xtick.labelsize': 10,\n",
    "    'ytick.labelsize': 10,\n",
    "    'legend.fontsize': 10,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': False,\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': 'white'\n",
    "})\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709ffb1",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Introduction ‚Äî What is Hyperscanning?\n",
    "\n",
    "Traditional neuroscience studies one brain at a time. A participant lies in an fMRI scanner alone, or sits with an EEG cap watching stimuli on a screen. This approach has taught us enormous amounts about how individual brains work ‚Äî but it misses something crucial about what makes us human.\n",
    "\n",
    "**Social interaction is a TWO-brain phenomenon.** When you have a conversation, play music together, or collaborate on a task, your brain doesn't operate in isolation ‚Äî it's constantly adapting to, predicting, and coordinating with another brain. \"It takes two to tango,\" as they say, and yet most of neuroscience has been trying to understand dancing by watching one dancer at a time.\n",
    "\n",
    "**Hyperscanning** changes this. The term comes from \"hyper\" (beyond) + \"scanning\" (brain imaging), meaning: going beyond single-brain recordings to simultaneously capture brain activity from two or more interacting individuals.\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "The field began with Montague et al. in 2002, who first demonstrated two-person fMRI while participants played economic games. EEG hyperscanning soon followed, offering practical advantages: better temporal resolution (milliseconds vs seconds), more natural settings, and easier setup for face-to-face interaction.\n",
    "\n",
    "Today, hyperscanning studies use EEG, fNIRS, fMRI, and even MEG. The core question remains beautifully simple:\n",
    "\n",
    "> **Do our brains synchronize during social interaction?**\n",
    "> \n",
    "> And if so ‚Äî what does it mean? What drives it? What are the consequences?\n",
    "\n",
    "This notebook introduces the foundations of hyperscanning. We'll explore why it matters, what challenges it presents, how to structure the data, and how to think about analyzing inter-brain connectivity. By the end, you'll be ready to dive into the specific connectivity metrics in the following notebooks.\n",
    "\n",
    "> üí° **Key insight**: Hyperscanning lets us study the social brain IN social context ‚Äî capturing the dynamic, bidirectional coupling that makes human interaction so rich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ca989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 1: Hyperscanning Concept ‚Äî Two Brains in Interaction\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Draw two stylized heads facing each other\n",
    "def draw_head(ax: plt.Axes, x_center: float, y_center: float, \n",
    "              facing_right: bool, color: str, label: str) -> None:\n",
    "    \"\"\"Draw a stylized head with EEG electrodes.\"\"\"\n",
    "    # Head circle\n",
    "    head = Circle((x_center, y_center), 0.35, color=color, alpha=0.3, zorder=1)\n",
    "    ax.add_patch(head)\n",
    "    head_outline = Circle((x_center, y_center), 0.35, fill=False, \n",
    "                          edgecolor=color, linewidth=2, zorder=2)\n",
    "    ax.add_patch(head_outline)\n",
    "    \n",
    "    # EEG electrodes (small circles on the head)\n",
    "    electrode_positions = [\n",
    "        (0.0, 0.30),   # Fz-like\n",
    "        (0.0, 0.0),    # Cz-like\n",
    "        (0.0, -0.25),  # Pz-like\n",
    "        (-0.20, 0.15), # F3/F4-like\n",
    "        (0.20, 0.15),\n",
    "        (-0.20, -0.10), # C3/C4-like\n",
    "        (0.20, -0.10),\n",
    "    ]\n",
    "    for dx, dy in electrode_positions:\n",
    "        electrode = Circle((x_center + dx, y_center + dy), 0.04, \n",
    "                          color=color, zorder=3)\n",
    "        ax.add_patch(electrode)\n",
    "    \n",
    "    # Nose indicator (triangle pointing in facing direction)\n",
    "    nose_x = x_center + (0.35 if facing_right else -0.35)\n",
    "    nose_dx = 0.1 if facing_right else -0.1\n",
    "    ax.plot([nose_x, nose_x + nose_dx, nose_x], \n",
    "            [y_center + 0.05, y_center, y_center - 0.05],\n",
    "            color=color, linewidth=2, zorder=2)\n",
    "    \n",
    "    # Label\n",
    "    ax.text(x_center, y_center - 0.55, label, ha='center', va='top',\n",
    "            fontsize=14, fontweight='bold', color=color)\n",
    "\n",
    "# Draw two heads\n",
    "draw_head(ax, -0.7, 0, facing_right=True, color=COLORS[\"signal_1\"], label=\"Participant 1\")\n",
    "draw_head(ax, 0.7, 0, facing_right=False, color=COLORS[\"signal_2\"], label=\"Participant 2\")\n",
    "\n",
    "# Draw brain waves emanating from each head\n",
    "t = np.linspace(0, 2 * np.pi, 100)\n",
    "\n",
    "# Waves from P1 (going right)\n",
    "for i, y_offset in enumerate([-0.15, 0, 0.15]):\n",
    "    wave_x = np.linspace(-0.3, 0.0, 50)\n",
    "    wave_y = 0.05 * np.sin(6 * wave_x * np.pi + i) + y_offset\n",
    "    alpha = 0.8 - i * 0.2\n",
    "    ax.plot(wave_x, wave_y, color=COLORS[\"signal_1\"], alpha=alpha, linewidth=2)\n",
    "\n",
    "# Waves from P2 (going left)\n",
    "for i, y_offset in enumerate([-0.15, 0, 0.15]):\n",
    "    wave_x = np.linspace(0.0, 0.3, 50)\n",
    "    wave_y = 0.05 * np.sin(6 * wave_x * np.pi + i + 0.3) + y_offset\n",
    "    alpha = 0.8 - i * 0.2\n",
    "    ax.plot(wave_x, wave_y, color=COLORS[\"signal_2\"], alpha=alpha, linewidth=2)\n",
    "\n",
    "# Central synchronization symbol\n",
    "sync_circle = Circle((0, 0), 0.12, fill=False, edgecolor=COLORS[\"high_sync\"],\n",
    "                      linewidth=3, linestyle='--', zorder=4)\n",
    "ax.add_patch(sync_circle)\n",
    "ax.text(0, 0, \"‚ü∑\", ha='center', va='center', fontsize=20, \n",
    "        color=COLORS[\"high_sync\"], fontweight='bold', zorder=5)\n",
    "\n",
    "# Title and annotations\n",
    "ax.text(0, 0.7, \"Hyperscanning\", ha='center', va='bottom',\n",
    "        fontsize=18, fontweight='bold', color=COLORS[\"text\"])\n",
    "ax.text(0, 0.55, \"Simultaneous brain recording from interacting individuals\",\n",
    "        ha='center', va='bottom', fontsize=11, style='italic', color=COLORS[\"grid\"])\n",
    "\n",
    "# Bottom annotation\n",
    "ax.text(0, -0.75, \"Do their brain signals synchronize?\",\n",
    "        ha='center', va='top', fontsize=12, color=COLORS[\"text\"])\n",
    "\n",
    "ax.set_xlim(-1.5, 1.5)\n",
    "ax.set_ylim(-0.9, 0.9)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 1: Hyperscanning concept\")\n",
    "print(\"  Two participants with EEG caps, brain signals potentially synchronizing\")\n",
    "print(\"  The central question: do brains couple during social interaction?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afe43a",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Why Hyperscanning Matters\n",
    "\n",
    "### The Social Brain Hypothesis\n",
    "\n",
    "Human brains are remarkable for many reasons, but perhaps most remarkable is our social cognition. The \"social brain hypothesis\" proposes that our large brains evolved primarily to handle the complexities of social life ‚Äî keeping track of relationships, predicting others' behavior, cooperating and competing in groups.\n",
    "\n",
    "Yet traditional neuroscience has mostly studied social cognition in isolation:\n",
    "- Watch videos of faces ‚Üí not a real interaction\n",
    "- Imagine social scenarios ‚Üí artificial, no actual partner\n",
    "- Play games against a computer ‚Üí no real stakes of social reputation\n",
    "\n",
    "What's missing is the **real-time, bidirectional, dynamic exchange** that characterizes actual human interaction.\n",
    "\n",
    "### What Hyperscanning Reveals\n",
    "\n",
    "Studies using hyperscanning have discovered fascinating phenomena:\n",
    "\n",
    "- **Inter-brain synchrony correlates with rapport**: People who \"click\" show more synchronized brain activity\n",
    "- **Synchrony predicts cooperation success**: Teams with higher brain coupling perform better\n",
    "- **Leader-follower dynamics are visible**: One brain can \"lead\" another in time\n",
    "- **Shared attention creates shared patterns**: Looking at the same thing together synchronizes brains\n",
    "\n",
    "### Applications Across Domains\n",
    "\n",
    "Hyperscanning has applications far beyond basic research:\n",
    "\n",
    "| Domain | Application | Finding |\n",
    "|--------|-------------|----------|\n",
    "| **Education** | Teacher-student dynamics | Synchrony predicts learning outcomes |\n",
    "| **Clinical** | Therapist-patient rapport | Synchrony relates to therapeutic alliance |\n",
    "| **Development** | Parent-child bonding | Attachment quality visible in brain coupling |\n",
    "| **Performance** | Music, sports, teams | Coordination linked to neural synchrony |\n",
    "| **Communication** | Conversation, storytelling | Speaker-listener coupling during understanding |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac1c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 2: Applications of Hyperscanning\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Application domains with text abbreviations (no emojis for font compatibility)\n",
    "domains = [\n",
    "    {\"name\": \"Education\", \"abbrev\": \"EDU\", \"desc\": \"Teacher-student\\nlearning\"},\n",
    "    {\"name\": \"Clinical\", \"abbrev\": \"CLN\", \"desc\": \"Therapist-patient\\nrapport\"},\n",
    "    {\"name\": \"Development\", \"abbrev\": \"DEV\", \"desc\": \"Parent-child\\nbonding\"},\n",
    "    {\"name\": \"Music\", \"abbrev\": \"MUS\", \"desc\": \"Musical\\ncoordination\"},\n",
    "    {\"name\": \"Sports\", \"abbrev\": \"SPT\", \"desc\": \"Team\\ncoordination\"},\n",
    "    {\"name\": \"Communication\", \"abbrev\": \"COM\", \"desc\": \"Conversation\\nunderstanding\"},\n",
    "]\n",
    "\n",
    "# Define colors for each domain\n",
    "domain_colors = [\n",
    "    COLORS[\"signal_1\"],  # Education\n",
    "    COLORS[\"signal_2\"],  # Clinical\n",
    "    COLORS[\"signal_3\"],  # Development\n",
    "    COLORS[\"signal_4\"],  # Music\n",
    "    COLORS[\"signal_5\"],  # Sports\n",
    "    COLORS[\"signal_6\"],  # Communication\n",
    "]\n",
    "\n",
    "# Draw central hub\n",
    "center_circle = Circle((0, 0), 0.25, color=COLORS[\"high_sync\"], alpha=0.9, zorder=3)\n",
    "ax.add_patch(center_circle)\n",
    "ax.text(0, 0.03, \"Hyper-\", ha='center', va='center', fontsize=12, \n",
    "        fontweight='bold', color='white', zorder=4)\n",
    "ax.text(0, -0.08, \"scanning\", ha='center', va='center', fontsize=12, \n",
    "        fontweight='bold', color='white', zorder=4)\n",
    "\n",
    "# Draw domains around center\n",
    "n_domains = len(domains)\n",
    "radius = 0.7\n",
    "angles = np.linspace(np.pi/2, np.pi/2 - 2*np.pi, n_domains, endpoint=False)\n",
    "\n",
    "for i, (domain, angle, color) in enumerate(zip(domains, angles, domain_colors)):\n",
    "    x = radius * np.cos(angle)\n",
    "    y = radius * np.sin(angle)\n",
    "    \n",
    "    # Domain circle (slightly more opaque)\n",
    "    domain_circle = Circle((x, y), 0.18, color=color, alpha=0.4, zorder=2)\n",
    "    ax.add_patch(domain_circle)\n",
    "    domain_outline = Circle((x, y), 0.18, fill=False, edgecolor=color, \n",
    "                            linewidth=2.5, zorder=2)\n",
    "    ax.add_patch(domain_outline)\n",
    "    \n",
    "    # Connection line to center\n",
    "    ax.plot([0, x * 0.5], [0, y * 0.5], color=color, linewidth=2, \n",
    "            alpha=0.6, zorder=1)\n",
    "    \n",
    "    # Abbreviation text inside circle (instead of emoji)\n",
    "    ax.text(x, y, domain[\"abbrev\"], ha='center', va='center', \n",
    "            fontsize=11, fontweight='bold', color=color, zorder=3)\n",
    "    \n",
    "    # Domain name (outside)\n",
    "    text_radius = radius + 0.28\n",
    "    text_x = text_radius * np.cos(angle)\n",
    "    text_y = text_radius * np.sin(angle)\n",
    "    ax.text(text_x, text_y, domain[\"name\"], ha='center', va='center',\n",
    "            fontsize=13, fontweight='bold', color=color)\n",
    "    \n",
    "    # Description (even further out) - use darker color for readability\n",
    "    desc_radius = radius + 0.45\n",
    "    desc_x = desc_radius * np.cos(angle)\n",
    "    desc_y = desc_radius * np.sin(angle)\n",
    "    ax.text(desc_x, desc_y, domain[\"desc\"], ha='center', va='center',\n",
    "            fontsize=10, color=COLORS[\"text\"], style='italic')\n",
    "\n",
    "ax.set_xlim(-1.4, 1.4)\n",
    "ax.set_ylim(-1.4, 1.4)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_title(\"Hyperscanning Applications Across Domains\", \n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 2: Hyperscanning applications\")\n",
    "print(\"  Research spans education, clinical, developmental, and performance domains\")\n",
    "print(\"  Common thread: understanding brain coupling during social interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cf4f58",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Common Hyperscanning Paradigms\n",
    "\n",
    "Hyperscanning studies use diverse experimental paradigms, each designed to probe different aspects of social interaction. Here are the main categories:\n",
    "\n",
    "### Cooperation Tasks\n",
    "Participants work together toward a shared goal:\n",
    "- **Joint button pressing**: Synchronize actions to hit targets together\n",
    "- **Cooperative games**: Solve puzzles or navigate challenges as a team\n",
    "- **Construction tasks**: Build something together (physical or virtual)\n",
    "\n",
    "*Research question*: Does neural synchrony support successful cooperation?\n",
    "\n",
    "### Communication Tasks\n",
    "Information exchange between participants:\n",
    "- **Conversation**: Structured or free-form dialogue\n",
    "- **Storytelling**: One speaks, one listens\n",
    "- **Teaching-learning**: Knowledge transfer situations\n",
    "\n",
    "*Research question*: How does information flow between brains during communication?\n",
    "\n",
    "### Joint Attention Tasks\n",
    "Sharing focus on the same object or event:\n",
    "- **Shared visual attention**: Looking at the same thing together\n",
    "- **Following gaze**: One directs, one follows\n",
    "- **Object manipulation**: Jointly attending to manipulated objects\n",
    "\n",
    "*Research question*: Does shared attention synchronize brains?\n",
    "\n",
    "### Imitation & Synchronization Tasks\n",
    "Coordinating movements:\n",
    "- **Mirror movements**: Copy each other's gestures\n",
    "- **Musical coordination**: Drumming, singing, playing together\n",
    "- **Dance**: Coordinated movement to music\n",
    "\n",
    "*Research question*: Does behavioral synchrony drive neural synchrony?\n",
    "\n",
    "### Competition Tasks\n",
    "Opposing goals:\n",
    "- **Economic games**: Prisoner's dilemma, ultimatum game\n",
    "- **Strategic games**: Chess, competitive video games\n",
    "- **Sports competition**: Head-to-head physical competition\n",
    "\n",
    "*Research question*: Does competition synchronize or desynchronize brains?\n",
    "\n",
    "### Naturalistic Paradigms\n",
    "Unconstrained interaction:\n",
    "- **Free conversation**: Natural dialogue without script\n",
    "- **Real-world settings**: Cafes, classrooms, living rooms\n",
    "\n",
    "*Research question*: What happens in ecologically valid contexts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5995ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 3: Hyperscanning Paradigms\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "\n",
    "# Define paradigms with simple visual representations\n",
    "paradigms = [\n",
    "    {\"name\": \"Cooperation\", \"desc\": \"Working together\\ntoward shared goal\"},\n",
    "    {\"name\": \"Communication\", \"desc\": \"Speaking and\\nlistening\"},\n",
    "    {\"name\": \"Joint Attention\", \"desc\": \"Looking at the\\nsame thing\"},\n",
    "    {\"name\": \"Imitation\", \"desc\": \"Coordinating\\nmovements\"},\n",
    "    {\"name\": \"Competition\", \"desc\": \"Opposing\\ngoals\"},\n",
    "    {\"name\": \"Naturalistic\", \"desc\": \"Free\\ninteraction\"},\n",
    "]\n",
    "\n",
    "paradigm_colors = [\n",
    "    COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"],\n",
    "    COLORS[\"signal_4\"], COLORS[\"signal_5\"], COLORS[\"signal_6\"],\n",
    "]\n",
    "\n",
    "for idx, (ax, paradigm, color) in enumerate(zip(axes.flat, paradigms, paradigm_colors)):\n",
    "    # Draw two simple heads\n",
    "    head_y = 0.3\n",
    "    \n",
    "    # Left head\n",
    "    left_head = Circle((-0.4, head_y), 0.25, color=COLORS[\"signal_1\"], alpha=0.6)\n",
    "    ax.add_patch(left_head)\n",
    "    \n",
    "    # Right head\n",
    "    right_head = Circle((0.4, head_y), 0.25, color=COLORS[\"signal_2\"], alpha=0.6)\n",
    "    ax.add_patch(right_head)\n",
    "    \n",
    "    # Task-specific visual element\n",
    "    if paradigm[\"name\"] == \"Cooperation\":\n",
    "        # Arrows pointing to same target\n",
    "        ax.annotate(\"\", xy=(0, -0.1), xytext=(-0.3, 0.1),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=color, lw=2))\n",
    "        ax.annotate(\"\", xy=(0, -0.1), xytext=(0.3, 0.1),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=color, lw=2))\n",
    "        target = Circle((0, -0.2), 0.1, color=color, alpha=0.8)\n",
    "        ax.add_patch(target)\n",
    "        \n",
    "    elif paradigm[\"name\"] == \"Communication\":\n",
    "        # Speech waves\n",
    "        for i, offset in enumerate([0.1, 0.2, 0.3]):\n",
    "            wave_x = np.linspace(-0.1 + offset, 0.1 + offset, 20)\n",
    "            wave_y = 0.05 * np.sin(wave_x * 30) + head_y\n",
    "            ax.plot(wave_x, wave_y, color=color, lw=2, alpha=0.8-i*0.2)\n",
    "            \n",
    "    elif paradigm[\"name\"] == \"Joint Attention\":\n",
    "        # Both looking at same object\n",
    "        obj = Circle((0, -0.15), 0.12, color=color, alpha=0.8)\n",
    "        ax.add_patch(obj)\n",
    "        ax.plot([-0.2, 0], [0.15, -0.05], color=COLORS[\"signal_1\"], lw=2, ls='--', alpha=0.6)\n",
    "        ax.plot([0.2, 0], [0.15, -0.05], color=COLORS[\"signal_2\"], lw=2, ls='--', alpha=0.6)\n",
    "        \n",
    "    elif paradigm[\"name\"] == \"Imitation\":\n",
    "        # Mirrored arrows\n",
    "        ax.annotate(\"\", xy=(-0.2, -0.1), xytext=(-0.5, -0.1),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=color, lw=2))\n",
    "        ax.annotate(\"\", xy=(0.5, -0.1), xytext=(0.2, -0.1),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=color, lw=2))\n",
    "        ax.text(0, -0.1, \"‚Üî\", ha='center', va='center', fontsize=20, color=color)\n",
    "        \n",
    "    elif paradigm[\"name\"] == \"Competition\":\n",
    "        # Opposing arrows\n",
    "        ax.annotate(\"\", xy=(0.05, -0.05), xytext=(-0.3, -0.05),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"signal_1\"], lw=2))\n",
    "        ax.annotate(\"\", xy=(-0.05, -0.15), xytext=(0.3, -0.15),\n",
    "                   arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"signal_2\"], lw=2))\n",
    "        ax.text(0, -0.1, \"‚öî\", ha='center', va='center', fontsize=18, color=color)\n",
    "        \n",
    "    elif paradigm[\"name\"] == \"Naturalistic\":\n",
    "        # Free wavy lines between them\n",
    "        x_wave = np.linspace(-0.15, 0.15, 40)\n",
    "        for i in range(3):\n",
    "            y_wave = 0.08 * np.sin(x_wave * 15 + i) + head_y - 0.35 - i * 0.1\n",
    "            ax.plot(x_wave, y_wave, color=color, lw=2, alpha=0.6)\n",
    "    \n",
    "    # Title\n",
    "    ax.set_title(paradigm[\"name\"], fontsize=13, fontweight='bold', color=color, pad=10)\n",
    "    \n",
    "    # Description below\n",
    "    ax.text(0, -0.5, paradigm[\"desc\"], ha='center', va='top', fontsize=9,\n",
    "            color=COLORS[\"grid\"], style='italic')\n",
    "    \n",
    "    ax.set_xlim(-0.8, 0.8)\n",
    "    ax.set_ylim(-0.65, 0.7)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Common Hyperscanning Paradigms\", fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 3: Six categories of hyperscanning paradigms\")\n",
    "print(\"  Each paradigm probes different aspects of social interaction\")\n",
    "print(\"  Choice depends on research question and practical constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea48174",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Inter-Brain Synchrony ‚Äî The Core Concept\n",
    "\n",
    "**Inter-brain synchrony (IBS)** is the central phenomenon of interest in hyperscanning. It refers to the statistical coupling or correlation between brain signals from two different individuals.\n",
    "\n",
    "### What We Mean by Synchrony\n",
    "\n",
    "When we say two brains are \"synchronized,\" we mean their signals show some form of statistical dependency:\n",
    "\n",
    "- **Phase synchrony**: The phases of neural oscillations align across brains\n",
    "- **Amplitude coupling**: Power fluctuations rise and fall together\n",
    "- **Frequency coupling**: Activity in specific bands correlates\n",
    "- **Directed coupling**: One brain's activity predicts the other's\n",
    "\n",
    "### Important Clarification\n",
    "\n",
    "Inter-brain synchrony is **NOT**:\n",
    "- ‚ùå Telepathy or direct brain-to-brain communication\n",
    "- ‚ùå Identical brain activity\n",
    "- ‚ùå Proof that brains are \"connected\"\n",
    "\n",
    "It **IS**:\n",
    "- ‚úÖ Statistical dependency between two brain signals\n",
    "- ‚úÖ Evidence of shared processing or coordinated dynamics\n",
    "- ‚úÖ A correlate of social interaction quality\n",
    "\n",
    "### Connection to What We've Learned\n",
    "\n",
    "Here's the beautiful insight: **everything we learned in the foundations applies!**\n",
    "\n",
    "| Foundation Topic | Application in Hyperscanning |\n",
    "|------------------|------------------------------|\n",
    "| Phase (B02) | Inter-brain phase synchrony |\n",
    "| Amplitude (B03) | Inter-brain amplitude correlation |\n",
    "| Coherence (coming in F01) | Inter-brain coherence |\n",
    "| PLV (coming in G01) | Inter-brain PLV |\n",
    "| Transfer Entropy (D03) | Inter-brain directed influence |\n",
    "\n",
    "The metrics are identical ‚Äî we just apply them BETWEEN brains instead of between channels within one brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000de050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 4: Inter-Brain Synchrony Example\n",
    "# =============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simulated EEG from two participants with CLEAR periods of sync/desync\n",
    "fs = 256  # Hz\n",
    "duration = 6  # seconds (longer for better illustration)\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "n_samples = len(t)\n",
    "\n",
    "# Define synchrony periods explicitly\n",
    "# Format: (start_time, end_time, is_synchronized)\n",
    "sync_periods = [\n",
    "    (0.0, 1.0, False),   # Desync\n",
    "    (1.0, 2.2, True),    # HIGH SYNC\n",
    "    (2.2, 3.3, False),   # Desync\n",
    "    (3.3, 4.8, True),    # HIGH SYNC\n",
    "    (4.8, 6.0, False),   # Desync\n",
    "]\n",
    "\n",
    "# Base alpha frequency\n",
    "freq = 10  # Hz\n",
    "\n",
    "# Generate signals with controlled synchrony\n",
    "signal_p1 = np.zeros(n_samples)\n",
    "signal_p2 = np.zeros(n_samples)\n",
    "\n",
    "for start, end, is_sync in sync_periods:\n",
    "    start_idx = int(start * fs)\n",
    "    end_idx = int(end * fs)\n",
    "    segment_t = t[start_idx:end_idx]\n",
    "    segment_len = end_idx - start_idx\n",
    "    \n",
    "    # Participant 1: always has consistent alpha\n",
    "    phase_1 = 2 * np.pi * freq * segment_t\n",
    "    seg_p1 = np.sin(phase_1) + 0.3 * np.random.randn(segment_len)\n",
    "    \n",
    "    if is_sync:\n",
    "        # SYNCHRONIZED: P2 follows P1 closely (small phase difference)\n",
    "        phase_2 = phase_1 + 0.2  # Small constant phase lag\n",
    "        seg_p2 = np.sin(phase_2) + 0.3 * np.random.randn(segment_len)\n",
    "    else:\n",
    "        # DESYNCHRONIZED: P2 has different/drifting phase\n",
    "        phase_drift = np.cumsum(np.random.randn(segment_len) * 0.15)\n",
    "        phase_2 = 2 * np.pi * (freq + 0.5) * segment_t + phase_drift\n",
    "        seg_p2 = np.sin(phase_2) + 0.4 * np.random.randn(segment_len)\n",
    "    \n",
    "    signal_p1[start_idx:end_idx] = seg_p1\n",
    "    signal_p2[start_idx:end_idx] = seg_p2\n",
    "\n",
    "# Smooth transitions at boundaries\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "signal_p1 = gaussian_filter1d(signal_p1, sigma=3)\n",
    "signal_p2 = gaussian_filter1d(signal_p2, sigma=3)\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 8), height_ratios=[1, 1, 0.8])\n",
    "\n",
    "# Plot participant 1\n",
    "ax1 = axes[0]\n",
    "ax1.plot(t, signal_p1, color=COLORS[\"signal_1\"], linewidth=0.8)\n",
    "ax1.set_ylabel(\"Amplitude (¬µV)\", fontsize=11)\n",
    "ax1.set_title(\"Participant 1 ‚Äî Frontal electrode (Fz)\", fontsize=12, fontweight='bold',\n",
    "              color=COLORS[\"signal_1\"])\n",
    "ax1.set_xlim(0, duration)\n",
    "ax1.set_ylim(-2, 2)\n",
    "ax1.axhline(0, color=COLORS[\"grid\"], linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Plot participant 2\n",
    "ax2 = axes[1]\n",
    "ax2.plot(t, signal_p2, color=COLORS[\"signal_2\"], linewidth=0.8)\n",
    "ax2.set_ylabel(\"Amplitude (¬µV)\", fontsize=11)\n",
    "ax2.set_title(\"Participant 2 ‚Äî Frontal electrode (Fz)\", fontsize=12, fontweight='bold',\n",
    "              color=COLORS[\"signal_2\"])\n",
    "ax2.set_xlim(0, duration)\n",
    "ax2.set_ylim(-2, 2)\n",
    "ax2.axhline(0, color=COLORS[\"grid\"], linewidth=0.5, alpha=0.5)\n",
    "\n",
    "# Bottom panel: compute ACTUAL synchrony using sliding window correlation\n",
    "ax3 = axes[2]\n",
    "\n",
    "window_size = int(0.4 * fs)  # 400 ms window\n",
    "step = int(0.05 * fs)  # 50 ms step (smoother)\n",
    "n_windows = (n_samples - window_size) // step\n",
    "\n",
    "sync_values = []\n",
    "sync_times = []\n",
    "\n",
    "for i in range(n_windows):\n",
    "    start_idx = i * step\n",
    "    end_idx = start_idx + window_size\n",
    "    \n",
    "    # Correlation as synchrony measure\n",
    "    corr = np.corrcoef(signal_p1[start_idx:end_idx], signal_p2[start_idx:end_idx])[0, 1]\n",
    "    sync_values.append(max(0, corr))  # Keep positive values\n",
    "    sync_times.append(t[start_idx + window_size // 2])\n",
    "\n",
    "sync_times = np.array(sync_times)\n",
    "sync_values = np.array(sync_values)\n",
    "\n",
    "# Now highlight regions based on ACTUAL computed synchrony\n",
    "threshold = 0.5\n",
    "high_sync_mask = sync_values > threshold\n",
    "\n",
    "# Find contiguous high-sync regions for highlighting\n",
    "in_high_sync = False\n",
    "highlight_regions = []\n",
    "for i, (time, is_high) in enumerate(zip(sync_times, high_sync_mask)):\n",
    "    if is_high and not in_high_sync:\n",
    "        region_start = time\n",
    "        in_high_sync = True\n",
    "    elif not is_high and in_high_sync:\n",
    "        region_end = sync_times[i-1]\n",
    "        highlight_regions.append((region_start, region_end))\n",
    "        in_high_sync = False\n",
    "if in_high_sync:\n",
    "    highlight_regions.append((region_start, sync_times[-1]))\n",
    "\n",
    "# Apply highlights to signal plots\n",
    "for start, end in highlight_regions:\n",
    "    ax1.axvspan(start, end, alpha=0.25, color=COLORS[\"high_sync\"], zorder=0)\n",
    "    ax2.axvspan(start, end, alpha=0.25, color=COLORS[\"high_sync\"], zorder=0)\n",
    "\n",
    "# Add \"High sync\" labels only on significant regions (> 0.5s duration)\n",
    "significant_regions = [(s, e) for s, e in highlight_regions if (e - s) > 0.5]\n",
    "for i, (start, end) in enumerate(significant_regions[:2]):  # Label first two\n",
    "    mid = (start + end) / 2\n",
    "    ax1.text(mid, 1.7, \"High sync\", fontsize=10, color=COLORS[\"high_sync\"], \n",
    "             fontweight='bold', ha='center')\n",
    "\n",
    "# Plot synchrony curve\n",
    "ax3.fill_between(sync_times, 0, sync_values, alpha=0.3, color=COLORS[\"high_sync\"])\n",
    "ax3.plot(sync_times, sync_values, color=COLORS[\"high_sync\"], linewidth=2)\n",
    "ax3.axhline(threshold, color=COLORS[\"grid\"], linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "ax3.text(duration - 0.1, threshold + 0.05, \"Threshold\", fontsize=9, \n",
    "         color=COLORS[\"grid\"], ha='right')\n",
    "ax3.set_xlabel(\"Time (s)\", fontsize=11)\n",
    "ax3.set_ylabel(\"Synchrony\", fontsize=11)\n",
    "ax3.set_title(\"Inter-Brain Synchrony (sliding window correlation)\", fontsize=12, fontweight='bold')\n",
    "ax3.set_xlim(0, duration)\n",
    "ax3.set_ylim(0, 1)\n",
    "\n",
    "# Shade high sync regions in bottom panel too\n",
    "for start, end in highlight_regions:\n",
    "    ax3.axvspan(start, end, alpha=0.15, color=COLORS[\"high_sync\"], zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 4: Inter-brain synchrony example\")\n",
    "print(\"  Signals are TRULY synchronized in purple regions (in-phase oscillations)\")\n",
    "print(\"  Signals are desynchronized in white regions (different phases)\")\n",
    "print(f\"  Synchrony ranges from {sync_values.min():.2f} to {sync_values.max():.2f}\")\n",
    "print(f\"  High sync periods: {len(highlight_regions)} detected above threshold {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857227a",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Unique Challenges of Hyperscanning\n",
    "\n",
    "Hyperscanning brings unique methodological challenges that don't exist in single-brain studies. Being aware of these challenges is crucial for designing and interpreting studies correctly.\n",
    "\n",
    "### Challenge 1: No Volume Conduction Between Brains ‚úÖ\n",
    "\n",
    "Wait ‚Äî this is actually an **ADVANTAGE**! \n",
    "\n",
    "Remember from notebook C01 how volume conduction creates spurious connectivity within a single brain? Different electrodes pick up the same source, creating fake correlations.\n",
    "\n",
    "In hyperscanning, the two brains are in **separate heads**. There's no shared skull or tissue to conduct signals between them. So any synchrony we observe cannot be an artifact of volume conduction!\n",
    "\n",
    "However, we must still worry about:\n",
    "- Shared electromagnetic interference (e.g., power line noise)\n",
    "- Movement artifacts if participants move together\n",
    "- Common physiological rhythms (breathing, heartbeat)\n",
    "\n",
    "### Challenge 2: Behavioral Confounds\n",
    "\n",
    "People naturally synchronize their behavior during interaction:\n",
    "- They match each other's speech patterns\n",
    "- They mirror movements\n",
    "- They breathe in sync\n",
    "\n",
    "This behavioral synchrony can create neural synchrony that's not truly \"social\":\n",
    "- Synchronized movements ‚Üí synchronized motor artifacts (EMG)\n",
    "- Same visual input ‚Üí similar visual processing\n",
    "- Matched breathing ‚Üí similar slow oscillations\n",
    "\n",
    "**Question**: Is the neural synchrony we observe something BEYOND behavioral synchrony?\n",
    "\n",
    "### Challenge 3: Stimulus-Driven vs. Interaction-Driven Synchrony\n",
    "\n",
    "This is perhaps the most important confound to understand. If both participants:\n",
    "- See the same screen\n",
    "- Hear the same sounds\n",
    "- Experience the same experimental events\n",
    "\n",
    "...then both brains will respond similarly to these shared stimuli. This creates \"synchrony\" that has nothing to do with social interaction ‚Äî it's just both brains processing the same input!\n",
    "\n",
    "**Solution**: Pseudo-pair analysis (we'll cover this in detail later).\n",
    "\n",
    "### Challenge 4: Data Structure Complexity\n",
    "\n",
    "Single-brain connectivity gives us an $n \\times n$ matrix for $n$ channels.\n",
    "\n",
    "Two-brain connectivity gives us:\n",
    "- Within-P1: $n_1 \\times n_1$\n",
    "- Within-P2: $n_2 \\times n_2$  \n",
    "- Between: $n_1 \\times n_2$\n",
    "\n",
    "Or combined: $(n_1 + n_2) \\times (n_1 + n_2)$\n",
    "\n",
    "More channels, more pairs, more multiple comparisons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355abf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 5: The Stimulus-Driven Confound\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left panel: Stimulus-driven synchrony (problematic)\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Draw screen in center\n",
    "screen = Rectangle((-0.15, 0.4), 0.3, 0.25, color=COLORS[\"grid\"], alpha=0.5)\n",
    "ax1.add_patch(screen)\n",
    "ax1.text(0, 0.55, \"Shared\\nStimulus\", ha='center', va='center', fontsize=10, \n",
    "         fontweight='bold', color='white')\n",
    "\n",
    "# Two heads looking at screen\n",
    "head1 = Circle((-0.5, 0), 0.2, color=COLORS[\"signal_1\"], alpha=0.6)\n",
    "ax1.add_patch(head1)\n",
    "ax1.text(-0.5, -0.35, \"P1\", ha='center', fontsize=11, fontweight='bold', \n",
    "         color=COLORS[\"signal_1\"])\n",
    "\n",
    "head2 = Circle((0.5, 0), 0.2, color=COLORS[\"signal_2\"], alpha=0.6)\n",
    "ax1.add_patch(head2)\n",
    "ax1.text(0.5, -0.35, \"P2\", ha='center', fontsize=11, fontweight='bold',\n",
    "         color=COLORS[\"signal_2\"])\n",
    "\n",
    "# Arrows from screen to both heads (stimulus input)\n",
    "ax1.annotate(\"\", xy=(-0.35, 0.15), xytext=(-0.1, 0.42),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"grid\"], lw=2))\n",
    "ax1.annotate(\"\", xy=(0.35, 0.15), xytext=(0.1, 0.42),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"grid\"], lw=2))\n",
    "\n",
    "# \"Synchrony\" between brains (but it's spurious!)\n",
    "ax1.annotate(\"\", xy=(0.25, 0), xytext=(-0.25, 0),\n",
    "            arrowprops=dict(arrowstyle=\"<->\", color=COLORS[\"low_sync\"], \n",
    "                           lw=3, ls='--'))\n",
    "ax1.text(0, -0.08, \"Apparent\\nsynchrony\", ha='center', va='top', fontsize=9,\n",
    "        color=COLORS[\"low_sync\"], style='italic')\n",
    "\n",
    "# Warning sign\n",
    "ax1.text(0, -0.65, \"‚ö† Both brains respond to same stimulus\", ha='center',\n",
    "        fontsize=11, color=COLORS[\"negative\"], fontweight='bold')\n",
    "ax1.text(0, -0.8, \"Not social synchrony ‚Äî just shared input!\", ha='center',\n",
    "        fontsize=10, color=COLORS[\"grid\"], style='italic')\n",
    "\n",
    "ax1.set_xlim(-0.9, 0.9)\n",
    "ax1.set_ylim(-0.9, 0.8)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title(\"Stimulus-Driven Synchrony (Confound)\", fontsize=13, \n",
    "              fontweight='bold', color=COLORS[\"negative\"])\n",
    "\n",
    "# Right panel: True interaction-driven synchrony\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Two heads facing each other (no shared screen)\n",
    "head1 = Circle((-0.4, 0), 0.2, color=COLORS[\"signal_1\"], alpha=0.6)\n",
    "ax2.add_patch(head1)\n",
    "ax2.text(-0.4, -0.35, \"P1\", ha='center', fontsize=11, fontweight='bold',\n",
    "         color=COLORS[\"signal_1\"])\n",
    "\n",
    "head2 = Circle((0.4, 0), 0.2, color=COLORS[\"signal_2\"], alpha=0.6)\n",
    "ax2.add_patch(head2)\n",
    "ax2.text(0.4, -0.35, \"P2\", ha='center', fontsize=11, fontweight='bold',\n",
    "         color=COLORS[\"signal_2\"])\n",
    "\n",
    "# Bidirectional arrows between heads\n",
    "ax2.annotate(\"\", xy=(0.15, 0.05), xytext=(-0.15, 0.05),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"high_sync\"], lw=2))\n",
    "ax2.annotate(\"\", xy=(-0.15, -0.05), xytext=(0.15, -0.05),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"high_sync\"], lw=2))\n",
    "\n",
    "# Synchrony symbol\n",
    "sync_circle = Circle((0, 0), 0.08, fill=False, edgecolor=COLORS[\"high_sync\"],\n",
    "                     linewidth=2, linestyle='-')\n",
    "ax2.add_patch(sync_circle)\n",
    "\n",
    "# Speech bubbles / interaction symbols\n",
    "ax2.text(-0.15, 0.25, \"...\", fontsize=14, color=COLORS[\"signal_1\"], ha='center')\n",
    "ax2.text(0.15, 0.25, \"...\", fontsize=14, color=COLORS[\"signal_2\"], ha='center')\n",
    "\n",
    "# Positive message\n",
    "ax2.text(0, -0.65, \"‚úì Synchrony from actual interaction\", ha='center',\n",
    "        fontsize=11, color=COLORS[\"positive\"], fontweight='bold')\n",
    "ax2.text(0, -0.8, \"Bidirectional coupling through communication\", ha='center',\n",
    "        fontsize=10, color=COLORS[\"grid\"], style='italic')\n",
    "\n",
    "ax2.set_xlim(-0.9, 0.9)\n",
    "ax2.set_ylim(-0.9, 0.8)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"Interaction-Driven Synchrony (Real)\", fontsize=13,\n",
    "              fontweight='bold', color=COLORS[\"positive\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 5: Stimulus-driven vs interaction-driven synchrony\")\n",
    "print(\"  Left: Confound where both participants respond to same stimulus\")\n",
    "print(\"  Right: True social synchrony from bidirectional interaction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c468b2",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Data Structure for Hyperscanning\n",
    "\n",
    "Understanding how to organize hyperscanning data is essential before any analysis. Unlike single-brain studies, we now have **two recordings** that must be aligned and combined properly.\n",
    "\n",
    "### Recording Setup\n",
    "\n",
    "In practice, hyperscanning can be done with:\n",
    "- **Two separate EEG systems** ‚Äî requires careful synchronization via triggers\n",
    "- **One system with dual cap** ‚Äî easier synchronization but limited channel count\n",
    "- **Multiple modalities** ‚Äî EEG + fNIRS, dual-fMRI, etc.\n",
    "\n",
    "The critical requirement: **temporal synchronization**. Both recordings must be aligned to the same time base, typically using shared trigger signals.\n",
    "\n",
    "### Data Dimensions\n",
    "\n",
    "For each participant, we have:\n",
    "- **Participant 1**: shape `(n_channels_P1, n_samples)`\n",
    "- **Participant 2**: shape `(n_channels_P2, n_samples)`\n",
    "\n",
    "Often `n_channels_P1 = n_channels_P2`, but not always (e.g., if one cap has a bad channel).\n",
    "\n",
    "### Connectivity Blocks\n",
    "\n",
    "When computing connectivity for hyperscanning, we get THREE distinct blocks:\n",
    "\n",
    "| Block | Description | Shape | Meaning |\n",
    "|-------|-------------|-------|---------|\n",
    "| **Within-P1** | Connectivity among P1's channels | `(n_P1, n_P1)` | Single-brain connectivity for P1 |\n",
    "| **Within-P2** | Connectivity among P2's channels | `(n_P2, n_P2)` | Single-brain connectivity for P2 |\n",
    "| **Between** | Connectivity from P1 to P2 | `(n_P1, n_P2)` | **Inter-brain connectivity** |\n",
    "\n",
    "The \"Between\" block is what makes hyperscanning unique ‚Äî it captures coupling **across** brains.\n",
    "\n",
    "### Combined Matrix Structure\n",
    "\n",
    "We can combine these blocks into a full connectivity matrix:\n",
    "\n",
    "```\n",
    "                P1 channels    P2 channels\n",
    "P1 channels [   Within-P1   |   Between    ]\n",
    "P2 channels [   Between.T   |   Within-P2  ]\n",
    "```\n",
    "\n",
    "This $(n_{P1} + n_{P2}) \\times (n_{P1} + n_{P2})$ matrix contains all connectivity information.\n",
    "\n",
    "### Channel Naming Convention\n",
    "\n",
    "To avoid confusion, we prefix channel names:\n",
    "- P1 channels: `P1_Fz`, `P1_Cz`, `P1_Pz`, ...\n",
    "- P2 channels: `P2_Fz`, `P2_Cz`, `P2_Pz`, ...\n",
    "\n",
    "This makes it immediately clear which participant each channel belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ce6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 6: Hyperscanning Data Structure\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Panel A: Two separate data arrays\n",
    "ax1 = axes[0]\n",
    "ax1.set_title(\"A. Raw Data Arrays\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "# P1 data array\n",
    "p1_rect = Rectangle((0.1, 0.55), 0.35, 0.35, color=COLORS[\"signal_1\"], alpha=0.6)\n",
    "ax1.add_patch(p1_rect)\n",
    "ax1.text(0.275, 0.725, \"P1 Data\\n(n_ch‚ÇÅ √ó n_samples)\", ha='center', va='center', \n",
    "         fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# P2 data array\n",
    "p2_rect = Rectangle((0.55, 0.55), 0.35, 0.35, color=COLORS[\"signal_2\"], alpha=0.6)\n",
    "ax1.add_patch(p2_rect)\n",
    "ax1.text(0.725, 0.725, \"P2 Data\\n(n_ch‚ÇÇ √ó n_samples)\", ha='center', va='center', \n",
    "         fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Arrow down\n",
    "ax1.annotate(\"\", xy=(0.5, 0.35), xytext=(0.5, 0.5),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"text\"], lw=2))\n",
    "ax1.text(0.5, 0.42, \"Combine\", ha='center', va='center', fontsize=9, color=COLORS[\"text\"])\n",
    "\n",
    "# Combined array\n",
    "combined_rect = Rectangle((0.15, 0.08), 0.7, 0.22, color=COLORS[\"high_sync\"], alpha=0.4)\n",
    "ax1.add_patch(combined_rect)\n",
    "# Show P1 and P2 sections\n",
    "ax1.axvline(x=0.5, ymin=0.08/1.0, ymax=0.3/1.0, color=COLORS[\"grid\"], linestyle='--', lw=1)\n",
    "ax1.text(0.325, 0.19, \"P1\", ha='center', va='center', fontsize=11, \n",
    "         fontweight='bold', color=COLORS[\"signal_1\"])\n",
    "ax1.text(0.675, 0.19, \"P2\", ha='center', va='center', fontsize=11, \n",
    "         fontweight='bold', color=COLORS[\"signal_2\"])\n",
    "ax1.text(0.5, 0.02, \"Combined: (n_ch‚ÇÅ + n_ch‚ÇÇ) √ó n_samples\", ha='center', \n",
    "         fontsize=9, color=COLORS[\"text\"])\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Panel B: Connectivity matrix blocks\n",
    "ax2 = axes[1]\n",
    "ax2.set_title(\"B. Connectivity Matrix Blocks\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "# Draw the 2x2 block structure\n",
    "block_size = 0.35\n",
    "gap = 0.05\n",
    "start_x = 0.15\n",
    "start_y = 0.15\n",
    "\n",
    "# Within-P1 (top-left)\n",
    "within_p1 = Rectangle((start_x, start_y + block_size + gap), block_size, block_size, \n",
    "                       color=COLORS[\"signal_1\"], alpha=0.5)\n",
    "ax2.add_patch(within_p1)\n",
    "ax2.text(start_x + block_size/2, start_y + block_size + gap + block_size/2, \n",
    "         \"Within\\nP1\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Between (top-right)\n",
    "between_rect = Rectangle((start_x + block_size + gap, start_y + block_size + gap), \n",
    "                          block_size, block_size, color=COLORS[\"high_sync\"], alpha=0.6)\n",
    "ax2.add_patch(between_rect)\n",
    "ax2.text(start_x + block_size + gap + block_size/2, start_y + block_size + gap + block_size/2, \n",
    "         \"Between\\nP1‚ÜîP2\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Between.T (bottom-left)\n",
    "between_t_rect = Rectangle((start_x, start_y), block_size, block_size, \n",
    "                            color=COLORS[\"high_sync\"], alpha=0.6)\n",
    "ax2.add_patch(between_t_rect)\n",
    "ax2.text(start_x + block_size/2, start_y + block_size/2, \n",
    "         \"Between\\n(transpose)\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
    "\n",
    "# Within-P2 (bottom-right)\n",
    "within_p2 = Rectangle((start_x + block_size + gap, start_y), block_size, block_size, \n",
    "                       color=COLORS[\"signal_2\"], alpha=0.5)\n",
    "ax2.add_patch(within_p2)\n",
    "ax2.text(start_x + block_size + gap + block_size/2, start_y + block_size/2, \n",
    "         \"Within\\nP2\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Labels\n",
    "ax2.text(start_x + block_size/2, start_y + 2*block_size + gap + 0.08, \"P1 ch\", \n",
    "         ha='center', fontsize=10, color=COLORS[\"signal_1\"], fontweight='bold')\n",
    "ax2.text(start_x + block_size + gap + block_size/2, start_y + 2*block_size + gap + 0.08, \"P2 ch\", \n",
    "         ha='center', fontsize=10, color=COLORS[\"signal_2\"], fontweight='bold')\n",
    "ax2.text(start_x - 0.08, start_y + block_size + gap + block_size/2, \"P1\", \n",
    "         ha='center', va='center', fontsize=10, color=COLORS[\"signal_1\"], fontweight='bold', rotation=90)\n",
    "ax2.text(start_x - 0.08, start_y + block_size/2, \"P2\", \n",
    "         ha='center', va='center', fontsize=10, color=COLORS[\"signal_2\"], fontweight='bold', rotation=90)\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.axis('off')\n",
    "\n",
    "# Panel C: Example with real channel names\n",
    "ax3 = axes[2]\n",
    "ax3.set_title(\"C. Channel Naming Example\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "# Create a small example matrix\n",
    "channels_p1 = [\"P1_Fz\", \"P1_Cz\", \"P1_Pz\"]\n",
    "channels_p2 = [\"P2_Fz\", \"P2_Cz\", \"P2_Pz\"]\n",
    "all_channels = channels_p1 + channels_p2\n",
    "\n",
    "# Create simulated connectivity values\n",
    "np.random.seed(123)\n",
    "n_total = 6\n",
    "matrix = np.random.rand(n_total, n_total) * 0.5 + 0.2\n",
    "matrix = (matrix + matrix.T) / 2  # Make symmetric\n",
    "np.fill_diagonal(matrix, 1.0)\n",
    "\n",
    "# Add higher values for between-brain block to highlight it\n",
    "matrix[0:3, 3:6] = np.random.rand(3, 3) * 0.3 + 0.6\n",
    "matrix[3:6, 0:3] = matrix[0:3, 3:6].T\n",
    "\n",
    "# Plot heatmap\n",
    "im = ax3.imshow(matrix, cmap='RdYlBu_r', vmin=0, vmax=1, aspect='equal')\n",
    "ax3.set_xticks(range(6))\n",
    "ax3.set_yticks(range(6))\n",
    "ax3.set_xticklabels(all_channels, fontsize=8, rotation=45, ha='right')\n",
    "ax3.set_yticklabels(all_channels, fontsize=8)\n",
    "\n",
    "# Add block boundaries\n",
    "ax3.axhline(2.5, color='white', linewidth=3)\n",
    "ax3.axvline(2.5, color='white', linewidth=3)\n",
    "\n",
    "# Add block labels\n",
    "ax3.text(1, -1.2, \"P1\", ha='center', fontsize=10, color=COLORS[\"signal_1\"], fontweight='bold')\n",
    "ax3.text(4, -1.2, \"P2\", ha='center', fontsize=10, color=COLORS[\"signal_2\"], fontweight='bold')\n",
    "ax3.text(-1.5, 1, \"P1\", ha='center', va='center', fontsize=10, color=COLORS[\"signal_1\"], \n",
    "         fontweight='bold', rotation=90)\n",
    "ax3.text(-1.5, 4, \"P2\", ha='center', va='center', fontsize=10, color=COLORS[\"signal_2\"], \n",
    "         fontweight='bold', rotation=90)\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax3, shrink=0.8)\n",
    "cbar.set_label(\"Connectivity\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 6: Hyperscanning data structure\")\n",
    "print(\"  A: Two data arrays combined into one\")\n",
    "print(\"  B: Connectivity matrix has 4 blocks (within-P1, within-P2, between, between.T)\")\n",
    "print(\"  C: Example 6√ó6 matrix with channel naming convention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f03d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Utility Functions: Hyperscanning Data Structure\n",
    "# =============================================================================\n",
    "\n",
    "def create_hyperscanning_data_structure(\n",
    "    data_p1: NDArray[np.float64],\n",
    "    data_p2: NDArray[np.float64],\n",
    "    channel_names_p1: List[str],\n",
    "    channel_names_p2: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a unified data structure for hyperscanning analysis.\n",
    "    \n",
    "    Combines data from two participants into a single structure with\n",
    "    proper channel labeling and metadata.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_p1 : NDArray[np.float64]\n",
    "        EEG data from participant 1, shape (n_channels_p1, n_samples).\n",
    "    data_p2 : NDArray[np.float64]\n",
    "        EEG data from participant 2, shape (n_channels_p2, n_samples).\n",
    "    channel_names_p1 : List[str]\n",
    "        Channel names for participant 1 (without prefix).\n",
    "    channel_names_p2 : List[str]\n",
    "        Channel names for participant 2 (without prefix).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary containing:\n",
    "        - 'data_combined': Combined data array (n_ch_total, n_samples)\n",
    "        - 'channel_names': Channel names with P1_/P2_ prefixes\n",
    "        - 'n_channels_p1': Number of channels for P1\n",
    "        - 'n_channels_p2': Number of channels for P2\n",
    "        - 'participant_labels': Array of 1s and 2s indicating participant\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> data_p1 = np.random.randn(4, 1000)\n",
    "    >>> data_p2 = np.random.randn(4, 1000)\n",
    "    >>> names = ['Fz', 'Cz', 'Pz', 'Oz']\n",
    "    >>> result = create_hyperscanning_data_structure(data_p1, data_p2, names, names)\n",
    "    >>> result['data_combined'].shape\n",
    "    (8, 1000)\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if data_p1.shape[1] != data_p2.shape[1]:\n",
    "        raise ValueError(\"Both participants must have same number of samples\")\n",
    "    if len(channel_names_p1) != data_p1.shape[0]:\n",
    "        raise ValueError(\"channel_names_p1 length must match data_p1 channels\")\n",
    "    if len(channel_names_p2) != data_p2.shape[0]:\n",
    "        raise ValueError(\"channel_names_p2 length must match data_p2 channels\")\n",
    "    \n",
    "    n_ch_p1 = data_p1.shape[0]\n",
    "    n_ch_p2 = data_p2.shape[0]\n",
    "    \n",
    "    # Add prefixes to channel names\n",
    "    names_p1_prefixed = [f\"P1_{name}\" for name in channel_names_p1]\n",
    "    names_p2_prefixed = [f\"P2_{name}\" for name in channel_names_p2]\n",
    "    \n",
    "    # Combine data\n",
    "    data_combined = np.vstack([data_p1, data_p2])\n",
    "    channel_names_combined = names_p1_prefixed + names_p2_prefixed\n",
    "    \n",
    "    # Create participant labels\n",
    "    participant_labels = np.array([1] * n_ch_p1 + [2] * n_ch_p2)\n",
    "    \n",
    "    return {\n",
    "        \"data_combined\": data_combined,\n",
    "        \"channel_names\": channel_names_combined,\n",
    "        \"n_channels_p1\": n_ch_p1,\n",
    "        \"n_channels_p2\": n_ch_p2,\n",
    "        \"participant_labels\": participant_labels\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_connectivity_blocks(\n",
    "    full_matrix: NDArray[np.float64],\n",
    "    n_channels_p1: int\n",
    ") -> Dict[str, NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Extract connectivity blocks from a combined hyperscanning matrix.\n",
    "    \n",
    "    Separates the full (n_total √ó n_total) matrix into within-participant\n",
    "    and between-participant blocks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    full_matrix : NDArray[np.float64]\n",
    "        Full connectivity matrix, shape (n_total, n_total).\n",
    "    n_channels_p1 : int\n",
    "        Number of channels belonging to participant 1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, NDArray[np.float64]]\n",
    "        Dictionary containing:\n",
    "        - 'within_p1': Connectivity within P1 (n_p1, n_p1)\n",
    "        - 'within_p2': Connectivity within P2 (n_p2, n_p2)\n",
    "        - 'between': Inter-brain connectivity (n_p1, n_p2)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> matrix = np.random.rand(8, 8)\n",
    "    >>> blocks = extract_connectivity_blocks(matrix, n_channels_p1=4)\n",
    "    >>> blocks['between'].shape\n",
    "    (4, 4)\n",
    "    \"\"\"\n",
    "    n_total = full_matrix.shape[0]\n",
    "    n_p1 = n_channels_p1\n",
    "    n_p2 = n_total - n_p1\n",
    "    \n",
    "    within_p1 = full_matrix[:n_p1, :n_p1]\n",
    "    within_p2 = full_matrix[n_p1:, n_p1:]\n",
    "    between = full_matrix[:n_p1, n_p1:]\n",
    "    \n",
    "    return {\n",
    "        \"within_p1\": within_p1,\n",
    "        \"within_p2\": within_p2,\n",
    "        \"between\": between\n",
    "    }\n",
    "\n",
    "\n",
    "def combine_connectivity_blocks(\n",
    "    within_p1: NDArray[np.float64],\n",
    "    within_p2: NDArray[np.float64],\n",
    "    between: NDArray[np.float64]\n",
    ") -> NDArray[np.float64]:\n",
    "    \"\"\"\n",
    "    Combine connectivity blocks into a full hyperscanning matrix.\n",
    "    \n",
    "    Assembles within-participant and between-participant blocks into\n",
    "    the complete (n_p1 + n_p2) √ó (n_p1 + n_p2) matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    within_p1 : NDArray[np.float64]\n",
    "        Connectivity within participant 1, shape (n_p1, n_p1).\n",
    "    within_p2 : NDArray[np.float64]\n",
    "        Connectivity within participant 2, shape (n_p2, n_p2).\n",
    "    between : NDArray[np.float64]\n",
    "        Inter-brain connectivity, shape (n_p1, n_p2).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    NDArray[np.float64]\n",
    "        Full connectivity matrix, shape (n_p1 + n_p2, n_p1 + n_p2).\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> w1 = np.eye(3)\n",
    "    >>> w2 = np.eye(3)\n",
    "    >>> b = np.ones((3, 3)) * 0.5\n",
    "    >>> full = combine_connectivity_blocks(w1, w2, b)\n",
    "    >>> full.shape\n",
    "    (6, 6)\n",
    "    \"\"\"\n",
    "    n_p1 = within_p1.shape[0]\n",
    "    n_p2 = within_p2.shape[0]\n",
    "    n_total = n_p1 + n_p2\n",
    "    \n",
    "    full_matrix = np.zeros((n_total, n_total))\n",
    "    \n",
    "    # Fill in blocks\n",
    "    full_matrix[:n_p1, :n_p1] = within_p1\n",
    "    full_matrix[n_p1:, n_p1:] = within_p2\n",
    "    full_matrix[:n_p1, n_p1:] = between\n",
    "    full_matrix[n_p1:, :n_p1] = between.T\n",
    "    \n",
    "    return full_matrix\n",
    "\n",
    "\n",
    "# Demo: Test the functions\n",
    "print(\"Testing hyperscanning data structure functions...\")\n",
    "\n",
    "# Create simulated data\n",
    "np.random.seed(42)\n",
    "fs_demo = 256\n",
    "duration_demo = 2\n",
    "n_samples_demo = fs_demo * duration_demo\n",
    "\n",
    "# Simulated EEG (4 channels each)\n",
    "data_p1_demo = np.random.randn(4, n_samples_demo)\n",
    "data_p2_demo = np.random.randn(4, n_samples_demo)\n",
    "channels_demo = [\"Fz\", \"Cz\", \"Pz\", \"Oz\"]\n",
    "\n",
    "# Create structure\n",
    "hyper_data = create_hyperscanning_data_structure(\n",
    "    data_p1_demo, data_p2_demo, channels_demo, channels_demo\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Combined data shape: {hyper_data['data_combined'].shape}\")\n",
    "print(f\"‚úì Channel names: {hyper_data['channel_names']}\")\n",
    "print(f\"‚úì Participant labels: {hyper_data['participant_labels']}\")\n",
    "\n",
    "# Create a dummy connectivity matrix\n",
    "dummy_matrix = np.random.rand(8, 8)\n",
    "dummy_matrix = (dummy_matrix + dummy_matrix.T) / 2  # Symmetric\n",
    "\n",
    "# Extract blocks\n",
    "blocks = extract_connectivity_blocks(dummy_matrix, n_channels_p1=4)\n",
    "print(f\"\\n‚úì Within-P1 shape: {blocks['within_p1'].shape}\")\n",
    "print(f\"‚úì Within-P2 shape: {blocks['within_p2'].shape}\")\n",
    "print(f\"‚úì Between shape: {blocks['between'].shape}\")\n",
    "\n",
    "# Reconstruct and verify\n",
    "reconstructed = combine_connectivity_blocks(\n",
    "    blocks['within_p1'], blocks['within_p2'], blocks['between']\n",
    ")\n",
    "is_equal = np.allclose(reconstructed, dummy_matrix)\n",
    "print(f\"\\n‚úì Reconstruction matches original: {is_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3c753",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Preprocessing for Hyperscanning\n",
    "\n",
    "All standard EEG preprocessing applies to hyperscanning (filtering, artifact rejection, re-referencing). However, hyperscanning introduces **specific considerations** that don't exist in single-brain studies.\n",
    "\n",
    "### Critical: Time Synchronization\n",
    "\n",
    "**This is THE most important preprocessing step for hyperscanning.**\n",
    "\n",
    "Both recordings must be temporally aligned to the same time base. Without this, all inter-brain connectivity measures are meaningless.\n",
    "\n",
    "Common synchronization methods:\n",
    "- **Shared trigger signals**: Both systems receive the same TTL pulses\n",
    "- **Photodiode markers**: Both record from shared visual stimulus\n",
    "- **Hardware synchronization**: Systems share a clock\n",
    "\n",
    "Always **verify** synchronization before analysis!\n",
    "\n",
    "### Reference Scheme\n",
    "\n",
    "Ideally, use the **same reference scheme** for both participants:\n",
    "- **Average reference**: Most common in hyperscanning\n",
    "- **Linked mastoids**: Also acceptable\n",
    "- **Different references**: Can bias between-brain metrics\n",
    "\n",
    "### Joint Artifact Rejection\n",
    "\n",
    "In single-brain studies, you reject epochs with artifacts. In hyperscanning:\n",
    "\n",
    "> **Reject epochs where EITHER participant has artifacts.**\n",
    "\n",
    "Why? If P1 has clean data but P2 has an eye blink, comparing that epoch is like comparing apples to noise. You may lose more data than in single-subject studies, but the remaining data is valid for between-brain analysis.\n",
    "\n",
    "### Movement Artifacts\n",
    "\n",
    "Social interaction involves movement ‚Äî gestures, speech, facial expressions. This creates:\n",
    "- **EMG artifacts**: Especially in face/jaw muscles\n",
    "- **Electrode movement**: If participants move their heads\n",
    "- **Synchronized artifacts**: If movements are coordinated!\n",
    "\n",
    "Solutions:\n",
    "- Higher high-pass filter (e.g., 1-2 Hz instead of 0.1 Hz)\n",
    "- ICA to remove muscle components\n",
    "- Motion tracking (if available) as covariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 7: Preprocessing Pipeline for Hyperscanning\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Pipeline stages as boxes\n",
    "def draw_box(ax: plt.Axes, x: float, y: float, width: float, height: float,\n",
    "             text: str, color: str, fontsize: int = 10) -> None:\n",
    "    \"\"\"Draw a labeled box.\"\"\"\n",
    "    box = Rectangle((x - width/2, y - height/2), width, height,\n",
    "                    color=color, alpha=0.7, zorder=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=fontsize,\n",
    "            fontweight='bold', color='white' if color != COLORS[\"grid\"] else COLORS[\"text\"],\n",
    "            zorder=3, wrap=True)\n",
    "\n",
    "def draw_arrow(ax: plt.Axes, x1: float, y1: float, x2: float, y2: float,\n",
    "               color: str = None) -> None:\n",
    "    \"\"\"Draw an arrow.\"\"\"\n",
    "    if color is None:\n",
    "        color = COLORS[\"text\"]\n",
    "    ax.annotate(\"\", xy=(x2, y2), xytext=(x1, y1),\n",
    "               arrowprops=dict(arrowstyle=\"->\", color=color, lw=2))\n",
    "\n",
    "# Layout parameters\n",
    "box_w = 0.22\n",
    "box_h = 0.12\n",
    "y_p1 = 0.75\n",
    "y_p2 = 0.45\n",
    "y_combined = 0.15\n",
    "\n",
    "# Title\n",
    "ax.text(0.5, 0.95, \"Hyperscanning Preprocessing Pipeline\", ha='center',\n",
    "        fontsize=14, fontweight='bold', color=COLORS[\"text\"])\n",
    "\n",
    "# Stage 1: Raw data (two parallel streams)\n",
    "ax.text(0.1, 0.9, \"Participant 1\", ha='center', fontsize=11, \n",
    "        fontweight='bold', color=COLORS[\"signal_1\"])\n",
    "draw_box(ax, 0.1, y_p1, box_w, box_h, \"Raw EEG\\n(P1)\", COLORS[\"signal_1\"])\n",
    "\n",
    "ax.text(0.1, y_p2 + 0.17, \"Participant 2\", ha='center', fontsize=11,\n",
    "        fontweight='bold', color=COLORS[\"signal_2\"])\n",
    "draw_box(ax, 0.1, y_p2, box_w, box_h, \"Raw EEG\\n(P2)\", COLORS[\"signal_2\"])\n",
    "\n",
    "# Stage 2: Filtering (parallel)\n",
    "draw_arrow(ax, 0.22, y_p1, 0.28, y_p1, COLORS[\"signal_1\"])\n",
    "draw_box(ax, 0.38, y_p1, box_w, box_h, \"Filter\\n(1-40 Hz)\", COLORS[\"signal_1\"])\n",
    "\n",
    "draw_arrow(ax, 0.22, y_p2, 0.28, y_p2, COLORS[\"signal_2\"])\n",
    "draw_box(ax, 0.38, y_p2, box_w, box_h, \"Filter\\n(1-40 Hz)\", COLORS[\"signal_2\"])\n",
    "\n",
    "# Stage 3: Artifact detection (parallel)\n",
    "draw_arrow(ax, 0.50, y_p1, 0.56, y_p1, COLORS[\"signal_1\"])\n",
    "draw_box(ax, 0.66, y_p1, box_w, box_h, \"Detect\\nArtifacts\", COLORS[\"signal_1\"])\n",
    "\n",
    "draw_arrow(ax, 0.50, y_p2, 0.56, y_p2, COLORS[\"signal_2\"])\n",
    "draw_box(ax, 0.66, y_p2, box_w, box_h, \"Detect\\nArtifacts\", COLORS[\"signal_2\"])\n",
    "\n",
    "# Convergence: Synchronization check\n",
    "ax.text(0.88, 0.9, \"CRITICAL\", ha='center', fontsize=10, \n",
    "        fontweight='bold', color=COLORS[\"negative\"])\n",
    "draw_arrow(ax, 0.78, y_p1, 0.82, (y_p1 + y_p2)/2 + 0.05, COLORS[\"high_sync\"])\n",
    "draw_arrow(ax, 0.78, y_p2, 0.82, (y_p1 + y_p2)/2 - 0.05, COLORS[\"high_sync\"])\n",
    "draw_box(ax, 0.88, (y_p1 + y_p2)/2, box_w, box_h * 1.3, \"Verify\\nSynchronization\", \n",
    "         COLORS[\"high_sync\"])\n",
    "\n",
    "# Convergence arrow down\n",
    "draw_arrow(ax, 0.88, (y_p1 + y_p2)/2 - 0.08, 0.66, y_combined + 0.08, COLORS[\"high_sync\"])\n",
    "\n",
    "# Joint artifact rejection\n",
    "draw_box(ax, 0.66, y_combined, box_w * 1.2, box_h, \"Joint Artifact\\nRejection\", COLORS[\"negative\"])\n",
    "ax.text(0.66, y_combined - 0.1, \"Reject if EITHER\\nparticipant has artifact\",\n",
    "        ha='center', fontsize=8, style='italic', color=COLORS[\"text\"])\n",
    "\n",
    "# Re-reference\n",
    "draw_arrow(ax, 0.54, y_combined, 0.46, y_combined, COLORS[\"text\"])\n",
    "draw_box(ax, 0.38, y_combined, box_w, box_h, \"Re-reference\\n(Average)\", COLORS[\"grid\"])\n",
    "\n",
    "# Clean data output\n",
    "draw_arrow(ax, 0.26, y_combined, 0.18, y_combined, COLORS[\"text\"])\n",
    "draw_box(ax, 0.1, y_combined, box_w, box_h, \"Clean\\nSynchronized\\nData\", COLORS[\"positive\"])\n",
    "\n",
    "# Legend for critical steps\n",
    "ax.text(0.03, 0.05, \"Key: \", fontsize=9, fontweight='bold', color=COLORS[\"text\"])\n",
    "critical_marker = Rectangle((0.08, 0.04), 0.04, 0.025, color=COLORS[\"high_sync\"])\n",
    "ax.add_patch(critical_marker)\n",
    "ax.text(0.13, 0.05, \"Hyperscanning-specific step\", fontsize=8, va='center', color=COLORS[\"text\"])\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 7: Hyperscanning preprocessing pipeline\")\n",
    "print(\"  Two parallel streams converge at synchronization check\")\n",
    "print(\"  Joint artifact rejection: reject if EITHER participant has artifact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752c4c1",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: Choosing Connectivity Metrics\n",
    "\n",
    "We'll cover many connectivity metrics in the upcoming notebooks. How do you choose the right one for your research question?\n",
    "\n",
    "### Decision Framework\n",
    "\n",
    "| Research Question | Metric Type | Examples |\n",
    "|-------------------|-------------|----------|\n",
    "| Are phases aligned between brains? | Phase-based | PLV, PLI, wPLI |\n",
    "| Do power fluctuations co-vary? | Amplitude-based | CCorr, PowCorr |\n",
    "| Linear coupling at specific frequencies? | Coherence-based | COH, ImCoh |\n",
    "| Any statistical dependency? | Information-theoretic | MI |\n",
    "| Who influences whom (direction)? | Directed | Transfer Entropy, Granger |\n",
    "\n",
    "### For Hyperscanning Specifically\n",
    "\n",
    "**Phase Locking Value (PLV)**: Most commonly used in hyperscanning. Intuitive interpretation: \"how often are the phases aligned?\" Easy to compute and understand. *Caveat*: sensitive to volume conduction, but this is less of an issue between brains.\n",
    "\n",
    "**Coherence (COH)**: Classic measure combining phase and amplitude. Well-established in the field. Good for exploratory analyses.\n",
    "\n",
    "**Imaginary Coherence (ImCoh)**: Zero-lag robust. Even though volume conduction isn't an issue between brains, it's good practice and handles other zero-lag confounds.\n",
    "\n",
    "**Phase Lag Index (PLI/wPLI)**: Most robust to artifacts. Conservative choice. May miss genuine zero-lag synchrony.\n",
    "\n",
    "**Amplitude Correlation (CCorr)**: Captures a different mechanism than phase. Complementary to PLV. Less studied but equally valid.\n",
    "\n",
    "**Transfer Entropy (TE)**: For directional questions: \"Does brain A lead brain B?\" More computationally intensive but answers unique questions.\n",
    "\n",
    "### Frequency Band Considerations\n",
    "\n",
    "Different frequency bands may show different effects:\n",
    "- **Theta (4-8 Hz)**: Often linked to memory, coordination\n",
    "- **Alpha (8-13 Hz)**: Attention, inhibition, widespread effects\n",
    "- **Beta (13-30 Hz)**: Motor, social cognition\n",
    "- **Gamma (30+ Hz)**: Local processing, harder to measure\n",
    "\n",
    "**Recommendation**: Either have a *hypothesis* for a specific band, or report multiple bands (with appropriate multiple comparisons correction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 8: Metric Selection Decision Tree (Simple Flowchart Style)\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Helper functions\n",
    "def draw_decision(ax: plt.Axes, x: float, y: float, text: str, color: str,\n",
    "                  width: float = 0.14, height: float = 0.08) -> None:\n",
    "    \"\"\"Draw a diamond-shaped decision node.\"\"\"\n",
    "    hw, hh = width/2, height/2\n",
    "    diamond = plt.Polygon([(x, y + hh), (x + hw, y), (x, y - hh), (x - hw, y)],\n",
    "                         color=color, alpha=0.4, edgecolor=color, linewidth=2, zorder=2)\n",
    "    ax.add_patch(diamond)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, \n",
    "            fontweight='bold', color=color, zorder=3)\n",
    "\n",
    "def draw_box(ax: plt.Axes, x: float, y: float, text: str, color: str,\n",
    "             width: float = 0.11, height: float = 0.055) -> None:\n",
    "    \"\"\"Draw a rectangular metric box.\"\"\"\n",
    "    box = Rectangle((x - width/2, y - height/2), width, height, \n",
    "                    color=color, alpha=0.8, zorder=2)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x, y, text, ha='center', va='center', fontsize=9, \n",
    "            fontweight='bold', color='white', zorder=3)\n",
    "\n",
    "# Orthogonal connector: vertical down, then horizontal, then vertical down\n",
    "def connect_down_branch(ax: plt.Axes, x1: float, y1: float, x2: float, y2: float,\n",
    "                        label: str = None, label_pos: str = \"left\") -> None:\n",
    "    \"\"\"Draw L-shaped connector: down from x1, then horizontal to x2, then down to y2.\"\"\"\n",
    "    mid_y = (y1 + y2) / 2\n",
    "    # Vertical segment from start\n",
    "    ax.plot([x1, x1], [y1, mid_y], color=COLORS[\"text\"], lw=1.5, zorder=1)\n",
    "    # Horizontal segment\n",
    "    ax.plot([x1, x2], [mid_y, mid_y], color=COLORS[\"text\"], lw=1.5, zorder=1)\n",
    "    # Vertical segment to end with arrow\n",
    "    ax.annotate(\"\", xy=(x2, y2), xytext=(x2, mid_y),\n",
    "               arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"text\"], lw=1.5))\n",
    "    # Label\n",
    "    if label:\n",
    "        offset = -0.02 if label_pos == \"left\" else 0.02\n",
    "        ha = \"right\" if label_pos == \"left\" else \"left\"\n",
    "        ax.text(x1 + offset, mid_y + 0.02, label, fontsize=9, ha=ha, \n",
    "                color=COLORS[\"text\"], fontweight='bold')\n",
    "\n",
    "# Simple vertical connector\n",
    "def connect_down(ax: plt.Axes, x: float, y1: float, y2: float, label: str = None) -> None:\n",
    "    \"\"\"Draw simple vertical arrow.\"\"\"\n",
    "    ax.annotate(\"\", xy=(x, y2), xytext=(x, y1),\n",
    "               arrowprops=dict(arrowstyle=\"->\", color=COLORS[\"text\"], lw=1.5))\n",
    "    if label:\n",
    "        ax.text(x + 0.02, (y1 + y2)/2, label, fontsize=9, ha='left', \n",
    "                color=COLORS[\"text\"], fontweight='bold')\n",
    "\n",
    "# Title\n",
    "ax.text(0.5, 0.97, \"Metric Selection Decision Tree\", ha='center',\n",
    "        fontsize=14, fontweight='bold', color=COLORS[\"text\"])\n",
    "\n",
    "# Layout - Y levels\n",
    "L1 = 0.85  # Direction?\n",
    "L2 = 0.68  # Directed metrics / Phase or Amplitude?\n",
    "L3 = 0.51  # Amplitude metrics / Robustness?\n",
    "L4 = 0.34  # Robust metrics / Include amplitude?\n",
    "L5 = 0.17  # PLV / Coherence\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LEVEL 1: Direction matters?\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "draw_decision(ax, 0.5, L1, \"Direction\\nmatters?\", COLORS[\"signal_4\"])\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LEVEL 2: Yes -> Directed | No -> Phase/Amplitude?\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# YES branch (left)\n",
    "connect_down_branch(ax, 0.5, L1 - 0.04, 0.20, L2 + 0.04, \"Yes\", \"left\")\n",
    "\n",
    "draw_box(ax, 0.13, L2, \"Transfer\\nEntropy\", COLORS[\"signal_4\"])\n",
    "draw_box(ax, 0.27, L2, \"Granger\\nCausality\", COLORS[\"signal_4\"])\n",
    "ax.text(0.20, L2 - 0.045, \"Directed\", ha='center', fontsize=9, \n",
    "        style='italic', color=COLORS[\"signal_4\"])\n",
    "\n",
    "# NO branch (right)\n",
    "connect_down_branch(ax, 0.5, L1 - 0.04, 0.70, L2 + 0.04, \"No\", \"right\")\n",
    "\n",
    "draw_decision(ax, 0.70, L2, \"Phase or\\nAmplitude?\", COLORS[\"signal_3\"])\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LEVEL 3: Amplitude -> metrics | Phase -> Robustness?\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# AMPLITUDE branch (right-right)\n",
    "connect_down_branch(ax, 0.70, L2 - 0.04, 0.88, L3 + 0.035, \"Amplitude\", \"right\")\n",
    "\n",
    "draw_box(ax, 0.82, L3, \"CCorr\", COLORS[\"signal_5\"])\n",
    "draw_box(ax, 0.95, L3, \"PowCorr\", COLORS[\"signal_5\"])\n",
    "ax.text(0.885, L3 - 0.045, \"Amplitude-based\", ha='center', fontsize=9,\n",
    "        style='italic', color=COLORS[\"signal_5\"])\n",
    "\n",
    "# PHASE branch (down-left from Phase/Amplitude)\n",
    "connect_down_branch(ax, 0.70, L2 - 0.04, 0.50, L3 + 0.04, \"Phase\", \"left\")\n",
    "\n",
    "draw_decision(ax, 0.50, L3, \"Robustness\\nneeded?\", COLORS[\"signal_1\"])\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LEVEL 4: High -> robust metrics | Standard -> Include amplitude?\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# HIGH robustness (left)\n",
    "connect_down_branch(ax, 0.50, L3 - 0.04, 0.25, L4 + 0.035, \"High\", \"left\")\n",
    "\n",
    "draw_box(ax, 0.18, L4, \"wPLI\", COLORS[\"signal_6\"])\n",
    "draw_box(ax, 0.32, L4, \"PLI\", COLORS[\"signal_6\"])\n",
    "ax.text(0.25, L4 - 0.045, \"Robust to artifacts\", ha='center', fontsize=9,\n",
    "        style='italic', color=COLORS[\"signal_6\"])\n",
    "\n",
    "# STANDARD (right)\n",
    "connect_down_branch(ax, 0.50, L3 - 0.04, 0.70, L4 + 0.04, \"Standard\", \"right\")\n",
    "\n",
    "draw_decision(ax, 0.70, L4, \"Include\\namplitude?\", COLORS[\"signal_2\"])\n",
    "\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# LEVEL 5: No -> PLV | Yes -> Coherence\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "# NO -> PLV (left)\n",
    "connect_down_branch(ax, 0.70, L4 - 0.04, 0.55, L5 + 0.035, \"No\", \"left\")\n",
    "\n",
    "draw_box(ax, 0.55, L5, \"PLV\", COLORS[\"signal_1\"])\n",
    "ax.text(0.55, L5 - 0.045, \"Most common\", ha='center', fontsize=8,\n",
    "        style='italic', color=COLORS[\"signal_1\"])\n",
    "\n",
    "# YES -> Coherence (right)\n",
    "connect_down_branch(ax, 0.70, L4 - 0.04, 0.85, L5 + 0.035, \"Yes\", \"right\")\n",
    "\n",
    "draw_box(ax, 0.78, L5, \"COH\", COLORS[\"signal_2\"])\n",
    "draw_box(ax, 0.92, L5, \"ImCoh\", COLORS[\"signal_2\"])\n",
    "ax.text(0.85, L5 - 0.045, \"Coherence-based\", ha='center', fontsize=8,\n",
    "        style='italic', color=COLORS[\"signal_2\"])\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0.05, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 8: Metric selection decision tree\")\n",
    "print(\"  Orthogonal flowchart style - cleaner and easier to follow\")\n",
    "print(\"  Each decision leads to appropriate metric choices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380e41d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Pseudo-Pair Analysis ‚Äî The Key Control\n",
    "\n",
    "This is perhaps the most important methodological concept in hyperscanning. If you remember only one thing from this notebook, let it be this.\n",
    "\n",
    "### The Fundamental Question\n",
    "\n",
    "When we observe inter-brain synchrony, we must ask:\n",
    "\n",
    "> **Is this synchrony due to the INTERACTION, or would it occur anyway?**\n",
    "\n",
    "### What Are Pseudo-Pairs?\n",
    "\n",
    "A **pseudo-pair** is created by pairing participants who **never actually interacted**:\n",
    "- Take P1 from real pair A\n",
    "- Take P2 from real pair B\n",
    "- Compute \"synchrony\" between them\n",
    "\n",
    "These pseudo-pairs experienced the **same experimental conditions** (same task, same stimuli, same environment) but had **no social interaction** with each other.\n",
    "\n",
    "### The Logic\n",
    "\n",
    "If synchrony is driven by **shared stimuli** (both see the same screen, hear the same sounds), then pseudo-pairs should show similar synchrony to real pairs ‚Äî both are processing the same input.\n",
    "\n",
    "If synchrony is driven by **actual interaction** (conversation, coordination, rapport), then real pairs should show **higher synchrony** than pseudo-pairs ‚Äî only real pairs had the interactive component.\n",
    "\n",
    "### Statistical Test\n",
    "\n",
    "1. Compute synchrony for all **real pairs** ‚Üí distribution of real synchrony values\n",
    "2. Compute synchrony for all **pseudo-pairs** ‚Üí null distribution\n",
    "3. Test: Is real synchrony significantly greater than pseudo-pair synchrony?\n",
    "\n",
    "If **real > pseudo**: ‚úÖ Synchrony is interaction-specific!\n",
    "If **real ‚âà pseudo**: ‚ö†Ô∏è Synchrony may just be shared stimulus response.\n",
    "If **real < pseudo**: ü§î Interesting! Interaction might actually *reduce* synchrony.\n",
    "\n",
    "### Implementation Notes\n",
    "\n",
    "- With $N$ participants in $N/2$ real pairs, you have many possible pseudo-pairs\n",
    "- Can use all pseudo-pairs or subsample\n",
    "- Test at the group level (are real pairs generally higher than pseudo?)\n",
    "- Can also test individual pairs against the pseudo-pair null distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed09329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 9: Pseudo-Pair Concept\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left panel: Real pairs\n",
    "ax1 = axes[0]\n",
    "ax1.set_title(\"Real Pairs (Interacted)\", fontsize=13, fontweight='bold', \n",
    "              color=COLORS[\"positive\"], pad=15)\n",
    "\n",
    "# Draw 3 real pairs\n",
    "pair_colors = [COLORS[\"signal_1\"], COLORS[\"signal_3\"], COLORS[\"signal_5\"]]\n",
    "y_positions = [0.75, 0.45, 0.15]\n",
    "\n",
    "for i, (y, color) in enumerate(zip(y_positions, pair_colors)):\n",
    "    # Left person\n",
    "    head_l = Circle((0.25, y), 0.08, color=color, alpha=0.7)\n",
    "    ax1.add_patch(head_l)\n",
    "    ax1.text(0.25, y - 0.13, f\"P{2*i+1}\", ha='center', fontsize=10, \n",
    "             fontweight='bold', color=color)\n",
    "    \n",
    "    # Right person\n",
    "    head_r = Circle((0.75, y), 0.08, color=color, alpha=0.7)\n",
    "    ax1.add_patch(head_r)\n",
    "    ax1.text(0.75, y - 0.13, f\"P{2*i+2}\", ha='center', fontsize=10, \n",
    "             fontweight='bold', color=color)\n",
    "    \n",
    "    # Interaction arrow (double-headed, solid)\n",
    "    ax1.annotate(\"\", xy=(0.65, y), xytext=(0.35, y),\n",
    "                arrowprops=dict(arrowstyle=\"<->\", color=COLORS[\"high_sync\"], lw=2))\n",
    "    \n",
    "    # Pair label\n",
    "    ax1.text(0.5, y + 0.12, f\"Pair {chr(65+i)}\", ha='center', fontsize=9, \n",
    "             style='italic', color=COLORS[\"text\"])\n",
    "\n",
    "# Legend\n",
    "ax1.text(0.5, -0.05, \"‚Üî = Actual interaction during experiment\", ha='center',\n",
    "        fontsize=10, color=COLORS[\"high_sync\"], fontweight='bold')\n",
    "\n",
    "ax1.set_xlim(0, 1)\n",
    "ax1.set_ylim(-0.1, 1)\n",
    "ax1.axis('off')\n",
    "\n",
    "# Right panel: Pseudo-pairs\n",
    "ax2 = axes[1]\n",
    "ax2.set_title(\"Pseudo-Pairs (Never Interacted)\", fontsize=13, fontweight='bold',\n",
    "              color=COLORS[\"negative\"], pad=15)\n",
    "\n",
    "# Draw pseudo-pairs: mix participants from different real pairs\n",
    "pseudo_pairs = [(0, 3), (1, 4), (2, 5)]  # P1 with P4, P2 with P5, P3 with P6\n",
    "pseudo_y = [0.75, 0.45, 0.15]\n",
    "left_colors = [COLORS[\"signal_1\"], COLORS[\"signal_1\"], COLORS[\"signal_3\"]]\n",
    "right_colors = [COLORS[\"signal_3\"], COLORS[\"signal_5\"], COLORS[\"signal_5\"]]\n",
    "\n",
    "for i, (y, l_color, r_color) in enumerate(zip(pseudo_y, left_colors, right_colors)):\n",
    "    p_left, p_right = pseudo_pairs[i]\n",
    "    \n",
    "    # Left person\n",
    "    head_l = Circle((0.25, y), 0.08, color=l_color, alpha=0.7)\n",
    "    ax2.add_patch(head_l)\n",
    "    ax2.text(0.25, y - 0.13, f\"P{p_left+1}\", ha='center', fontsize=10, \n",
    "             fontweight='bold', color=l_color)\n",
    "    \n",
    "    # Right person\n",
    "    head_r = Circle((0.75, y), 0.08, color=r_color, alpha=0.7)\n",
    "    ax2.add_patch(head_r)\n",
    "    ax2.text(0.75, y - 0.13, f\"P{p_right+1}\", ha='center', fontsize=10, \n",
    "             fontweight='bold', color=r_color)\n",
    "    \n",
    "    # NO interaction (dashed, crossed out)\n",
    "    ax2.plot([0.35, 0.65], [y, y], color=COLORS[\"low_sync\"], lw=2, ls='--')\n",
    "    # Cross mark\n",
    "    ax2.plot([0.48, 0.52], [y + 0.03, y - 0.03], color=COLORS[\"negative\"], lw=2)\n",
    "    ax2.plot([0.48, 0.52], [y - 0.03, y + 0.03], color=COLORS[\"negative\"], lw=2)\n",
    "    \n",
    "    # Pseudo-pair label\n",
    "    ax2.text(0.5, y + 0.12, f\"Pseudo {i+1}\", ha='center', fontsize=9,\n",
    "             style='italic', color=COLORS[\"grid\"])\n",
    "\n",
    "# Legend\n",
    "ax2.text(0.5, -0.05, \"‚úó = Same experiment, but never interacted\", ha='center',\n",
    "        fontsize=10, color=COLORS[\"negative\"], fontweight='bold')\n",
    "\n",
    "ax2.set_xlim(0, 1)\n",
    "ax2.set_ylim(-0.1, 1)\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 9: Real pairs vs pseudo-pairs\")\n",
    "print(\"  Real pairs: participants who actually interacted during the experiment\")\n",
    "print(\"  Pseudo-pairs: participants from different real pairs (never interacted)\")\n",
    "print(\"  If synchrony is interaction-specific, real pairs > pseudo-pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7894f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 10: Pseudo-Pair Null Distribution\n",
    "# =============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate synchrony values\n",
    "# Real pairs: higher synchrony (interaction effect)\n",
    "real_pair_sync = np.random.normal(0.55, 0.08, 15)  # 15 real pairs\n",
    "real_pair_sync = np.clip(real_pair_sync, 0, 1)\n",
    "\n",
    "# Pseudo-pairs: lower synchrony (no interaction)\n",
    "pseudo_pair_sync = np.random.normal(0.35, 0.10, 100)  # Many more pseudo-pairs\n",
    "pseudo_pair_sync = np.clip(pseudo_pair_sync, 0, 1)\n",
    "\n",
    "# Color scheme with good contrast:\n",
    "# - Pseudo-pairs: Sky blue (neutral, clear)\n",
    "# - Real pairs: Purple (high sync, stands out)\n",
    "# - Conclusion text: Standard text color\n",
    "pseudo_color = COLORS[\"signal_1\"]  # Sky Blue - neutral null distribution\n",
    "real_color = COLORS[\"high_sync\"]   # Purple - real pairs (high sync)\n",
    "success_color = COLORS[\"text\"]     # Standard text color for conclusion\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left panel: Distributions\n",
    "ax1 = axes[0]\n",
    "\n",
    "# Pseudo-pair histogram (null distribution)\n",
    "ax1.hist(pseudo_pair_sync, bins=20, alpha=0.7, color=pseudo_color,\n",
    "         label=\"Pseudo-pairs (null)\", density=True, edgecolor='white')\n",
    "\n",
    "# Real pairs as vertical lines\n",
    "for i, val in enumerate(real_pair_sync):\n",
    "    ax1.axvline(val, color=real_color, alpha=0.8, linewidth=2.5,\n",
    "               label=\"Real pairs\" if i == 0 else None)\n",
    "\n",
    "# Mean lines\n",
    "ax1.axvline(pseudo_pair_sync.mean(), color=pseudo_color, linestyle='--', \n",
    "            linewidth=2.5, label=f\"Pseudo mean: {pseudo_pair_sync.mean():.2f}\")\n",
    "ax1.axvline(real_pair_sync.mean(), color=real_color, linestyle='--',\n",
    "            linewidth=2.5, label=f\"Real mean: {real_pair_sync.mean():.2f}\")\n",
    "\n",
    "ax1.set_xlabel(\"Synchrony\", fontsize=11)\n",
    "ax1.set_ylabel(\"Density\", fontsize=11)\n",
    "ax1.set_title(\"Real Pairs vs Pseudo-Pair Null Distribution\", fontsize=12, fontweight='bold')\n",
    "ax1.legend(loc='upper right', fontsize=9)\n",
    "ax1.set_xlim(0, 1)\n",
    "\n",
    "# Right panel: Summary statistics\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Bar plot comparing means\n",
    "means = [pseudo_pair_sync.mean(), real_pair_sync.mean()]\n",
    "stds = [pseudo_pair_sync.std(), real_pair_sync.std()]\n",
    "colors_bars = [pseudo_color, real_color]\n",
    "labels = [\"Pseudo-pairs\\n(null)\", \"Real pairs\"]\n",
    "\n",
    "bars = ax2.bar(labels, means, color=colors_bars, alpha=0.8, edgecolor='white', linewidth=2)\n",
    "ax2.errorbar(labels, means, yerr=stds, fmt='none', color=COLORS[\"text\"], \n",
    "             capsize=5, capthick=2, linewidth=2)\n",
    "\n",
    "# Add significance annotation\n",
    "from scipy import stats\n",
    "t_stat, p_val = stats.ttest_ind(real_pair_sync, pseudo_pair_sync)\n",
    "\n",
    "# Significance bracket\n",
    "y_max = max(means) + max(stds) + 0.05\n",
    "ax2.plot([0, 0, 1, 1], [y_max, y_max + 0.02, y_max + 0.02, y_max], \n",
    "         color=COLORS[\"text\"], linewidth=1.5)\n",
    "\n",
    "# Format p-value nicely\n",
    "sig_text = \"***\" if p_val < 0.001 else (\"**\" if p_val < 0.01 else (\"*\" if p_val < 0.05 else \"n.s.\"))\n",
    "if p_val < 0.001:\n",
    "    p_display = \"p < 0.001\"\n",
    "else:\n",
    "    p_display = f\"p = {p_val:.3f}\"\n",
    "\n",
    "ax2.text(0.5, y_max + 0.04, f\"{sig_text}\\n{p_display}\", ha='center', fontsize=10,\n",
    "         color=real_color if p_val < 0.05 else COLORS[\"text\"])\n",
    "\n",
    "ax2.set_ylabel(\"Mean Synchrony\", fontsize=11)\n",
    "ax2.set_title(\"Statistical Comparison\", fontsize=12, fontweight='bold')\n",
    "ax2.set_ylim(0, 0.85)\n",
    "\n",
    "# Add interpretation text\n",
    "if real_pair_sync.mean() > pseudo_pair_sync.mean() and p_val < 0.05:\n",
    "    conclusion = \"‚úì Synchrony is INTERACTION-SPECIFIC!\"\n",
    "    text_color = success_color\n",
    "else:\n",
    "    conclusion = \"‚ö† Synchrony may be stimulus-driven\"\n",
    "    text_color = COLORS[\"negative\"]\n",
    "\n",
    "ax2.text(0.5, 0.02, conclusion, ha='center', fontsize=12, fontweight='bold',\n",
    "         color=text_color, transform=ax2.transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 10: Pseudo-pair null distribution test\")\n",
    "print(f\"  Real pairs mean: {real_pair_sync.mean():.3f} ¬± {real_pair_sync.std():.3f}\")\n",
    "print(f\"  Pseudo-pairs mean: {pseudo_pair_sync.mean():.3f} ¬± {pseudo_pair_sync.std():.3f}\")\n",
    "print(f\"  t-test p-value: {p_val:.2e}\")\n",
    "print(f\"  Conclusion: Real pairs show significantly higher synchrony!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65220b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Interpreting Inter-Brain Synchrony\n",
    "\n",
    "**‚è±Ô∏è Duration: 8 minutes**\n",
    "\n",
    "### What Does Synchrony Actually Mean?\n",
    "\n",
    "Finding inter-brain synchrony is exciting, but **interpretation requires caution**. Synchrony does NOT automatically mean:\n",
    "- üß†‚ÜîÔ∏èüß† Telepathic communication\n",
    "- üí≠ Understanding each other's thoughts\n",
    "- ‚úÖ Better interaction quality (always)\n",
    "\n",
    "### Possible Mechanisms\n",
    "\n",
    "**1. Shared Stimulus Processing**\n",
    "Both participants perceive the same environment (sounds, visuals) ‚Üí similar sensory processing ‚Üí correlated brain activity. This is NOT truly \"social\" synchrony.\n",
    "> **Control**: Pseudo-pair analysis\n",
    "\n",
    "**2. Behavioral Coordination**\n",
    "Synchronized movements (gestures, speech rhythm) ‚Üí synchronized sensorimotor activity. May include movement artifacts, but also genuine coordination signatures.\n",
    "> **Example**: Drummers synchronizing create motor cortex synchrony\n",
    "\n",
    "**3. Predictive Coupling**\n",
    "Brain A predicts Brain B's behavior and prepares responses; Brain B does the same. This interactive loop creates emergent synchrony.\n",
    "> **Most \"social\"**: Reflects true interactive dynamics\n",
    "\n",
    "**4. Common Physiological Rhythms**\n",
    "Shared arousal, breathing synchronization, heart rate coupling. Less \"cognitive\" but still socially relevant.\n",
    "> **Example**: Calm therapist ‚Üí calm patient ‚Üí physiological alignment\n",
    "\n",
    "### What Synchrony Correlates With\n",
    "\n",
    "| Finding | Domain | Reference |\n",
    "|---------|--------|-----------|\n",
    "| Task performance | Cooperation | Astolfi et al., 2010 |\n",
    "| Subjective rapport | Conversation | P√©rez et al., 2017 |\n",
    "| Learning outcomes | Education | Dikker et al., 2017 |\n",
    "| Therapeutic alliance | Clinical | Ramseyer & Tschacher, 2011 |\n",
    "| Musical coordination | Performance | Lindenberger et al., 2009 |\n",
    "\n",
    "### The Causality Question\n",
    "\n",
    "```\n",
    "Does synchrony CAUSE better interaction?\n",
    "       ‚ÜïÔ∏è (or both?)\n",
    "Does good interaction CAUSE synchrony?\n",
    "```\n",
    "\n",
    "**Current evidence is mostly correlational.** Experimental manipulations (disrupting synchrony, inducing it artificially) are active research frontiers.\n",
    "\n",
    "> üí° **Key insight**: Inter-brain synchrony is a **signature** of successful social interaction, but the causal mechanisms are still being investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualization 11: Mechanisms of Inter-Brain Synchrony\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Helper to draw a simple head\n",
    "def draw_head(ax: plt.Axes, x: float, y: float, radius: float, color: str, label: str) -> None:\n",
    "    head = Circle((x, y), radius, facecolor=color, edgecolor=COLORS[\"text\"], \n",
    "                  linewidth=2, alpha=0.7)\n",
    "    ax.add_patch(head)\n",
    "    ax.text(x, y, label, ha='center', va='center', fontsize=10, fontweight='bold',\n",
    "            color=COLORS[\"text\"])\n",
    "\n",
    "# Panel 1: Shared Stimulus Processing\n",
    "ax1 = axes[0, 0]\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 8)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axis('off')\n",
    "ax1.set_title(\"1. Shared Stimulus Processing\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "# Screen in center\n",
    "screen = Rectangle((4, 5), 2, 1.5, facecolor=COLORS[\"grid\"], edgecolor=COLORS[\"text\"], linewidth=2)\n",
    "ax1.add_patch(screen)\n",
    "ax1.text(5, 5.75, \"SCREEN\", fontsize=8, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Two participants looking at screen\n",
    "draw_head(ax1, 2, 3, 0.8, COLORS[\"signal_1\"], \"P1\")\n",
    "draw_head(ax1, 8, 3, 0.8, COLORS[\"signal_2\"], \"P2\")\n",
    "\n",
    "# Arrows from screen to both\n",
    "ax1.annotate('', xy=(2.5, 3.8), xytext=(4.2, 5), \n",
    "             arrowprops=dict(arrowstyle='->', color=COLORS[\"signal_4\"], lw=2))\n",
    "ax1.annotate('', xy=(7.5, 3.8), xytext=(5.8, 5),\n",
    "             arrowprops=dict(arrowstyle='->', color=COLORS[\"signal_4\"], lw=2))\n",
    "\n",
    "# Similar waves under each head\n",
    "t = np.linspace(0, 2*np.pi, 50)\n",
    "ax1.plot(np.linspace(1, 3, 50), 1.5 + 0.3*np.sin(t), color=COLORS[\"signal_1\"], lw=2)\n",
    "ax1.plot(np.linspace(7, 9, 50), 1.5 + 0.3*np.sin(t), color=COLORS[\"signal_2\"], lw=2)\n",
    "ax1.text(5, 0.5, \"Similar responses to same input\\n(NOT interaction-specific)\", \n",
    "         ha='center', fontsize=9, style='italic', color=COLORS[\"text\"])\n",
    "\n",
    "# Panel 2: Behavioral Coordination\n",
    "ax2 = axes[0, 1]\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 8)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.axis('off')\n",
    "ax2.set_title(\"2. Behavioral Coordination\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "draw_head(ax2, 3, 4, 0.8, COLORS[\"signal_1\"], \"P1\")\n",
    "draw_head(ax2, 7, 4, 0.8, COLORS[\"signal_2\"], \"P2\")\n",
    "\n",
    "# Synchronized movement arrows\n",
    "for y_off in [0.5, -0.5]:\n",
    "    ax2.annotate('', xy=(4, 4 + y_off), xytext=(3.8, 4 + y_off),\n",
    "                 arrowprops=dict(arrowstyle='->', color=COLORS[\"signal_1\"], lw=2))\n",
    "    ax2.annotate('', xy=(6, 4 + y_off), xytext=(6.2, 4 + y_off),\n",
    "                 arrowprops=dict(arrowstyle='->', color=COLORS[\"signal_2\"], lw=2))\n",
    "\n",
    "# Synchronized waves\n",
    "ax2.plot(np.linspace(1.5, 4.5, 50), 2 + 0.3*np.sin(t), color=COLORS[\"signal_1\"], lw=2)\n",
    "ax2.plot(np.linspace(5.5, 8.5, 50), 2 + 0.3*np.sin(t), color=COLORS[\"signal_2\"], lw=2)\n",
    "\n",
    "# Connection between movements\n",
    "ax2.plot([4.2, 5.8], [4, 4], '--', color=COLORS[\"high_sync\"], lw=2)\n",
    "ax2.text(5, 6.5, \"[ Joint Action ]\", fontsize=11, ha='center', fontweight='bold',\n",
    "         color=COLORS[\"high_sync\"])\n",
    "ax2.text(5, 0.5, \"Synchronized actions -> synchronized motor activity\", \n",
    "         ha='center', fontsize=9, style='italic', color=COLORS[\"text\"])\n",
    "\n",
    "# Panel 3: Predictive Coupling\n",
    "ax3 = axes[1, 0]\n",
    "ax3.set_xlim(0, 10)\n",
    "ax3.set_ylim(0, 8)\n",
    "ax3.set_aspect('equal')\n",
    "ax3.axis('off')\n",
    "ax3.set_title(\"3. Predictive Coupling\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "draw_head(ax3, 3, 4, 0.8, COLORS[\"signal_1\"], \"P1\")\n",
    "draw_head(ax3, 7, 4, 0.8, COLORS[\"signal_2\"], \"P2\")\n",
    "\n",
    "# Bidirectional arrows with labels\n",
    "ax3.annotate('', xy=(6, 4.5), xytext=(4, 4.5),\n",
    "             arrowprops=dict(arrowstyle='->', color=COLORS[\"high_sync\"], lw=2.5))\n",
    "ax3.annotate('', xy=(4, 3.5), xytext=(6, 3.5),\n",
    "             arrowprops=dict(arrowstyle='->', color=COLORS[\"high_sync\"], lw=2.5))\n",
    "\n",
    "ax3.text(5, 5.2, \"Predict & respond\", fontsize=9, ha='center', color=COLORS[\"high_sync\"])\n",
    "ax3.text(5, 2.8, \"Adjust & adapt\", fontsize=9, ha='center', color=COLORS[\"high_sync\"])\n",
    "\n",
    "# Question marks for prediction\n",
    "ax3.text(2.2, 5.5, \"?\", fontsize=16, fontweight='bold', color=COLORS[\"signal_1\"])\n",
    "ax3.text(7.8, 5.5, \"?\", fontsize=16, fontweight='bold', color=COLORS[\"signal_2\"])\n",
    "\n",
    "ax3.text(5, 0.5, \"Interactive loop creates emergent synchrony\\n(Most 'social' mechanism)\", \n",
    "         ha='center', fontsize=9, style='italic', color=COLORS[\"text\"])\n",
    "\n",
    "# Panel 4: Physiological Coupling\n",
    "ax4 = axes[1, 1]\n",
    "ax4.set_xlim(0, 10)\n",
    "ax4.set_ylim(0, 8)\n",
    "ax4.set_aspect('equal')\n",
    "ax4.axis('off')\n",
    "ax4.set_title(\"4. Physiological Coupling\", fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "draw_head(ax4, 3, 4, 0.8, COLORS[\"signal_1\"], \"P1\")\n",
    "draw_head(ax4, 7, 4, 0.8, COLORS[\"signal_2\"], \"P2\")\n",
    "\n",
    "# Heart symbols as text\n",
    "ax4.text(3, 2.5, \"<3\", fontsize=14, ha='center', color=COLORS[\"negative\"], fontweight='bold')\n",
    "ax4.text(7, 2.5, \"<3\", fontsize=14, ha='center', color=COLORS[\"negative\"], fontweight='bold')\n",
    "\n",
    "# Synchronized heartbeat waves\n",
    "heartbeat_t = np.linspace(0, 4*np.pi, 100)\n",
    "heartbeat = np.sin(heartbeat_t) * np.exp(-0.1 * np.abs(heartbeat_t % (2*np.pi) - np.pi))\n",
    "ax4.plot(np.linspace(1.5, 4.5, 100), 1.5 + 0.3*heartbeat, color=COLORS[\"negative\"], lw=2)\n",
    "ax4.plot(np.linspace(5.5, 8.5, 100), 1.5 + 0.3*heartbeat, color=COLORS[\"negative\"], lw=2)\n",
    "\n",
    "# Connection\n",
    "ax4.plot([4, 6], [2.5, 2.5], ':', color=COLORS[\"negative\"], lw=2)\n",
    "\n",
    "ax4.text(5, 6, \"Breathing, arousal, heart rate\", fontsize=10, ha='center')\n",
    "ax4.text(5, 0.5, \"Shared physiological state\\n(Less cognitive, but socially meaningful)\", \n",
    "         ha='center', fontsize=9, style='italic', color=COLORS[\"text\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Visualization 11: Four mechanisms of inter-brain synchrony\")\n",
    "print(\"  1. Shared stimulus -> similar sensory processing (control with pseudo-pairs)\")\n",
    "print(\"  2. Behavioral coordination -> synchronized motor/sensory activity\")\n",
    "print(\"  3. Predictive coupling -> interactive dynamics (most 'social')\")\n",
    "print(\"  4. Physiological coupling -> shared arousal state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df344d22",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. HyPyP and the Tool Ecosystem\n",
    "\n",
    "**‚è±Ô∏è Duration: 5 minutes**\n",
    "\n",
    "### HyPyP: Hyperscanning Python Pipeline\n",
    "\n",
    "**HyPyP** is an open-source Python library specifically designed for hyperscanning analysis:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Built on MNE-Python** | Leverages the powerful MNE ecosystem |\n",
    "| **Connectivity metrics** | PLV, coherence, correlation, and more |\n",
    "| **Statistical analysis** | Surrogates, permutation tests, pseudo-pairs |\n",
    "| **Visualization** | Topoplots, circular plots, matrices |\n",
    "| **Data structures** | Handles dual-subject data natively |\n",
    "\n",
    "> üìö **Reference**: Ayrolles et al. (2021). \"HyPyP: A toolkit for hyperscanning analysis\"\n",
    "\n",
    "### Our Approach in This Workshop\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                       UNDERSTAND                          ‚îÇ\n",
    "‚îÇ  Build from scratch ‚Üí know WHAT the metrics compute       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                        COMPARE                            ‚îÇ\n",
    "‚îÇ  Validate against HyPyP ‚Üí ensure correctness              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                         APPLY                             ‚îÇ\n",
    "‚îÇ  Use HyPyP/MNE in practice ‚Üí efficient production code    ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Tool Ecosystem Overview\n",
    "\n",
    "| Tool | Purpose | Relationship |\n",
    "|------|---------|--------------|\n",
    "| **MNE-Python** | EEG/MEG analysis foundation | Core dependency |\n",
    "| **mne-connectivity** | Single-brain connectivity | Base for metrics |\n",
    "| **HyPyP** | Hyperscanning-specific | Our comparison target |\n",
    "| **This workshop** | Educational implementations | Understanding-focused |\n",
    "| **BCT (MATLAB)** | Graph theory metrics | Graph analysis inspiration |\n",
    "\n",
    "### In Each Metric Notebook\n",
    "\n",
    "For every connectivity metric (PLV, coherence, etc.), we will:\n",
    "\n",
    "1. **Intuition**: What does this metric capture?\n",
    "2. **Mathematics**: The actual formula\n",
    "3. **Implementation**: Code from scratch\n",
    "4. **Validation**: Compare to HyPyP\n",
    "5. **Application**: Use on hyperscanning data\n",
    "6. **Interpretation**: What do results mean?\n",
    "\n",
    "> üí° **Goal**: You'll be able to use HyPyP confidently AND know exactly what's happening under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49d329",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. Complete Hyperscanning Pipeline\n",
    "\n",
    "**Duration: 5 minutes**\n",
    "\n",
    "### End-to-End Workflow\n",
    "\n",
    "A complete hyperscanning analysis follows these steps:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 1: Data Loading & Synchronization                   ‚îÇ\n",
    "‚îÇ    - Load recordings from both participants               ‚îÇ\n",
    "‚îÇ    - Verify temporal alignment (trigger channels)         ‚îÇ\n",
    "‚îÇ    - Interpolate to common time base if needed            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 2: Preprocessing                                    ‚îÇ\n",
    "‚îÇ    - Filter to frequency band of interest                 ‚îÇ\n",
    "‚îÇ    - Artifact rejection (BOTH participants must be clean) ‚îÇ\n",
    "‚îÇ    - Re-reference (same scheme for both)                  ‚îÇ\n",
    "‚îÇ    - Bad channel interpolation                            ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 3: Create Hyperscanning Data Structure              ‚îÇ\n",
    "‚îÇ    - Combine data with P1_, P2_ channel prefixes          ‚îÇ\n",
    "‚îÇ    - Create participant labels                            ‚îÇ\n",
    "‚îÇ    - Organize for connectivity analysis                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 4: Compute Connectivity                             ‚îÇ\n",
    "‚îÇ    - Within-P1 block (n_p1 √ó n_p1)                        ‚îÇ\n",
    "‚îÇ    - Within-P2 block (n_p2 √ó n_p2)                        ‚îÇ\n",
    "‚îÇ    - Between-brain block (n_p1 √ó n_p2)                    ‚îÇ\n",
    "‚îÇ    - Full combined matrix ((n_p1+n_p2) √ó (n_p1+n_p2))     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 5: Statistical Testing                              ‚îÇ\n",
    "‚îÇ    - Surrogate distribution (phase shuffling)             ‚îÇ\n",
    "‚îÇ    - Pseudo-pair comparison                               ‚îÇ\n",
    "‚îÇ    - Multiple comparisons correction                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                            ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  STEP 6: Visualization & Interpretation                   ‚îÇ\n",
    "‚îÇ    - Connectivity matrices                                ‚îÇ\n",
    "‚îÇ    - Significant connections                              ‚îÇ\n",
    "‚îÇ    - Summary statistics                                   ‚îÇ\n",
    "‚îÇ    - Relate to behavioral/clinical outcomes               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Key Code Functions (Preview)\n",
    "\n",
    "We've defined utility functions earlier in this notebook:\n",
    "\n",
    "| Function | Purpose |\n",
    "|----------|---------|\n",
    "| `create_hyperscanning_data_structure()` | Combine P1 + P2 data |\n",
    "| `extract_connectivity_blocks()` | Split full matrix into blocks |\n",
    "| `combine_connectivity_blocks()` | Merge blocks into full matrix |\n",
    "\n",
    "In subsequent notebooks, we'll add connectivity computation functions (PLV, coherence, etc.) that integrate with this pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c84122",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. What's Coming Next\n",
    "\n",
    "**Duration: 3 minutes**\n",
    "\n",
    "### Your Journey Through Connectivity Metrics\n",
    "\n",
    "Now that you understand the hyperscanning framework, you're ready to learn the specific metrics!\n",
    "\n",
    "```\n",
    "                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "                    ‚îÇ   E02: Introduction to          ‚îÇ\n",
    "                    ‚îÇ   Hyperscanning (YOU ARE HERE)  ‚îÇ\n",
    "                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                                    ‚îÇ\n",
    "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "          ‚îÇ                         ‚îÇ                         ‚îÇ\n",
    "          ‚ñº                         ‚ñº                         ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  BLOCK F        ‚îÇ      ‚îÇ  BLOCK G        ‚îÇ      ‚îÇ  BLOCK H        ‚îÇ\n",
    "‚îÇ  Coherence      ‚îÇ      ‚îÇ  Phase-Based    ‚îÇ      ‚îÇ  Amplitude      ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ F01: Spectral   ‚îÇ      ‚îÇ G01: PLV        ‚îÇ      ‚îÇ H01: Envelope   ‚îÇ\n",
    "‚îÇ      Coherence  ‚îÇ      ‚îÇ G02: PLI        ‚îÇ      ‚îÇ      Correlation‚îÇ\n",
    "‚îÇ F02: Imaginary  ‚îÇ      ‚îÇ G03: wPLI       ‚îÇ      ‚îÇ H02: Power      ‚îÇ\n",
    "‚îÇ      Coherence  ‚îÇ      ‚îÇ                 ‚îÇ      ‚îÇ      Correlation‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### What Each Block Covers\n",
    "\n",
    "| Block | Focus | Key Question |\n",
    "|-------|-------|--------------|\n",
    "| **F: Coherence** | Frequency-domain coupling | Are signals linearly related at each frequency? |\n",
    "| **G: Phase** | Phase relationship | Are phases aligned across signals? |\n",
    "| **H: Amplitude** | Power fluctuations | Do power envelopes co-vary? |\n",
    "\n",
    "### In Each Metric Notebook\n",
    "\n",
    "1. **Intuition**: What does this metric capture?\n",
    "2. **Mathematics**: The formula, step by step\n",
    "3. **Implementation**: Python code from scratch\n",
    "4. **Visualization**: See what the metric reveals\n",
    "5. **HyPyP Comparison**: Validate against production tools\n",
    "6. **Hyperscanning Application**: Apply to inter-brain analysis\n",
    "7. **Interpretation Guide**: What do results mean?\n",
    "\n",
    "> **Ready to dive in?** Next up: **F01 - Spectral Coherence**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa56efe",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|-----------|\n",
    "| **Hyperscanning** | Simultaneous recording from 2+ interacting people |\n",
    "| **Inter-brain synchrony** | Coupling between brain signals of different people |\n",
    "| **No volume conduction** | Between-brain: no shared conduction (advantage!) |\n",
    "| **Confounds** | Behavioral sync, shared stimuli, movement artifacts |\n",
    "| **Pseudo-pairs** | Essential control for interaction specificity |\n",
    "| **Data structure** | Within-P1, Within-P2, Between blocks |\n",
    "| **Metric choice** | Depends on phase vs amplitude, directed vs undirected |\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "1. **Definition**: Hyperscanning studies social brain IN social context\n",
    "2. **Applications**: Education, clinical, development, performance\n",
    "3. **Paradigms**: Cooperation, communication, joint attention, imitation\n",
    "4. **Challenges**: Behavioral confounds, stimulus-driven effects\n",
    "5. **Data organization**: Combined matrices with clear labeling\n",
    "6. **Preprocessing**: Synchronization, joint artifact rejection\n",
    "7. **Metric selection**: PLV, coherence, correlation, transfer entropy\n",
    "8. **Pseudo-pairs**: The key control analysis\n",
    "9. **Interpretation**: Correlation vs causation, multiple mechanisms\n",
    "\n",
    "### Utility Functions Created\n",
    "\n",
    "```python\n",
    "# Data structure\n",
    "create_hyperscanning_data_structure(data_p1, data_p2, channels_p1, channels_p2)\n",
    "extract_connectivity_blocks(full_matrix, n_channels_p1)\n",
    "combine_connectivity_blocks(within_p1, within_p2, between)\n",
    "```\n",
    "\n",
    "### Ready for Connectivity Metrics!\n",
    "\n",
    "You now have the conceptual foundation to understand:\n",
    "- **WHY** we measure inter-brain connectivity\n",
    "- **WHAT** challenges we face\n",
    "- **HOW** to structure and analyze hyperscanning data\n",
    "\n",
    "Next: Learn the specific metrics (coherence, PLV, PLI, etc.) and apply them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00843a59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Discussion Questions\n",
    "\n",
    "1. **Experimental Design**: You want to study parent-child attachment through hyperscanning. What paradigm would you design? What frequencies might be most relevant? What confounds would you worry about?\n",
    "\n",
    "2. **Addressing Criticism**: A reviewer criticizes your study: \"This synchrony is just because both participants see the same screen.\" How do you respond? What analyses would address this concern?\n",
    "\n",
    "3. **Causality**: Inter-brain synchrony during therapy predicts treatment outcome. Does this mean we should try to INCREASE synchrony to improve treatment? What cautions would you raise?\n",
    "\n",
    "4. **Frequency Specificity**: You find theta synchrony during cooperation but alpha synchrony during competition. What might explain frequency-specific effects in different social contexts?\n",
    "\n",
    "5. **Remote Interaction**: Your hyperscanning study involves video calls (remote interaction). What additional challenges does this introduce compared to face-to-face interaction?\n",
    "\n",
    "---\n",
    "\n",
    "## 16. Exercises\n",
    "\n",
    "### Exercise 1: Data Structure Practice\n",
    "Using the simulated data from this notebook:\n",
    "- Create a hyperscanning data structure with 6 channels per participant\n",
    "- Verify that channel names are correctly prefixed\n",
    "- Extract the between-brain block and compute its mean value\n",
    "\n",
    "### Exercise 2: Pseudo-Pair Simulation\n",
    "- Generate data for 6 \"participants\" forming 3 real pairs\n",
    "- Create all possible pseudo-pairs\n",
    "- Compute simple correlation for real vs pseudo pairs\n",
    "- Is there a significant difference?\n",
    "\n",
    "### Exercise 3: Metric Selection\n",
    "For each scenario, choose the most appropriate metric and justify:\n",
    "- a) \"We want to know if phases align during conversation\" \n",
    "- b) \"We want to know who influences whom during teaching\"\n",
    "- c) \"We need a robust measure for clinical application\"\n",
    "\n",
    "### Exercise 4: Critical Thinking\n",
    "You find that inter-brain synchrony is LOWER for real pairs than pseudo-pairs during a competitive task. What might explain this counterintuitive finding?\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook completed. Proceed to F01: Spectral Coherence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Exercise 1 Solution: Data Structure Practice\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Exercise 1: Data Structure Practice\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data for 6 channels per participant\n",
    "channels_ex1_p1 = [\"Fz\", \"Cz\", \"Pz\", \"F3\", \"F4\", \"Oz\"]\n",
    "channels_ex1_p2 = [\"Fz\", \"Cz\", \"Pz\", \"F3\", \"F4\", \"Oz\"]\n",
    "\n",
    "fs_ex1 = 256\n",
    "duration_ex1 = 5\n",
    "n_samples_ex1 = fs_ex1 * duration_ex1\n",
    "\n",
    "# Simulate data\n",
    "np.random.seed(123)\n",
    "data_ex1_p1 = np.random.randn(len(channels_ex1_p1), n_samples_ex1)\n",
    "data_ex1_p2 = np.random.randn(len(channels_ex1_p2), n_samples_ex1)\n",
    "\n",
    "# Create hyperscanning data structure\n",
    "def create_hyperscanning_structure(\n",
    "    data_p1: NDArray[np.floating],\n",
    "    data_p2: NDArray[np.floating],\n",
    "    channels_p1: List[str],\n",
    "    channels_p2: List[str],\n",
    "    fs: int\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Create a hyperscanning data structure from two participants' data.\"\"\"\n",
    "    # Prefix channel names\n",
    "    prefixed_p1 = [f\"P1_{ch}\" for ch in channels_p1]\n",
    "    prefixed_p2 = [f\"P2_{ch}\" for ch in channels_p2]\n",
    "    \n",
    "    # Combine data\n",
    "    combined_data = np.vstack([data_p1, data_p2])\n",
    "    combined_channels = prefixed_p1 + prefixed_p2\n",
    "    \n",
    "    # Create labels\n",
    "    labels = np.array([1] * len(channels_p1) + [2] * len(channels_p2))\n",
    "    \n",
    "    return {\n",
    "        \"data\": combined_data,\n",
    "        \"channels\": combined_channels,\n",
    "        \"participant_labels\": labels,\n",
    "        \"n_channels_p1\": len(channels_p1),\n",
    "        \"n_channels_p2\": len(channels_p2),\n",
    "        \"fs\": fs\n",
    "    }\n",
    "\n",
    "hyper_ex1 = create_hyperscanning_structure(\n",
    "    data_ex1_p1, data_ex1_p2, channels_ex1_p1, channels_ex1_p2, fs_ex1\n",
    ")\n",
    "\n",
    "print(f\"Combined channels: {hyper_ex1['channels']}\")\n",
    "print(f\"Data shape: {hyper_ex1['data'].shape}\")\n",
    "print(f\"Participant labels: {hyper_ex1['participant_labels']}\")\n",
    "\n",
    "# Extract between-brain block\n",
    "n_p1 = hyper_ex1[\"n_channels_p1\"]\n",
    "n_p2 = hyper_ex1[\"n_channels_p2\"]\n",
    "\n",
    "# Compute simple correlation matrix\n",
    "from scipy.stats import pearsonr\n",
    "corr_matrix = np.zeros((n_p1 + n_p2, n_p1 + n_p2))\n",
    "for i in range(n_p1 + n_p2):\n",
    "    for j in range(n_p1 + n_p2):\n",
    "        corr_matrix[i, j], _ = pearsonr(hyper_ex1[\"data\"][i], hyper_ex1[\"data\"][j])\n",
    "\n",
    "# Extract between-brain block (P1 rows, P2 columns)\n",
    "between_block = corr_matrix[:n_p1, n_p1:]\n",
    "print(f\"\\nBetween-brain block shape: {between_block.shape}\")\n",
    "print(f\"Between-brain block mean: {between_block.mean():.4f}\")\n",
    "print(\"(Expected close to 0 for random data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Exercise 2 Solution: Pseudo-Pair Simulation\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Exercise 2: Pseudo-Pair Simulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "# Generate 6 participants forming 3 real pairs\n",
    "# Real pairs have correlated signals (shared component)\n",
    "n_participants = 6\n",
    "n_samples_ex2 = 1000\n",
    "\n",
    "# Shared component for each real pair\n",
    "shared_1 = np.random.randn(n_samples_ex2)\n",
    "shared_2 = np.random.randn(n_samples_ex2)\n",
    "shared_3 = np.random.randn(n_samples_ex2)\n",
    "\n",
    "# Individual noise\n",
    "noise_level = 0.5\n",
    "\n",
    "participants = {\n",
    "    \"P1\": 0.7 * shared_1 + noise_level * np.random.randn(n_samples_ex2),  # Pair 1\n",
    "    \"P2\": 0.7 * shared_1 + noise_level * np.random.randn(n_samples_ex2),  # Pair 1\n",
    "    \"P3\": 0.7 * shared_2 + noise_level * np.random.randn(n_samples_ex2),  # Pair 2\n",
    "    \"P4\": 0.7 * shared_2 + noise_level * np.random.randn(n_samples_ex2),  # Pair 2\n",
    "    \"P5\": 0.7 * shared_3 + noise_level * np.random.randn(n_samples_ex2),  # Pair 3\n",
    "    \"P6\": 0.7 * shared_3 + noise_level * np.random.randn(n_samples_ex2),  # Pair 3\n",
    "}\n",
    "\n",
    "# Real pairs\n",
    "real_pairs = [(\"P1\", \"P2\"), (\"P3\", \"P4\"), (\"P5\", \"P6\")]\n",
    "\n",
    "# All possible pseudo-pairs (participants from different real pairs)\n",
    "pseudo_pairs_ex2 = [\n",
    "    (\"P1\", \"P3\"), (\"P1\", \"P4\"), (\"P1\", \"P5\"), (\"P1\", \"P6\"),\n",
    "    (\"P2\", \"P3\"), (\"P2\", \"P4\"), (\"P2\", \"P5\"), (\"P2\", \"P6\"),\n",
    "    (\"P3\", \"P5\"), (\"P3\", \"P6\"),\n",
    "    (\"P4\", \"P5\"), (\"P4\", \"P6\"),\n",
    "]\n",
    "\n",
    "# Compute correlations\n",
    "real_correlations = []\n",
    "for p1, p2 in real_pairs:\n",
    "    corr, _ = pearsonr(participants[p1], participants[p2])\n",
    "    real_correlations.append(corr)\n",
    "\n",
    "pseudo_correlations = []\n",
    "for p1, p2 in pseudo_pairs_ex2:\n",
    "    corr, _ = pearsonr(participants[p1], participants[p2])\n",
    "    pseudo_correlations.append(corr)\n",
    "\n",
    "print(f\"Real pairs: {real_pairs}\")\n",
    "print(f\"Real pair correlations: {[f'{c:.3f}' for c in real_correlations]}\")\n",
    "print(f\"Real mean: {np.mean(real_correlations):.3f}\")\n",
    "print(f\"\\nNumber of pseudo-pairs: {len(pseudo_pairs_ex2)}\")\n",
    "print(f\"Pseudo-pair mean: {np.mean(pseudo_correlations):.3f}\")\n",
    "\n",
    "# Statistical test\n",
    "t_ex2, p_ex2 = stats.ttest_ind(real_correlations, pseudo_correlations)\n",
    "print(f\"\\nt-test: t = {t_ex2:.3f}, p = {p_ex2:.4f}\")\n",
    "print(f\"Significant difference: {'YES' if p_ex2 < 0.05 else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caa6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Exercise 3 Solution: Metric Selection\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Exercise 3: Metric Selection\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "solutions_ex3 = \"\"\"\n",
    "a) \"We want to know if phases align during conversation\"\n",
    "   ‚Üí PHASE-LOCKING VALUE (PLV)\n",
    "   Justification: PLV measures phase consistency across trials/time.\n",
    "   It captures whether neural oscillations are aligned in phase,\n",
    "   which is the core question here.\n",
    "\n",
    "b) \"We want to know who influences whom during teaching\"\n",
    "   ‚Üí TRANSFER ENTROPY or GRANGER CAUSALITY\n",
    "   Justification: These are directional metrics that can reveal\n",
    "   information flow from teacher to student (or vice versa).\n",
    "   Transfer entropy is particularly suited for nonlinear relationships.\n",
    "\n",
    "c) \"We need a robust measure for clinical application\"\n",
    "   ‚Üí WEIGHTED PHASE LAG INDEX (wPLI) or IMAGINARY COHERENCE\n",
    "   Justification: These metrics are robust to volume conduction,\n",
    "   which is critical for clinical reliability. wPLI also handles\n",
    "   noise well and is less sensitive to outliers.\n",
    "\"\"\"\n",
    "print(solutions_ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec446b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Exercise 4 Solution: Critical Thinking\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Exercise 4: Critical Thinking\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "solution_ex4 = \"\"\"\n",
    "Finding: Inter-brain synchrony LOWER for real pairs than pseudo-pairs\n",
    "during competition.\n",
    "\n",
    "Possible explanations:\n",
    "\n",
    "1. ACTIVE DESYNCHRONIZATION\n",
    "   During competition, participants may actively try to be unpredictable.\n",
    "   Being synchronized would make you predictable to your opponent.\n",
    "   ‚Üí Lower synchrony = successful competitive strategy\n",
    "\n",
    "2. DIFFERENT STRATEGIES\n",
    "   Real competitors develop complementary (not similar) strategies.\n",
    "   They occupy different \"cognitive niches\" to gain advantage.\n",
    "   ‚Üí Functional differentiation, not coordination\n",
    "\n",
    "3. STIMULUS-DRIVEN BASELINE\n",
    "   If the task has strong visual/auditory components, pseudo-pairs\n",
    "   might show HIGH stimulus-locked synchrony simply from processing\n",
    "   the same sensory input.\n",
    "   Real pairs might show LOWER stimulus-locking because they're\n",
    "   focused on opponent modeling rather than stimulus processing.\n",
    "\n",
    "4. AROUSAL DIFFERENCES\n",
    "   Competition increases arousal, which can desynchronize neural activity.\n",
    "   Real competitive pairs have higher arousal than pseudo-pairs.\n",
    "\n",
    "5. ATTENTION ALLOCATION\n",
    "   Real competitors attend to different aspects (opponent's actions)\n",
    "   while pseudo-pairs both attend to the same task elements.\n",
    "\n",
    "Key insight: \"Higher synchrony = better\" is NOT always true!\n",
    "The meaning of synchrony depends entirely on the context.\n",
    "\"\"\"\n",
    "print(solution_ex4)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úì All exercises completed!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectivity-metrics-tutorials-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
