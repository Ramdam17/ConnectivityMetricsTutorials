{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2e8502",
   "metadata": {},
   "source": [
    "## 1. Introduction â€” Beyond Correlation\n",
    "\n",
    "In previous modules, we explored connectivity measures like phase-based metrics (PLV, PLI) and amplitude correlations. These capture **linear** or **periodic** relationships between signals.\n",
    "\n",
    "But neural relationships can be **nonlinear**!\n",
    "\n",
    "### A Motivating Example\n",
    "\n",
    "Consider two signals X and Y where Y increases when |X| is large, regardless of whether X is positive or negative:\n",
    "\n",
    "- **Correlation = 0** (no linear relationship â€” high positive and negative X values both relate to high Y)\n",
    "- **But there IS a relationship!** Y clearly depends on X\n",
    "\n",
    "### Enter Mutual Information\n",
    "\n",
    "**Mutual Information (MI)** captures **any** statistical dependency between two variables:\n",
    "\n",
    "> *\"How much does knowing X tell us about Y?\"*\n",
    "\n",
    "MI is:\n",
    "- **General**: Detects linear AND nonlinear relationships\n",
    "- **Symmetric**: MI(X, Y) = MI(Y, X)\n",
    "- **Non-negative**: MI â‰¥ 0, with MI = 0 only when X and Y are independent\n",
    "\n",
    "The trade-off: MI is more powerful but computationally harder to estimate than correlation.\n",
    "\n",
    "> ðŸ’¡ **Key insight**: MI detects relationships that correlation misses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dadb52",
   "metadata": {},
   "source": [
    "## 2. Intuition â€” Shared Information\n",
    "\n",
    "Before the math, let's build intuition.\n",
    "\n",
    "### Thought Experiment: Two Weather Stations\n",
    "\n",
    "- **Station A** records temperature in Paris\n",
    "- **Station B** records temperature in Lyon\n",
    "\n",
    "If you know Paris is 25Â°C, you can make a better guess about Lyon's temperature than without any information. The cities share weather patterns!\n",
    "\n",
    "**Mutual information** quantifies this shared uncertainty:\n",
    "- If X and Y are **independent**: knowing X tells nothing about Y â†’ **MI = 0**\n",
    "- If X **determines** Y completely: knowing X removes ALL uncertainty about Y â†’ **MI = H(Y)** (maximum)\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- MI is **symmetric**: MI(X, Y) = MI(Y, X)\n",
    "- MI measures \"how much information is common to both variables\"\n",
    "- MI = 0 â†” statistical independence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7466c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle, Wedge\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib.patches as mpatches\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "from scipy import stats\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from src.colors import COLORS\n",
    "from src.plotting import configure_plots\n",
    "from src.information import (\n",
    "    compute_entropy_discrete,\n",
    "    compute_entropy_continuous,\n",
    "    compute_entropy_from_counts,\n",
    "    optimal_n_bins\n",
    ")\n",
    "\n",
    "configure_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Different relationships â€” correlation vs MI\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate different relationships\n",
    "x_base = np.random.randn(n_samples)\n",
    "\n",
    "# 1. Independent\n",
    "y_independent = np.random.randn(n_samples)\n",
    "\n",
    "# 2. Linear relationship\n",
    "y_linear = 0.8 * x_base + 0.6 * np.random.randn(n_samples)\n",
    "\n",
    "# 3. Nonlinear (quadratic) â€” correlation â‰ˆ 0 but dependent!\n",
    "y_quadratic = x_base**2 + 0.3 * np.random.randn(n_samples)\n",
    "\n",
    "# Compute correlations\n",
    "corr_indep = np.corrcoef(x_base, y_independent)[0, 1]\n",
    "corr_linear = np.corrcoef(x_base, y_linear)[0, 1]\n",
    "corr_quad = np.corrcoef(x_base, y_quadratic)[0, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Independent\n",
    "axes[0].scatter(x_base, y_independent, alpha=0.5, s=20, color=COLORS[\"signal_1\"])\n",
    "axes[0].set_xlabel(\"X\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Y\", fontsize=12)\n",
    "axes[0].set_title(f\"Independent\\nCorr = {corr_indep:.3f}\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].text(0.05, 0.95, \"MI â‰ˆ 0\", transform=axes[0].transAxes, fontsize=11,\n",
    "             fontweight=\"bold\", va=\"top\", bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Plot 2: Linear\n",
    "axes[1].scatter(x_base, y_linear, alpha=0.5, s=20, color=COLORS[\"signal_2\"])\n",
    "axes[1].set_xlabel(\"X\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Y\", fontsize=12)\n",
    "axes[1].set_title(f\"Linear Relationship\\nCorr = {corr_linear:.3f}\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].text(0.05, 0.95, \"MI > 0\", transform=axes[1].transAxes, fontsize=11,\n",
    "             fontweight=\"bold\", va=\"top\", bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "# Plot 3: Quadratic (nonlinear)\n",
    "axes[2].scatter(x_base, y_quadratic, alpha=0.5, s=20, color=COLORS[\"signal_3\"])\n",
    "axes[2].set_xlabel(\"X\", fontsize=12)\n",
    "axes[2].set_ylabel(\"Y\", fontsize=12)\n",
    "axes[2].set_title(f\"Quadratic (Nonlinear)\\nCorr = {corr_quad:.3f}\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].text(0.05, 0.95, \"MI > 0 !\", transform=axes[2].transAxes, fontsize=11,\n",
    "             fontweight=\"bold\", va=\"top\", color=\"red\",\n",
    "             bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "plt.suptitle(\"MI Captures Relationships That Correlation Misses\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key observation:\")\n",
    "print(f\"   â€¢ Quadratic relationship: Correlation = {corr_quad:.3f} (nearly zero!)\")\n",
    "print(\"   â€¢ But Y clearly depends on X â€” MI will detect this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb9259",
   "metadata": {},
   "source": [
    "## 3. The Entropy Venn Diagram\n",
    "\n",
    "The relationship between entropy and mutual information is beautifully captured by a **Venn diagram**.\n",
    "\n",
    "### The Diagram\n",
    "\n",
    "Imagine two overlapping circles:\n",
    "- **Circle X**: Total entropy H(X)\n",
    "- **Circle Y**: Total entropy H(Y)\n",
    "- **Overlap**: Mutual Information I(X; Y)\n",
    "- **X only** (left crescent): H(X|Y) â€” uncertainty about X given Y\n",
    "- **Y only** (right crescent): H(Y|X) â€” uncertainty about Y given X\n",
    "- **Union** (both circles): H(X, Y) â€” joint entropy\n",
    "\n",
    "### Key Relationships\n",
    "\n",
    "$$I(X; Y) = H(X) + H(Y) - H(X, Y)$$\n",
    "\n",
    "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$\n",
    "\n",
    "MI = \"what's shared\" = \"uncertainty reduced by knowing the other variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5da1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Entropy Venn Diagram\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Circle parameters\n",
    "r = 1.5\n",
    "offset = 0.9\n",
    "\n",
    "# Draw circles\n",
    "circle_x = plt.Circle((-offset, 0), r, fill=False, color=COLORS[\"signal_1\"], linewidth=3)\n",
    "circle_y = plt.Circle((offset, 0), r, fill=False, color=COLORS[\"signal_2\"], linewidth=3)\n",
    "\n",
    "# Fill regions with alpha\n",
    "circle_x_fill = plt.Circle((-offset, 0), r, alpha=0.3, color=COLORS[\"signal_1\"])\n",
    "circle_y_fill = plt.Circle((offset, 0), r, alpha=0.3, color=COLORS[\"signal_2\"])\n",
    "\n",
    "ax.add_patch(circle_x_fill)\n",
    "ax.add_patch(circle_y_fill)\n",
    "ax.add_patch(circle_x)\n",
    "ax.add_patch(circle_y)\n",
    "\n",
    "# Labels\n",
    "ax.text(-offset - 0.9, 0, \"H(X|Y)\", fontsize=14, fontweight=\"bold\", ha=\"center\", va=\"center\")\n",
    "ax.text(offset + 0.9, 0, \"H(Y|X)\", fontsize=14, fontweight=\"bold\", ha=\"center\", va=\"center\")\n",
    "ax.text(0, 0, \"I(X;Y)\", fontsize=16, fontweight=\"bold\", ha=\"center\", va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.9))\n",
    "\n",
    "# Circle labels\n",
    "ax.text(-offset, r + 0.3, \"H(X)\", fontsize=14, fontweight=\"bold\", ha=\"center\", color=COLORS[\"signal_1\"])\n",
    "ax.text(offset, r + 0.3, \"H(Y)\", fontsize=14, fontweight=\"bold\", ha=\"center\", color=COLORS[\"signal_2\"])\n",
    "\n",
    "# Joint entropy brace/label\n",
    "ax.annotate(\"\", xy=(-offset - r, -r - 0.5), xytext=(offset + r, -r - 0.5),\n",
    "            arrowprops=dict(arrowstyle=\"<->\", color=\"black\", lw=2))\n",
    "ax.text(0, -r - 0.8, \"H(X, Y) = Joint Entropy\", fontsize=12, fontweight=\"bold\", ha=\"center\")\n",
    "\n",
    "ax.set_xlim(-3.5, 3.5)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.axis(\"off\")\n",
    "ax.set_title(\"The Information Venn Diagram\", fontsize=16, fontweight=\"bold\", pad=20)\n",
    "\n",
    "# Add formulas\n",
    "formulas = [\n",
    "    r\"$I(X;Y) = H(X) + H(Y) - H(X,Y)$\",\n",
    "    r\"$I(X;Y) = H(X) - H(X|Y)$\",\n",
    "    r\"$I(X;Y) = H(Y) - H(Y|X)$\"\n",
    "]\n",
    "for i, formula in enumerate(formulas):\n",
    "    ax.text(3.2, 1.5 - i * 0.6, formula, fontsize=11, ha=\"left\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ The overlap (I(X;Y)) represents SHARED information.\")\n",
    "print(\"   More overlap = more mutual information = stronger dependency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Three cases of dependency\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "cases = [\n",
    "    (\"Independent\\nI(X;Y) = 0\", 2.5, 0),      # No overlap\n",
    "    (\"Partially Dependent\\nI(X;Y) moderate\", 1.0, 0.5),  # Some overlap\n",
    "    (\"Fully Dependent\\nI(X;Y) = H(Y)\", 0, 0.8)   # One inside other\n",
    "]\n",
    "\n",
    "for ax, (title, offset, scale_y) in zip(axes, cases):\n",
    "    r_x = 1.2\n",
    "    r_y = 1.2 * (1 - scale_y * 0.5) if scale_y > 0 else 1.2\n",
    "    \n",
    "    circle_x = plt.Circle((-offset/2, 0), r_x, alpha=0.4, color=COLORS[\"signal_1\"])\n",
    "    circle_y = plt.Circle((offset/2, 0), r_y, alpha=0.4, color=COLORS[\"signal_2\"])\n",
    "    circle_x_line = plt.Circle((-offset/2, 0), r_x, fill=False, color=COLORS[\"signal_1\"], linewidth=2)\n",
    "    circle_y_line = plt.Circle((offset/2, 0), r_y, fill=False, color=COLORS[\"signal_2\"], linewidth=2)\n",
    "    \n",
    "    ax.add_patch(circle_x)\n",
    "    ax.add_patch(circle_y)\n",
    "    ax.add_patch(circle_x_line)\n",
    "    ax.add_patch(circle_y_line)\n",
    "    \n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(-offset/2, -1.7, \"X\", fontsize=12, ha=\"center\", fontweight=\"bold\", color=COLORS[\"signal_1\"])\n",
    "    ax.text(offset/2 if offset > 0 else 0, -1.7 if offset > 0 else -1.0, \"Y\", \n",
    "            fontsize=12, ha=\"center\", fontweight=\"bold\", color=COLORS[\"signal_2\"])\n",
    "\n",
    "plt.suptitle(\"How MI Reflects Dependency\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3f752",
   "metadata": {},
   "source": [
    "## 4. Joint Entropy\n",
    "\n",
    "**Joint entropy** H(X, Y) measures the uncertainty about the **pair** (X, Y) together.\n",
    "\n",
    "### Definition\n",
    "\n",
    "For discrete variables:\n",
    "\n",
    "$$H(X, Y) = -\\sum_{x}\\sum_{y} p(x, y) \\log p(x, y)$$\n",
    "\n",
    "Where $p(x, y)$ is the **joint probability distribution**.\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Subadditivity**: $H(X, Y) \\leq H(X) + H(Y)$\n",
    "- Equality when X and Y are **independent**\n",
    "- **Lower bound**: $H(X, Y) \\geq \\max(H(X), H(Y))$\n",
    "\n",
    "### For Continuous Signals\n",
    "\n",
    "We need **2D binning**:\n",
    "1. Create a 2D histogram of (x, y) pairs\n",
    "2. Normalize to get joint probability\n",
    "3. Compute entropy of this 2D distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf961ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_joint_histogram(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_bins: int = 20\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute 2D histogram for joint distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First signal.\n",
    "    y : NDArray[np.float64]\n",
    "        Second signal.\n",
    "    n_bins : int, optional\n",
    "        Number of bins per dimension. Default is 20.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[NDArray, NDArray, NDArray]\n",
    "        (histogram_2d, x_edges, y_edges)\n",
    "    \"\"\"\n",
    "    hist_2d, x_edges, y_edges = np.histogram2d(x, y, bins=n_bins)\n",
    "    return hist_2d, x_edges, y_edges\n",
    "\n",
    "\n",
    "def compute_joint_entropy(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_bins: int = 20,\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute joint entropy H(X, Y) via 2D binning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First signal.\n",
    "    y : NDArray[np.float64]\n",
    "        Second signal.\n",
    "    n_bins : int, optional\n",
    "        Number of bins per dimension. Default is 20.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2 (bits).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Joint entropy H(X, Y).\n",
    "    \"\"\"\n",
    "    # Compute 2D histogram\n",
    "    hist_2d, _, _ = compute_joint_histogram(x, y, n_bins)\n",
    "    \n",
    "    # Normalize to get joint probability\n",
    "    joint_prob = hist_2d / np.sum(hist_2d)\n",
    "    \n",
    "    # Flatten and remove zeros\n",
    "    p = joint_prob.flatten()\n",
    "    p = p[p > 0]\n",
    "    \n",
    "    # Compute entropy\n",
    "    if base == np.e:\n",
    "        entropy = -np.sum(p * np.log(p))\n",
    "    else:\n",
    "        entropy = -np.sum(p * np.log(p) / np.log(base))\n",
    "    \n",
    "    return float(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Joint distribution heatmap with marginals\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate correlated Gaussian signals\n",
    "n_samples = 2000\n",
    "correlation = 0.7\n",
    "x = np.random.randn(n_samples)\n",
    "y = correlation * x + np.sqrt(1 - correlation**2) * np.random.randn(n_samples)\n",
    "\n",
    "n_bins = 30\n",
    "\n",
    "# Create figure with marginals\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "gs = fig.add_gridspec(3, 3, width_ratios=[0.2, 1, 0.05], height_ratios=[0.2, 1, 0.05],\n",
    "                      hspace=0.05, wspace=0.05)\n",
    "\n",
    "# Main 2D histogram\n",
    "ax_main = fig.add_subplot(gs[1, 1])\n",
    "hist_2d, x_edges, y_edges, im = ax_main.hist2d(x, y, bins=n_bins, cmap=\"viridis\")\n",
    "ax_main.set_xlabel(\"X\", fontsize=12)\n",
    "ax_main.set_ylabel(\"Y\", fontsize=12)\n",
    "\n",
    "# Colorbar\n",
    "ax_cbar = fig.add_subplot(gs[1, 2])\n",
    "plt.colorbar(im, cax=ax_cbar, label=\"Count\")\n",
    "\n",
    "# Top marginal (X)\n",
    "ax_top = fig.add_subplot(gs[0, 1], sharex=ax_main)\n",
    "ax_top.hist(x, bins=n_bins, color=COLORS[\"signal_1\"], edgecolor=\"white\", alpha=0.8)\n",
    "ax_top.set_ylabel(\"Count\")\n",
    "ax_top.tick_params(labelbottom=False)\n",
    "ax_top.set_title(\"Marginal X\", fontsize=11)\n",
    "\n",
    "# Left marginal (Y)\n",
    "ax_left = fig.add_subplot(gs[1, 0], sharey=ax_main)\n",
    "ax_left.hist(y, bins=n_bins, orientation=\"horizontal\", color=COLORS[\"signal_2\"], \n",
    "             edgecolor=\"white\", alpha=0.8)\n",
    "ax_left.set_xlabel(\"Count\")\n",
    "ax_left.tick_params(labelleft=False)\n",
    "ax_left.invert_xaxis()\n",
    "ax_left.set_title(\"Marginal Y\", fontsize=11, rotation=90, x=-0.3, y=0.5)\n",
    "\n",
    "# Compute entropies\n",
    "H_x, _ = compute_entropy_continuous(x, n_bins=n_bins)\n",
    "H_y, _ = compute_entropy_continuous(y, n_bins=n_bins)\n",
    "H_xy = compute_joint_entropy(x, y, n_bins=n_bins)\n",
    "\n",
    "plt.suptitle(f\"Joint Distribution (r = {correlation})\\n\" +\n",
    "             f\"H(X) = {H_x:.2f}, H(Y) = {H_y:.2f}, H(X,Y) = {H_xy:.2f} bits\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Entropy Analysis:\")\n",
    "print(f\"   H(X) = {H_x:.3f} bits\")\n",
    "print(f\"   H(Y) = {H_y:.3f} bits\")\n",
    "print(f\"   H(X) + H(Y) = {H_x + H_y:.3f} bits\")\n",
    "print(f\"   H(X, Y) = {H_xy:.3f} bits\")\n",
    "print(f\"   â†’ H(X,Y) < H(X) + H(Y) because X and Y are dependent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6864dd",
   "metadata": {},
   "source": [
    "## 5. Conditional Entropy\n",
    "\n",
    "**Conditional entropy** H(X|Y) measures the remaining uncertainty about X **after** we observe Y.\n",
    "\n",
    "### Definition\n",
    "\n",
    "$$H(X|Y) = H(X, Y) - H(Y)$$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$$H(X|Y) = -\\sum_{x,y} p(x, y) \\log p(x|y)$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **H(X|Y) = 0**: Y completely determines X (no remaining uncertainty)\n",
    "- **H(X|Y) = H(X)**: Y tells us nothing about X (X and Y independent)\n",
    "- In between: Y partially reduces uncertainty about X\n",
    "\n",
    "### Connection to MI\n",
    "\n",
    "$$I(X; Y) = H(X) - H(X|Y)$$\n",
    "\n",
    "MI = initial uncertainty minus remaining uncertainty = **uncertainty reduced by knowing Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_entropy(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_bins: int = 20,\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute conditional entropy H(X|Y).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First signal (the one we're uncertain about).\n",
    "    y : NDArray[np.float64]\n",
    "        Second signal (the one we condition on).\n",
    "    n_bins : int, optional\n",
    "        Number of bins per dimension. Default is 20.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2 (bits).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Conditional entropy H(X|Y) = H(X,Y) - H(Y).\n",
    "    \"\"\"\n",
    "    H_xy = compute_joint_entropy(x, y, n_bins, base)\n",
    "    H_y, _ = compute_entropy_continuous(y, n_bins=n_bins)\n",
    "    \n",
    "    # Convert H_y to same base if needed\n",
    "    if base != 2.0:\n",
    "        H_y = H_y * np.log(2) / np.log(base)\n",
    "    \n",
    "    return H_xy - H_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fef928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Entropy decomposition\n",
    "\n",
    "# Use same signals from before\n",
    "H_x_given_y = compute_conditional_entropy(x, y, n_bins=n_bins)\n",
    "H_y_given_x = compute_conditional_entropy(y, x, n_bins=n_bins)\n",
    "MI = H_x - H_x_given_y  # I(X;Y) = H(X) - H(X|Y)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Left: H(X) decomposition\n",
    "ax = axes[0]\n",
    "bar_width = 0.5\n",
    "ax.bar([0], [H_x_given_y], bar_width, label=f\"H(X|Y) = {H_x_given_y:.2f}\", color=COLORS[\"signal_1\"], alpha=0.7)\n",
    "ax.bar([0], [MI], bar_width, bottom=[H_x_given_y], label=f\"I(X;Y) = {MI:.2f}\", color=COLORS[\"signal_3\"], alpha=0.7)\n",
    "ax.axhline(H_x, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "ax.text(0.6, H_x, f\"H(X) = {H_x:.2f}\", fontsize=11, va=\"center\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(0, H_x * 1.2)\n",
    "ax.set_xticks([0])\n",
    "ax.set_xticklabels([\"Entropy of X\"])\n",
    "ax.set_ylabel(\"Entropy (bits)\", fontsize=12)\n",
    "ax.set_title(\"H(X) = H(X|Y) + I(X;Y)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "# Right: H(Y) decomposition\n",
    "ax = axes[1]\n",
    "ax.bar([0], [H_y_given_x], bar_width, label=f\"H(Y|X) = {H_y_given_x:.2f}\", color=COLORS[\"signal_2\"], alpha=0.7)\n",
    "ax.bar([0], [MI], bar_width, bottom=[H_y_given_x], label=f\"I(X;Y) = {MI:.2f}\", color=COLORS[\"signal_3\"], alpha=0.7)\n",
    "ax.axhline(H_y, color=\"black\", linestyle=\"--\", linewidth=2)\n",
    "ax.text(0.6, H_y, f\"H(Y) = {H_y:.2f}\", fontsize=11, va=\"center\")\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(0, H_y * 1.2)\n",
    "ax.set_xticks([0])\n",
    "ax.set_xticklabels([\"Entropy of Y\"])\n",
    "ax.set_ylabel(\"Entropy (bits)\", fontsize=12)\n",
    "ax.set_title(\"H(Y) = H(Y|X) + I(X;Y)\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.suptitle(\"Entropy Decomposition: MI is the Shared Part\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š The same I(X;Y) = {MI:.3f} bits appears in BOTH decompositions!\")\n",
    "print(\"   This is the 'shared information' â€” the overlap in the Venn diagram.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f03fa",
   "metadata": {},
   "source": [
    "## 6. Mutual Information â€” The Formula\n",
    "\n",
    "Now we can formally define mutual information.\n",
    "\n",
    "### Definition 1: Via Joint and Marginal Distributions\n",
    "\n",
    "$$I(X; Y) = \\sum_{x,y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$\n",
    "\n",
    "This measures how much the joint distribution differs from the product of marginals (what we'd expect if independent).\n",
    "\n",
    "### Definition 2: Via Entropies\n",
    "\n",
    "$$I(X; Y) = H(X) + H(Y) - H(X, Y)$$\n",
    "\n",
    "### Definition 3: Via Conditional Entropy\n",
    "\n",
    "$$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$$\n",
    "\n",
    "All three are equivalent!\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **Non-negative**: $I(X; Y) \\geq 0$ always\n",
    "- **Zero iff independent**: $I(X; Y) = 0 \\Leftrightarrow$ X and Y are statistically independent\n",
    "- **Symmetric**: $I(X; Y) = I(Y; X)$\n",
    "- **Self-information**: $I(X; X) = H(X)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mutual_information(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_bins: int = 20,\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute mutual information I(X; Y).\n",
    "    \n",
    "    Uses the formula: I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First signal.\n",
    "    y : NDArray[np.float64]\n",
    "        Second signal.\n",
    "    n_bins : int, optional\n",
    "        Number of bins per dimension. Default is 20.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2 (bits).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mutual information I(X; Y).\n",
    "    \"\"\"\n",
    "    # Compute individual entropies\n",
    "    H_x, _ = compute_entropy_continuous(x, n_bins=n_bins)\n",
    "    H_y, _ = compute_entropy_continuous(y, n_bins=n_bins)\n",
    "    \n",
    "    # Convert to specified base if needed\n",
    "    if base != 2.0:\n",
    "        H_x = H_x * np.log(2) / np.log(base)\n",
    "        H_y = H_y * np.log(2) / np.log(base)\n",
    "    \n",
    "    # Compute joint entropy\n",
    "    H_xy = compute_joint_entropy(x, y, n_bins, base)\n",
    "    \n",
    "    # MI = H(X) + H(Y) - H(X,Y)\n",
    "    mi = H_x + H_y - H_xy\n",
    "    \n",
    "    # Ensure non-negative (can be slightly negative due to estimation)\n",
    "    return max(0.0, float(mi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec644817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify our MI calculation with different formulas\n",
    "\n",
    "MI_formula1 = H_x + H_y - H_xy  # Definition 2\n",
    "MI_formula2 = H_x - H_x_given_y  # Definition 3a\n",
    "MI_formula3 = H_y - H_y_given_x  # Definition 3b\n",
    "MI_function = compute_mutual_information(x, y, n_bins=n_bins)\n",
    "\n",
    "print(\"ðŸ“Š Verification: All formulas give the same MI\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  H(X) + H(Y) - H(X,Y)  = {MI_formula1:.4f} bits\")\n",
    "print(f\"  H(X) - H(X|Y)         = {MI_formula2:.4f} bits\")\n",
    "print(f\"  H(Y) - H(Y|X)         = {MI_formula3:.4f} bits\")\n",
    "print(f\"  compute_mutual_information() = {MI_function:.4f} bits\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nâœ“ All formulas are equivalent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae987b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: MI vs correlation strength\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "correlations = np.linspace(0, 0.99, 20)\n",
    "mi_values = []\n",
    "\n",
    "for corr in correlations:\n",
    "    x_temp = np.random.randn(n_samples)\n",
    "    y_temp = corr * x_temp + np.sqrt(1 - corr**2) * np.random.randn(n_samples)\n",
    "    mi = compute_mutual_information(x_temp, y_temp, n_bins=20)\n",
    "    mi_values.append(mi)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(correlations, mi_values, color=COLORS[\"signal_1\"], linewidth=2.5, marker=\"o\", markersize=6)\n",
    "ax.set_xlabel(\"Correlation (r)\", fontsize=12)\n",
    "ax.set_ylabel(\"Mutual Information (bits)\", fontsize=12)\n",
    "ax.set_title(\"MI Increases with Statistical Dependency\", fontsize=14, fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, max(mi_values) * 1.1)\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate(\"Independent\\n(r=0, MIâ‰ˆ0)\", xy=(0.05, mi_values[1]), xytext=(0.2, 0.3),\n",
    "            fontsize=10, arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "ax.annotate(\"Strong dependency\\n(high r, high MI)\", xy=(0.9, mi_values[-2]), xytext=(0.6, mi_values[-2] * 0.7),\n",
    "            fontsize=10, arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ For Gaussian variables, MI and correlation are related:\")\n",
    "print(\"   I(X;Y) = -0.5 Ã— logâ‚‚(1 - rÂ²) for jointly Gaussian X, Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4936f3e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Excellent! We've covered the foundations. Let's continue to the key advantage of MI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nonlinear_relationship(\n",
    "    n_samples: int,\n",
    "    relationship: str = \"quadratic\",\n",
    "    noise_level: float = 0.2,\n",
    "    seed: Optional[int] = None\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Generate X, Y with specified nonlinear relationship.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of samples to generate.\n",
    "    relationship : str, optional\n",
    "        Type of relationship: \"linear\", \"quadratic\", \"sinusoidal\", \n",
    "        \"absolute\", \"circular\". Default is \"quadratic\".\n",
    "    noise_level : float, optional\n",
    "        Standard deviation of additive noise. Default is 0.2.\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[NDArray, NDArray]\n",
    "        (x, y) signal pair.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    x = np.random.uniform(-2, 2, n_samples)\n",
    "    noise = noise_level * np.random.randn(n_samples)\n",
    "    \n",
    "    if relationship == \"linear\":\n",
    "        y = 0.8 * x + noise\n",
    "    elif relationship == \"quadratic\":\n",
    "        y = x**2 + noise\n",
    "    elif relationship == \"sinusoidal\":\n",
    "        y = np.sin(2 * np.pi * x / 2) + noise\n",
    "    elif relationship == \"absolute\":\n",
    "        y = np.abs(x) + noise\n",
    "    elif relationship == \"circular\":\n",
    "        # XOR-like pattern\n",
    "        y = np.sign(x) * np.random.choice([-1, 1], n_samples) + noise\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown relationship: {relationship}\")\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: MI vs Correlation â€” The Key Comparison\n",
    "\n",
    "relationships = [\"linear\", \"quadratic\", \"sinusoidal\"]\n",
    "n_samples = 1000\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "results = []\n",
    "\n",
    "for idx, rel in enumerate(relationships):\n",
    "    x, y = generate_nonlinear_relationship(n_samples, rel, noise_level=0.3, seed=42)\n",
    "    \n",
    "    # Compute metrics\n",
    "    corr = np.corrcoef(x, y)[0, 1]\n",
    "    mi = compute_mutual_information(x, y, n_bins=20)\n",
    "    results.append((rel, corr, mi))\n",
    "    \n",
    "    # Top row: scatter plots\n",
    "    colors_rel = [COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"]]\n",
    "    axes[0, idx].scatter(x, y, alpha=0.4, s=15, color=colors_rel[idx])\n",
    "    axes[0, idx].set_xlabel(\"X\", fontsize=11)\n",
    "    axes[0, idx].set_ylabel(\"Y\", fontsize=11)\n",
    "    axes[0, idx].set_title(f\"{rel.capitalize()}\\nCorr = {corr:.3f}, MI = {mi:.3f}\", \n",
    "                           fontsize=12, fontweight=\"bold\")\n",
    "    axes[0, idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom row: bar chart comparison\n",
    "x_pos = np.arange(len(relationships))\n",
    "width = 0.35\n",
    "\n",
    "corrs = [r[1] for r in results]\n",
    "mis = [r[2] for r in results]\n",
    "\n",
    "axes[1, 0].bar(x_pos - width/2, np.abs(corrs), width, label=\"|Correlation|\", color=COLORS[\"signal_4\"], alpha=0.8)\n",
    "axes[1, 0].bar(x_pos + width/2, mis, width, label=\"MI (bits)\", color=COLORS[\"signal_5\"], alpha=0.8)\n",
    "axes[1, 0].set_xticks(x_pos)\n",
    "axes[1, 0].set_xticklabels([r[0].capitalize() for r in results])\n",
    "axes[1, 0].set_ylabel(\"Value\", fontsize=11)\n",
    "axes[1, 0].set_title(\"Comparison: |Correlation| vs MI\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Highlight the key insight\n",
    "axes[1, 1].text(0.5, 0.7, \"KEY INSIGHT\", fontsize=18, fontweight=\"bold\", ha=\"center\", va=\"center\",\n",
    "                transform=axes[1, 1].transAxes)\n",
    "axes[1, 1].text(0.5, 0.5, \"Quadratic & Sinusoidal:\\nCorrelation â‰ˆ 0\\nbut MI > 0!\", \n",
    "                fontsize=14, ha=\"center\", va=\"center\", transform=axes[1, 1].transAxes,\n",
    "                bbox=dict(boxstyle=\"round\", facecolor=COLORS[\"signal_3\"], alpha=0.3))\n",
    "axes[1, 1].text(0.5, 0.2, \"MI detects these\\nnonlinear relationships!\", \n",
    "                fontsize=12, ha=\"center\", va=\"center\", transform=axes[1, 1].transAxes,\n",
    "                style=\"italic\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "# Summary table\n",
    "axes[1, 2].axis(\"off\")\n",
    "table_data = [[\"Relationship\", \"|Corr|\", \"MI\"]]\n",
    "for rel, corr, mi in results:\n",
    "    table_data.append([rel.capitalize(), f\"{abs(corr):.3f}\", f\"{mi:.3f}\"])\n",
    "\n",
    "table = axes[1, 2].table(cellText=table_data, loc=\"center\", cellLoc=\"center\",\n",
    "                          colWidths=[0.4, 0.3, 0.3])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1.2, 1.8)\n",
    "\n",
    "# Style header row\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor(COLORS[\"signal_1\"])\n",
    "    table[(0, i)].set_text_props(color=\"white\", fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"MI Captures Nonlinear Relationships That Correlation Misses\", \n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key observation:\")\n",
    "print(\"   â€¢ Linear: Both correlation and MI detect the relationship\")\n",
    "print(\"   â€¢ Quadratic: Correlation â‰ˆ 0, but MI clearly shows dependency!\")\n",
    "print(\"   â€¢ Sinusoidal: Same story â€” MI wins for nonlinear relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450feb97",
   "metadata": {},
   "source": [
    "## 8. Normalized Mutual Information\n",
    "\n",
    "Raw MI depends on the entropy of the variables â€” hard to compare across different signals.\n",
    "\n",
    "### Normalization Options\n",
    "\n",
    "| Name | Formula | Range |\n",
    "|------|---------|-------|\n",
    "| Geometric | $\\frac{I(X;Y)}{\\sqrt{H(X) \\cdot H(Y)}}$ | [0, 1] |\n",
    "| Max | $\\frac{I(X;Y)}{\\max(H(X), H(Y))}$ | [0, 1] |\n",
    "| Min | $\\frac{I(X;Y)}{\\min(H(X), H(Y))}$ | [0, 1] |\n",
    "| Arithmetic | $\\frac{2 \\cdot I(X;Y)}{H(X) + H(Y)}$ | [0, 1] |\n",
    "\n",
    "**Normalized MI = 1** means perfect dependency (one determines the other).\n",
    "\n",
    "**Normalized MI = 0** means independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc84699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_mi(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_bins: int = 20,\n",
    "    normalization: str = \"geometric\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute normalized mutual information (range 0-1).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First signal.\n",
    "    y : NDArray[np.float64]\n",
    "        Second signal.\n",
    "    n_bins : int, optional\n",
    "        Number of bins per dimension. Default is 20.\n",
    "    normalization : str, optional\n",
    "        Normalization method: \"geometric\", \"max\", \"min\", \"arithmetic\".\n",
    "        Default is \"geometric\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Normalized MI in range [0, 1].\n",
    "    \"\"\"\n",
    "    mi = compute_mutual_information(x, y, n_bins)\n",
    "    H_x, _ = compute_entropy_continuous(x, n_bins=n_bins)\n",
    "    H_y, _ = compute_entropy_continuous(y, n_bins=n_bins)\n",
    "    \n",
    "    if normalization == \"geometric\":\n",
    "        denom = np.sqrt(H_x * H_y)\n",
    "    elif normalization == \"max\":\n",
    "        denom = max(H_x, H_y)\n",
    "    elif normalization == \"min\":\n",
    "        denom = min(H_x, H_y)\n",
    "    elif normalization == \"arithmetic\":\n",
    "        denom = (H_x + H_y) / 2\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown normalization: {normalization}\")\n",
    "    \n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return min(1.0, mi / denom)  # Clip to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3778a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 8: Raw MI vs Normalized MI\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create signals with different entropy levels but same dependency strength\n",
    "# High entropy signals\n",
    "x_high = np.random.randn(n_samples)\n",
    "y_high = 0.7 * x_high + 0.71 * np.random.randn(n_samples)\n",
    "\n",
    "# Low entropy signals (more peaked distribution)\n",
    "x_low = 0.3 * np.random.randn(n_samples)\n",
    "y_low = 0.7 * x_low + 0.71 * 0.3 * np.random.randn(n_samples)\n",
    "\n",
    "# Compute metrics\n",
    "mi_high = compute_mutual_information(x_high, y_high, n_bins=20)\n",
    "mi_low = compute_mutual_information(x_low, y_low, n_bins=20)\n",
    "nmi_high = compute_normalized_mi(x_high, y_high, n_bins=20)\n",
    "nmi_low = compute_normalized_mi(x_low, y_low, n_bins=20)\n",
    "corr_high = np.corrcoef(x_high, y_high)[0, 1]\n",
    "corr_low = np.corrcoef(x_low, y_low)[0, 1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Scatter plots\n",
    "axes[0].scatter(x_high, y_high, alpha=0.3, s=10, color=COLORS[\"signal_1\"], label=\"High entropy\")\n",
    "axes[0].scatter(x_low, y_low, alpha=0.5, s=10, color=COLORS[\"signal_2\"], label=\"Low entropy\")\n",
    "axes[0].set_xlabel(\"X\", fontsize=11)\n",
    "axes[0].set_ylabel(\"Y\", fontsize=11)\n",
    "axes[0].set_title(\"Same Correlation, Different Entropy\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Raw MI comparison\n",
    "x_pos = [0, 1]\n",
    "axes[1].bar(x_pos, [mi_high, mi_low], color=[COLORS[\"signal_1\"], COLORS[\"signal_2\"]], alpha=0.8)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([\"High Entropy\", \"Low Entropy\"])\n",
    "axes[1].set_ylabel(\"MI (bits)\", fontsize=11)\n",
    "axes[1].set_title(f\"Raw MI: Different Values!\\nHigh={mi_high:.3f}, Low={mi_low:.3f}\", \n",
    "                  fontsize=12, fontweight=\"bold\")\n",
    "axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Normalized MI comparison\n",
    "axes[2].bar(x_pos, [nmi_high, nmi_low], color=[COLORS[\"signal_1\"], COLORS[\"signal_2\"]], alpha=0.8)\n",
    "axes[2].set_xticks(x_pos)\n",
    "axes[2].set_xticklabels([\"High Entropy\", \"Low Entropy\"])\n",
    "axes[2].set_ylabel(\"Normalized MI\", fontsize=11)\n",
    "axes[2].set_title(f\"Normalized MI: Similar!\\nHigh={nmi_high:.3f}, Low={nmi_low:.3f}\", \n",
    "                  fontsize=12, fontweight=\"bold\")\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.suptitle(\"Why Normalize MI?\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Both have similar correlation: {corr_high:.3f} vs {corr_low:.3f}\")\n",
    "print(f\"   Raw MI differs: {mi_high:.3f} vs {mi_low:.3f}\")\n",
    "print(f\"   Normalized MI is comparable: {nmi_high:.3f} vs {nmi_low:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ccde63",
   "metadata": {},
   "source": [
    "## 9. Estimation Challenges\n",
    "\n",
    "MI estimation from finite samples faces several challenges.\n",
    "\n",
    "### Challenge 1: Binning Choice\n",
    "- **Too few bins**: Underestimate MI (lose resolution)\n",
    "- **Too many bins**: Overestimate MI (sparse sampling bias)\n",
    "- Optimal depends on sample size and relationship\n",
    "\n",
    "### Challenge 2: Positive Bias\n",
    "- MI estimates are **biased upward**\n",
    "- Even **independent** signals show positive MI due to finite sampling!\n",
    "- More bins = more bias\n",
    "\n",
    "### Challenge 3: Computational Cost\n",
    "- 2D histograms scale with binsÂ²\n",
    "- For n channels: nÂ² pairs to compute\n",
    "\n",
    "### Solutions\n",
    "- Adaptive binning rules\n",
    "- **Surrogate-based bias correction**\n",
    "- KNN-based estimators (more advanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 9: Bias demonstration\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate INDEPENDENT signals\n",
    "x_indep = np.random.randn(n_samples)\n",
    "y_indep = np.random.randn(n_samples)\n",
    "\n",
    "# Compute MI with different bin counts\n",
    "bin_counts = [5, 10, 15, 20, 30, 50, 75, 100]\n",
    "mi_values_bias = []\n",
    "\n",
    "for n_bins in bin_counts:\n",
    "    mi = compute_mutual_information(x_indep, y_indep, n_bins=n_bins)\n",
    "    mi_values_bias.append(mi)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: MI vs bins for independent signals\n",
    "axes[0].plot(bin_counts, mi_values_bias, color=COLORS[\"signal_1\"], linewidth=2.5, marker=\"o\", markersize=8)\n",
    "axes[0].axhline(0, color=COLORS[\"grid\"], linestyle=\"--\", linewidth=2, label=\"True MI = 0\")\n",
    "axes[0].set_xlabel(\"Number of Bins\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Estimated MI (bits)\", fontsize=12)\n",
    "axes[0].set_title(\"Bias: Independent Signals Show Positive MI!\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "axes[0].annotate(\"More bins = more bias!\", xy=(75, mi_values_bias[-2]), \n",
    "                 xytext=(50, mi_values_bias[-2] + 0.1),\n",
    "                 fontsize=11, arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "\n",
    "# Right: scatter plot showing they ARE independent\n",
    "axes[1].scatter(x_indep, y_indep, alpha=0.4, s=20, color=COLORS[\"signal_2\"])\n",
    "axes[1].set_xlabel(\"X\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Y\", fontsize=12)\n",
    "corr_indep = np.corrcoef(x_indep, y_indep)[0, 1]\n",
    "axes[1].set_title(f\"These ARE Independent!\\nCorr = {corr_indep:.3f}\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"The Positive Bias Problem in MI Estimation\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nâš ï¸ WARNING: MI estimated from independent signals is NOT zero!\")\n",
    "print(f\"   With 20 bins: MI = {mi_values_bias[3]:.4f} bits\")\n",
    "print(f\"   With 100 bins: MI = {mi_values_bias[-1]:.4f} bits\")\n",
    "print(\"   This is BIAS from finite sampling â€” we need to correct for it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba4a67",
   "metadata": {},
   "source": [
    "## 10. Surrogate Testing for MI\n",
    "\n",
    "Just like in C03 (Statistical Significance), we use **surrogates** to:\n",
    "1. Test if MI is significantly different from what we'd expect by chance\n",
    "2. Estimate and correct for bias\n",
    "\n",
    "### Procedure\n",
    "\n",
    "1. Compute observed MI\n",
    "2. Generate N surrogates by **shuffling** one signal (destroys dependency while preserving marginal distribution)\n",
    "3. Compute MI for each surrogate\n",
    "4. **P-value** = proportion of surrogates â‰¥ observed MI\n",
    "5. **Bias correction**: MI_corrected = MI_observed - mean(MI_surrogates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b76a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mi_significance_test(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    n_bins: int = 20,\n",
    "    n_surrogates: int = 200,\n",
    "    seed: Optional[int] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Significance test for mutual information using surrogates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First signal.\n",
    "    y : NDArray[np.float64]\n",
    "        Second signal.\n",
    "    n_bins : int, optional\n",
    "        Number of bins. Default is 20.\n",
    "    n_surrogates : int, optional\n",
    "        Number of surrogate samples. Default is 200.\n",
    "    seed : int, optional\n",
    "        Random seed for reproducibility.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, float]\n",
    "        Dictionary with:\n",
    "        - \"mi_observed\": Raw MI value\n",
    "        - \"mi_corrected\": Bias-corrected MI\n",
    "        - \"pvalue\": P-value from surrogate test\n",
    "        - \"null_mean\": Mean of null distribution\n",
    "        - \"null_std\": Std of null distribution\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Observed MI\n",
    "    mi_observed = compute_mutual_information(x, y, n_bins)\n",
    "    \n",
    "    # Generate surrogates\n",
    "    mi_surrogates = []\n",
    "    for _ in range(n_surrogates):\n",
    "        # Shuffle one signal to destroy dependency\n",
    "        y_shuffled = np.random.permutation(y)\n",
    "        mi_surr = compute_mutual_information(x, y_shuffled, n_bins)\n",
    "        mi_surrogates.append(mi_surr)\n",
    "    \n",
    "    mi_surrogates = np.array(mi_surrogates)\n",
    "    \n",
    "    # Statistics\n",
    "    null_mean = np.mean(mi_surrogates)\n",
    "    null_std = np.std(mi_surrogates)\n",
    "    \n",
    "    # P-value: proportion of surrogates >= observed\n",
    "    pvalue = np.mean(mi_surrogates >= mi_observed)\n",
    "    \n",
    "    # Bias-corrected MI\n",
    "    mi_corrected = mi_observed - null_mean\n",
    "    \n",
    "    return {\n",
    "        \"mi_observed\": float(mi_observed),\n",
    "        \"mi_corrected\": float(mi_corrected),\n",
    "        \"pvalue\": float(pvalue),\n",
    "        \"null_mean\": float(null_mean),\n",
    "        \"null_std\": float(null_std)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81870311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 10: Surrogate testing demonstration\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Case 1: Correlated signals (should be significant)\n",
    "x_corr = np.random.randn(n_samples)\n",
    "y_corr = 0.6 * x_corr + 0.8 * np.random.randn(n_samples)\n",
    "\n",
    "# Case 2: Independent signals (should NOT be significant)\n",
    "x_indep = np.random.randn(n_samples)\n",
    "y_indep = np.random.randn(n_samples)\n",
    "\n",
    "# Run significance tests\n",
    "result_corr = mi_significance_test(x_corr, y_corr, n_bins=20, n_surrogates=500, seed=42)\n",
    "result_indep = mi_significance_test(x_indep, y_indep, n_bins=20, n_surrogates=500, seed=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Generate surrogate distributions for visualization\n",
    "np.random.seed(42)\n",
    "surr_corr = [compute_mutual_information(x_corr, np.random.permutation(y_corr), 20) for _ in range(500)]\n",
    "surr_indep = [compute_mutual_information(x_indep, np.random.permutation(y_indep), 20) for _ in range(500)]\n",
    "\n",
    "# Left: Correlated signals\n",
    "axes[0].hist(surr_corr, bins=30, color=COLORS[\"signal_1\"], alpha=0.7, density=True, label=\"Null distribution\")\n",
    "axes[0].axvline(result_corr[\"mi_observed\"], color=\"red\", linewidth=3, linestyle=\"-\", \n",
    "                label=f\"Observed MI = {result_corr['mi_observed']:.3f}\")\n",
    "axes[0].axvline(result_corr[\"null_mean\"], color=COLORS[\"grid\"], linewidth=2, linestyle=\"--\",\n",
    "                label=f\"Null mean = {result_corr['null_mean']:.3f}\")\n",
    "axes[0].set_xlabel(\"MI (bits)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Density\", fontsize=12)\n",
    "axes[0].set_title(f\"Correlated Signals\\np = {result_corr['pvalue']:.4f} (SIGNIFICANT)\", \n",
    "                  fontsize=12, fontweight=\"bold\", color=\"green\")\n",
    "axes[0].legend(fontsize=9)\n",
    "\n",
    "# Right: Independent signals\n",
    "axes[1].hist(surr_indep, bins=30, color=COLORS[\"signal_2\"], alpha=0.7, density=True, label=\"Null distribution\")\n",
    "axes[1].axvline(result_indep[\"mi_observed\"], color=\"red\", linewidth=3, linestyle=\"-\",\n",
    "                label=f\"Observed MI = {result_indep['mi_observed']:.3f}\")\n",
    "axes[1].axvline(result_indep[\"null_mean\"], color=COLORS[\"grid\"], linewidth=2, linestyle=\"--\",\n",
    "                label=f\"Null mean = {result_indep['null_mean']:.3f}\")\n",
    "axes[1].set_xlabel(\"MI (bits)\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Density\", fontsize=12)\n",
    "axes[1].set_title(f\"Independent Signals\\np = {result_indep['pvalue']:.3f} (not significant)\", \n",
    "                  fontsize=12, fontweight=\"bold\", color=\"gray\")\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "plt.suptitle(\"Surrogate Testing for MI Significance\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nðŸ“Š Results Summary:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Correlated signals:\")\n",
    "print(f\"  MI_observed = {result_corr['mi_observed']:.4f}, MI_corrected = {result_corr['mi_corrected']:.4f}\")\n",
    "print(f\"  p-value = {result_corr['pvalue']:.4f} â†’ {'SIGNIFICANT' if result_corr['pvalue'] < 0.05 else 'not significant'}\")\n",
    "print(f\"\\\\nIndependent signals:\")\n",
    "print(f\"  MI_observed = {result_indep['mi_observed']:.4f}, MI_corrected = {result_indep['mi_corrected']:.4f}\")\n",
    "print(f\"  p-value = {result_indep['pvalue']:.4f} â†’ {'SIGNIFICANT' if result_indep['pvalue'] < 0.05 else 'not significant'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1eda87",
   "metadata": {},
   "source": [
    "## 11. MI for Time Series â€” Dynamic Analysis\n",
    "\n",
    "Neural signals are **time series**, not static samples. We can compute MI in different ways:\n",
    "\n",
    "### Option 1: Global MI\n",
    "Treat each time point as a sample. Simple, assumes stationarity.\n",
    "\n",
    "### Option 2: Sliding Window MI\n",
    "Compute MI in short windows â†’ get MI over time. Captures **dynamic changes** in coupling.\n",
    "\n",
    "### Option 3: Time-Lagged MI\n",
    "MI between X(t) and Y(t + Ï„). Can reveal **delayed relationships**.\n",
    "\n",
    "This is a preview of **Transfer Entropy** (D03)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi_sliding_window(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    window_samples: int,\n",
    "    step_samples: int,\n",
    "    n_bins: int = 15\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute MI in sliding windows over time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First time series.\n",
    "    y : NDArray[np.float64]\n",
    "        Second time series.\n",
    "    window_samples : int\n",
    "        Window size in samples.\n",
    "    step_samples : int\n",
    "        Step size in samples.\n",
    "    n_bins : int, optional\n",
    "        Number of bins for MI estimation. Default is 15.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[NDArray, NDArray]\n",
    "        (window_centers, mi_values)\n",
    "    \"\"\"\n",
    "    n_samples = len(x)\n",
    "    centers = []\n",
    "    mi_values = []\n",
    "    \n",
    "    for start in range(0, n_samples - window_samples + 1, step_samples):\n",
    "        end = start + window_samples\n",
    "        mi = compute_mutual_information(x[start:end], y[start:end], n_bins)\n",
    "        centers.append((start + end) / 2)\n",
    "        mi_values.append(mi)\n",
    "    \n",
    "    return np.array(centers), np.array(mi_values)\n",
    "\n",
    "\n",
    "def compute_mi_lagged(\n",
    "    x: NDArray[np.float64],\n",
    "    y: NDArray[np.float64],\n",
    "    max_lag_samples: int,\n",
    "    n_bins: int = 20\n",
    ") -> Tuple[NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute MI as function of time lag.\n",
    "    \n",
    "    MI(X(t), Y(t+lag)) for various lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : NDArray[np.float64]\n",
    "        First time series.\n",
    "    y : NDArray[np.float64]\n",
    "        Second time series.\n",
    "    max_lag_samples : int\n",
    "        Maximum lag in samples (both positive and negative).\n",
    "    n_bins : int, optional\n",
    "        Number of bins. Default is 20.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[NDArray, NDArray]\n",
    "        (lags, mi_values)\n",
    "    \"\"\"\n",
    "    lags = np.arange(-max_lag_samples, max_lag_samples + 1)\n",
    "    mi_values = []\n",
    "    \n",
    "    for lag in lags:\n",
    "        if lag < 0:\n",
    "            # Y leads X\n",
    "            x_aligned = x[-lag:]\n",
    "            y_aligned = y[:lag]\n",
    "        elif lag > 0:\n",
    "            # X leads Y\n",
    "            x_aligned = x[:-lag]\n",
    "            y_aligned = y[lag:]\n",
    "        else:\n",
    "            x_aligned = x\n",
    "            y_aligned = y\n",
    "        \n",
    "        mi = compute_mutual_information(x_aligned, y_aligned, n_bins)\n",
    "        mi_values.append(mi)\n",
    "    \n",
    "    return lags, np.array(mi_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281617a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 11: Time-varying MI\n",
    "\n",
    "np.random.seed(42)\n",
    "fs = 256\n",
    "duration = 15  # seconds\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "n_samples = len(t)\n",
    "\n",
    "# Create signals with time-varying coupling\n",
    "# 0-5s: independent\n",
    "# 5-10s: coupled\n",
    "# 10-15s: independent again\n",
    "\n",
    "x = np.random.randn(n_samples)\n",
    "y = np.zeros(n_samples)\n",
    "\n",
    "# Independent phase 1 (0-5s)\n",
    "idx1 = t < 5\n",
    "y[idx1] = np.random.randn(np.sum(idx1))\n",
    "\n",
    "# Coupled phase (5-10s)\n",
    "idx2 = (t >= 5) & (t < 10)\n",
    "y[idx2] = 0.7 * x[idx2] + 0.71 * np.random.randn(np.sum(idx2))\n",
    "\n",
    "# Independent phase 2 (10-15s)\n",
    "idx3 = t >= 10\n",
    "y[idx3] = np.random.randn(np.sum(idx3))\n",
    "\n",
    "# Compute sliding window MI\n",
    "window_sec = 2  # 2 second window\n",
    "step_sec = 0.25  # 250ms step\n",
    "window_samples = int(window_sec * fs)\n",
    "step_samples = int(step_sec * fs)\n",
    "\n",
    "centers, mi_time = compute_mi_sliding_window(x, y, window_samples, step_samples, n_bins=15)\n",
    "time_centers = centers / fs\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot signals\n",
    "axes[0].plot(t, x, color=COLORS[\"signal_1\"], linewidth=0.5, alpha=0.8, label=\"X\")\n",
    "axes[0].set_ylabel(\"X\", fontsize=12)\n",
    "axes[0].set_title(\"Signal X\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend(loc=\"upper right\")\n",
    "\n",
    "axes[1].plot(t, y, color=COLORS[\"signal_2\"], linewidth=0.5, alpha=0.8, label=\"Y\")\n",
    "axes[1].set_ylabel(\"Y\", fontsize=12)\n",
    "axes[1].set_title(\"Signal Y (coupled to X during 5-10s)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend(loc=\"upper right\")\n",
    "\n",
    "# Highlight coupling period\n",
    "for ax in axes[:2]:\n",
    "    ax.axvspan(5, 10, alpha=0.2, color=COLORS[\"signal_3\"], label=\"Coupled period\")\n",
    "\n",
    "# Plot MI over time\n",
    "axes[2].plot(time_centers, mi_time, color=COLORS[\"signal_3\"], linewidth=2.5)\n",
    "axes[2].axvspan(5, 10, alpha=0.2, color=COLORS[\"signal_3\"])\n",
    "axes[2].set_xlabel(\"Time (s)\", fontsize=12)\n",
    "axes[2].set_ylabel(\"MI (bits)\", fontsize=12)\n",
    "axes[2].set_title(\"Sliding Window MI â€” Detects Dynamic Coupling!\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "axes[2].annotate(\"Independent\", xy=(2.5, np.mean(mi_time[:10])), fontsize=11, ha=\"center\")\n",
    "axes[2].annotate(\"COUPLED\", xy=(7.5, np.max(mi_time)), fontsize=11, ha=\"center\", fontweight=\"bold\", color=\"red\")\n",
    "axes[2].annotate(\"Independent\", xy=(12.5, np.mean(mi_time[-10:])), fontsize=11, ha=\"center\")\n",
    "\n",
    "plt.suptitle(\"Time-Resolved MI Reveals Dynamic Changes in Coupling\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nðŸ“Š MI clearly increases during the coupled period (5-10s)!\")\n",
    "print(\"   This shows MI can track DYNAMIC changes in statistical dependency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ecbc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_lagged_mi(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    max_lag: int,\n",
    "    n_bins: int = 20\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute MI between two signals at different time lags.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        First signal.\n",
    "    y : np.ndarray\n",
    "        Second signal.\n",
    "    max_lag : int\n",
    "        Maximum lag in samples (both positive and negative).\n",
    "    n_bins : int\n",
    "        Number of bins for histogram estimation.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lags : np.ndarray\n",
    "        Array of lag values (negative = X leads, positive = Y leads).\n",
    "    mi_values : np.ndarray\n",
    "        MI values at each lag.\n",
    "    \"\"\"\n",
    "    lags = np.arange(-max_lag, max_lag + 1)\n",
    "    mi_values = np.zeros(len(lags))\n",
    "    \n",
    "    for i, lag in enumerate(lags):\n",
    "        if lag < 0:\n",
    "            x_shifted = x[:lag]  # X leads\n",
    "            y_shifted = y[-lag:]\n",
    "        elif lag > 0:\n",
    "            x_shifted = x[lag:]  # Y leads\n",
    "            y_shifted = y[:-lag]\n",
    "        else:\n",
    "            x_shifted = x\n",
    "            y_shifted = y\n",
    "        \n",
    "        mi_values[i] = compute_mutual_information(x_shifted, y_shifted, n_bins)\n",
    "    \n",
    "    return lags, mi_values\n",
    "\n",
    "\n",
    "# Visualization 12: Time-lagged MI can reveal directionality\n",
    "\n",
    "np.random.seed(42)\n",
    "fs = 256\n",
    "duration = 10\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "\n",
    "# Create signals with X leading Y by ~20ms (5 samples at 256 Hz)\n",
    "x = np.random.randn(len(t))\n",
    "# Low-pass filter to create temporal structure\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "x = gaussian_filter1d(x, sigma=5)\n",
    "\n",
    "# Y follows X with a delay\n",
    "delay_samples = 5\n",
    "y = np.zeros_like(x)\n",
    "y[delay_samples:] = 0.8 * x[:-delay_samples] + 0.4 * np.random.randn(len(x) - delay_samples)\n",
    "\n",
    "# Compute time-lagged MI\n",
    "max_lag = 50  # ~200ms\n",
    "lags, mi_lagged = compute_time_lagged_mi(x, y, max_lag, n_bins=20)\n",
    "lags_ms = lags * 1000 / fs  # Convert to ms\n",
    "\n",
    "# Find peak lag\n",
    "peak_idx = np.argmax(mi_lagged)\n",
    "peak_lag_ms = lags_ms[peak_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Time series\n",
    "axes[0].plot(t[:500], x[:500], color=COLORS[\"signal_1\"], linewidth=1.5, label=\"X (driver)\")\n",
    "axes[0].plot(t[:500], y[:500], color=COLORS[\"signal_2\"], linewidth=1.5, label=\"Y (follower)\")\n",
    "axes[0].set_xlabel(\"Time (s)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Amplitude\", fontsize=12)\n",
    "axes[0].set_title(\"X Drives Y with ~20ms Delay\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Time-lagged MI\n",
    "axes[1].plot(lags_ms, mi_lagged, color=COLORS[\"signal_3\"], linewidth=2.5)\n",
    "axes[1].axvline(x=0, color=COLORS[\"grid\"], linestyle=\"--\", alpha=0.7, label=\"Zero lag\")\n",
    "axes[1].axvline(x=peak_lag_ms, color=\"red\", linestyle=\"-\", linewidth=2, \n",
    "                label=f\"Peak: {peak_lag_ms:.1f} ms\")\n",
    "axes[1].fill_between(lags_ms, mi_lagged, alpha=0.3, color=COLORS[\"signal_3\"])\n",
    "axes[1].set_xlabel(\"Lag (ms) â€” Negative = X leads\", fontsize=12)\n",
    "axes[1].set_ylabel(\"MI (bits)\", fontsize=12)\n",
    "axes[1].set_title(\"Time-Lagged MI Reveals Directionality\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "expected_delay = delay_samples * 1000 / fs\n",
    "print(f\"\\\\nðŸŽ¯ Expected delay: {expected_delay:.1f} ms\")\n",
    "print(f\"   Detected peak lag: {peak_lag_ms:.1f} ms\")\n",
    "print(\"\\\\nðŸ’¡ The peak at NEGATIVE lag indicates X leads Y.\")\n",
    "print(\"   This is a simple form of 'directionality' analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b94a0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 12. MI Connectivity Matrix ðŸ”—\n",
    "\n",
    "Just like we computed **entropy for multiple signals** in D01, we can compute MI between **all pairs** of signals to build a **connectivity matrix**.\n",
    "\n",
    "This is essential for hyperscanning where we want to measure statistical dependencies between multiple EEG channels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mi_matrix(\n",
    "    signals: np.ndarray,\n",
    "    n_bins: int = 20,\n",
    "    normalize: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute MI connectivity matrix for multiple signals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signals : np.ndarray\n",
    "        2D array of shape (n_channels, n_samples).\n",
    "    n_bins : int\n",
    "        Number of bins for histogram estimation.\n",
    "    normalize : bool\n",
    "        If True, normalize MI to [0, 1] range.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mi_matrix : np.ndarray\n",
    "        Symmetric MI matrix of shape (n_channels, n_channels).\n",
    "    \"\"\"\n",
    "    n_channels = signals.shape[0]\n",
    "    mi_matrix = np.zeros((n_channels, n_channels))\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        for j in range(i + 1, n_channels):\n",
    "            mi = compute_mutual_information(signals[i], signals[j], n_bins)\n",
    "            \n",
    "            if normalize:\n",
    "                # Normalized MI - compute_entropy_continuous returns (entropy, n_bins)\n",
    "                h_i, _ = compute_entropy_continuous(signals[i], n_bins)\n",
    "                h_j, _ = compute_entropy_continuous(signals[j], n_bins)\n",
    "                if h_i > 0 and h_j > 0:\n",
    "                    mi = 2 * mi / (h_i + h_j)\n",
    "            \n",
    "            mi_matrix[i, j] = mi\n",
    "            mi_matrix[j, i] = mi\n",
    "    \n",
    "    return mi_matrix\n",
    "\n",
    "\n",
    "# Visualization 13: MI connectivity matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "n_channels = 8\n",
    "n_samples = 2048\n",
    "\n",
    "# Create signals with cluster structure\n",
    "# Cluster 1: channels 0, 1, 2 (coupled)\n",
    "# Cluster 2: channels 4, 5, 6 (coupled)\n",
    "# Channels 3 and 7: independent\n",
    "\n",
    "signals = np.random.randn(n_channels, n_samples)\n",
    "\n",
    "# Add coupling within clusters\n",
    "base_1 = np.random.randn(n_samples)\n",
    "base_2 = np.random.randn(n_samples)\n",
    "\n",
    "signals[0] += 2 * base_1\n",
    "signals[1] += 2 * base_1 + 0.5 * np.random.randn(n_samples)\n",
    "signals[2] += 2 * base_1 + 0.5 * np.random.randn(n_samples)\n",
    "\n",
    "signals[4] += 2 * base_2\n",
    "signals[5] += 2 * base_2 + 0.5 * np.random.randn(n_samples)\n",
    "signals[6] += 2 * base_2 + 0.5 * np.random.randn(n_samples)\n",
    "\n",
    "# Compute MI matrix\n",
    "mi_matrix = compute_mi_matrix(signals, n_bins=20, normalize=True)\n",
    "\n",
    "# Create labels\n",
    "channel_labels = [f\"Ch{i}\" for i in range(n_channels)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(mi_matrix, cmap=\"RdYlBu_r\", vmin=0, vmax=1)\n",
    "cbar = plt.colorbar(im, ax=ax, label=\"Normalized MI\", shrink=0.8)\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_channels):\n",
    "    for j in range(n_channels):\n",
    "        if i != j:\n",
    "            text = ax.text(j, i, f\"{mi_matrix[i, j]:.2f}\",\n",
    "                          ha=\"center\", va=\"center\", fontsize=9,\n",
    "                          color=\"white\" if mi_matrix[i, j] > 0.5 else \"black\")\n",
    "\n",
    "ax.set_xticks(range(n_channels))\n",
    "ax.set_yticks(range(n_channels))\n",
    "ax.set_xticklabels(channel_labels)\n",
    "ax.set_yticklabels(channel_labels)\n",
    "ax.set_xlabel(\"Channel\", fontsize=12)\n",
    "ax.set_ylabel(\"Channel\", fontsize=12)\n",
    "ax.set_title(\"MI Connectivity Matrix â€” Two Clusters Detected!\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add cluster annotations\n",
    "ax.add_patch(plt.Rectangle((-0.5, -0.5), 3, 3, fill=False, \n",
    "                           edgecolor=COLORS[\"signal_1\"], linewidth=3, label=\"Cluster 1\"))\n",
    "ax.add_patch(plt.Rectangle((3.5, 3.5), 3, 3, fill=False, \n",
    "                           edgecolor=COLORS[\"signal_2\"], linewidth=3, label=\"Cluster 2\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”— The MI matrix reveals the TRUE connectivity structure:\")\n",
    "print(\"   - Cluster 1: Ch0, Ch1, Ch2 (high within-cluster MI)\")\n",
    "print(\"   - Cluster 2: Ch4, Ch5, Ch6 (high within-cluster MI)\")\n",
    "print(\"   - Ch3 and Ch7: independent nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffd3a06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Application: Hyperscanning Inter-Brain MI ðŸ§ â†”ï¸ðŸ§ \n",
    "\n",
    "In **hyperscanning**, we record EEG from **two or more people** simultaneously. MI can measure **information sharing** between their brain signals!\n",
    "\n",
    "**Key insight**: Unlike correlation, MI can capture:\n",
    "- Non-linear neural coupling\n",
    "- Complex social interactions\n",
    "- Implicit communication patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c5a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 14: Inter-brain MI in hyperscanning scenario\n",
    "\n",
    "np.random.seed(42)\n",
    "fs = 256\n",
    "duration = 60  # 60 seconds\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "n_samples = len(t)\n",
    "\n",
    "# Simulate EEG from two subjects (3 channels each)\n",
    "# Scenario: Cooperative task with phases\n",
    "# 0-20s: Independent (baseline)\n",
    "# 20-40s: Cooperative task (inter-brain coupling)\n",
    "# 40-60s: Independent (rest)\n",
    "\n",
    "n_channels_per_subject = 3\n",
    "channel_names = [\"Fz\", \"Cz\", \"Pz\"]\n",
    "\n",
    "# Generate base signals (alpha oscillations ~10 Hz)\n",
    "def generate_eeg_alpha(n_samples: int, fs: int) -> np.ndarray:\n",
    "    \"\"\"Generate simulated alpha band EEG.\"\"\"\n",
    "    t = np.arange(n_samples) / fs\n",
    "    alpha = np.sin(2 * np.pi * 10 * t + np.random.uniform(0, 2*np.pi))\n",
    "    noise = np.random.randn(n_samples) * 0.5\n",
    "    return alpha + noise\n",
    "\n",
    "# Subject 1 signals\n",
    "subject1 = np.zeros((n_channels_per_subject, n_samples))\n",
    "for ch in range(n_channels_per_subject):\n",
    "    subject1[ch] = generate_eeg_alpha(n_samples, fs)\n",
    "\n",
    "# Subject 2 signals - coupled during task phase\n",
    "subject2 = np.zeros((n_channels_per_subject, n_samples))\n",
    "for ch in range(n_channels_per_subject):\n",
    "    # Independent phases\n",
    "    subject2[ch, :20*fs] = generate_eeg_alpha(20*fs, fs)\n",
    "    subject2[ch, 40*fs:] = generate_eeg_alpha(20*fs, fs)\n",
    "    \n",
    "    # Coupled phase - share some common information\n",
    "    coupled_base = subject1[ch, 20*fs:40*fs]\n",
    "    subject2[ch, 20*fs:40*fs] = (0.6 * coupled_base + \n",
    "                                  0.8 * generate_eeg_alpha(20*fs, fs))\n",
    "\n",
    "# Compute inter-brain MI in sliding windows\n",
    "window_samples = 5 * fs  # 5 second window\n",
    "step_samples = 1 * fs    # 1 second step\n",
    "\n",
    "def compute_interbrain_mi_timecourse(s1: np.ndarray, s2: np.ndarray, \n",
    "                                      window: int, step: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute mean inter-brain MI over time.\"\"\"\n",
    "    n_channels = s1.shape[0]\n",
    "    n_windows = (s1.shape[1] - window) // step + 1\n",
    "    \n",
    "    times = np.zeros(n_windows)\n",
    "    mi_timecourse = np.zeros(n_windows)\n",
    "    \n",
    "    for w in range(n_windows):\n",
    "        start = w * step\n",
    "        end = start + window\n",
    "        times[w] = (start + end) / 2 / fs\n",
    "        \n",
    "        # Compute MI for all inter-brain pairs and average\n",
    "        mi_sum = 0\n",
    "        n_pairs = 0\n",
    "        for i in range(n_channels):\n",
    "            for j in range(n_channels):\n",
    "                mi = compute_mutual_information(s1[i, start:end], \n",
    "                                               s2[j, start:end], n_bins=15)\n",
    "                mi_sum += mi\n",
    "                n_pairs += 1\n",
    "        \n",
    "        mi_timecourse[w] = mi_sum / n_pairs\n",
    "    \n",
    "    return times, mi_timecourse\n",
    "\n",
    "times, mi_timecourse = compute_interbrain_mi_timecourse(subject1, subject2, \n",
    "                                                         window_samples, step_samples)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Subject 1 EEG (one channel)\n",
    "ax = axes[0]\n",
    "ax.plot(t, subject1[1], color=COLORS[\"signal_1\"], linewidth=0.5, alpha=0.8)\n",
    "ax.set_ylabel(\"Subject 1\\\\nCz\", fontsize=12)\n",
    "ax.set_title(\"Subject 1 EEG\", fontsize=12, fontweight=\"bold\")\n",
    "ax.axvspan(20, 40, alpha=0.2, color=COLORS[\"signal_3\"])\n",
    "ax.set_xlim([0, 60])\n",
    "\n",
    "# Subject 2 EEG (one channel)\n",
    "ax = axes[1]\n",
    "ax.plot(t, subject2[1], color=COLORS[\"signal_2\"], linewidth=0.5, alpha=0.8)\n",
    "ax.set_ylabel(\"Subject 2\\\\nCz\", fontsize=12)\n",
    "ax.set_title(\"Subject 2 EEG\", fontsize=12, fontweight=\"bold\")\n",
    "ax.axvspan(20, 40, alpha=0.2, color=COLORS[\"signal_3\"], label=\"Cooperative task\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlim([0, 60])\n",
    "\n",
    "# Inter-brain MI\n",
    "ax = axes[2]\n",
    "ax.plot(times, mi_timecourse, color=COLORS[\"signal_3\"], linewidth=2.5)\n",
    "ax.fill_between(times, mi_timecourse, alpha=0.3, color=COLORS[\"signal_3\"])\n",
    "ax.axvspan(20, 40, alpha=0.2, color=COLORS[\"signal_3\"])\n",
    "ax.axhline(y=np.mean(mi_timecourse[:15]), color=COLORS[\"grid\"], linestyle=\"--\", \n",
    "           label=\"Baseline level\")\n",
    "ax.set_xlabel(\"Time (s)\", fontsize=12)\n",
    "ax.set_ylabel(\"Mean Inter-Brain MI\", fontsize=12)\n",
    "ax.set_title(\"Inter-Brain MI Increases During Cooperation!\", fontsize=12, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim([0, 60])\n",
    "\n",
    "plt.suptitle(\"Hyperscanning: MI Detects Inter-Brain Coupling During Social Interaction\",\n",
    "             fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "mi_baseline = np.mean(np.concatenate([mi_timecourse[:15], mi_timecourse[-15:]]))\n",
    "mi_task = np.mean(mi_timecourse[18:38])\n",
    "increase = ((mi_task - mi_baseline) / mi_baseline) * 100\n",
    "\n",
    "print(f\"\\\\nðŸ“Š Inter-Brain MI Analysis:\")\n",
    "print(f\"   Baseline MI: {mi_baseline:.4f} bits\")\n",
    "print(f\"   Task MI:     {mi_task:.4f} bits\")\n",
    "print(f\"   Increase:    {increase:.1f}%\")\n",
    "print(\"\\\\nðŸ§  This demonstrates how MI can track inter-brain coupling during social tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3accda6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 14. Exercises ðŸ“\n",
    "\n",
    "Test your understanding of Mutual Information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Relationship between MI and relationship type\n",
    "# =========================================================\n",
    "# Create three pairs of signals:\n",
    "# 1. Linear relationship: Y = 2*X + noise\n",
    "# 2. Quadratic relationship: Y = X^2 + noise\n",
    "# 3. Circular relationship: Y = sin(X) + cos(X) + noise\n",
    "#\n",
    "# Compute MI and correlation for each. Which metric captures non-linear dependencies better?\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "x = np.random.randn(n)\n",
    "\n",
    "# Linear\n",
    "y_linear = 2 * x + 0.5 * np.random.randn(n)\n",
    "\n",
    "# Quadratic\n",
    "y_quadratic = x**2 + 0.5 * np.random.randn(n)\n",
    "\n",
    "# Circular (use x in radians)\n",
    "x_rad = np.random.uniform(-np.pi, np.pi, n)\n",
    "y_circular = np.sin(x_rad) + np.cos(x_rad) + 0.3 * np.random.randn(n)\n",
    "\n",
    "# Compute MI and correlation for each pair\n",
    "results_ex1 = []\n",
    "\n",
    "for name, x_sig, y_sig in [(\"Linear\", x, y_linear), \n",
    "                            (\"Quadratic\", x, y_quadratic), \n",
    "                            (\"Circular\", x_rad, y_circular)]:\n",
    "    mi = compute_mutual_information(x_sig, y_sig, n_bins=20)\n",
    "    corr = np.abs(np.corrcoef(x_sig, y_sig)[0, 1])\n",
    "    results_ex1.append({\"Relationship\": name, \"MI\": mi, \"|Correlation|\": corr})\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“Š Exercise 1: MI vs Correlation for Different Relationships\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Relationship':<15} {'MI (bits)':<15} {'|Correlation|':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for r in results_ex1:\n",
    "    print(f\"{r['Relationship']:<15} {r['MI']:<15.4f} {r['|Correlation|']:<15.4f}\")\n",
    "print(\"-\" * 60)\n",
    "print(\"\\nðŸ’¡ Key insight:\")\n",
    "print(\"   - Correlation captures LINEAR relationships well\")\n",
    "print(\"   - MI captures ALL relationships (linear AND nonlinear)\")\n",
    "print(\"   - Quadratic: correlation â‰ˆ 0, but MI is HIGH!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ac0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Effect of binning on MI estimation\n",
    "# ================================================\n",
    "# Using two coupled signals, compute MI with different numbers of bins:\n",
    "# [5, 10, 20, 50, 100, 200]\n",
    "#\n",
    "# Plot MI as a function of number of bins. What do you observe?\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "x = np.random.randn(n)\n",
    "y = 0.5 * x + 0.87 * np.random.randn(n)  # True correlation = 0.5\n",
    "\n",
    "n_bins_list = [5, 10, 20, 50, 100, 200]\n",
    "mi_values_ex2 = []\n",
    "\n",
    "for n_bins in n_bins_list:\n",
    "    mi = compute_mutual_information(x, y, n_bins)\n",
    "    mi_values_ex2.append(mi)\n",
    "\n",
    "# Plot MI vs n_bins\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(n_bins_list, mi_values_ex2, \"o-\", color=COLORS[\"signal_1\"], \n",
    "        linewidth=2, markersize=10)\n",
    "ax.axhline(y=mi_values_ex2[2], color=COLORS[\"grid\"], linestyle=\"--\", \n",
    "           label=f\"Reference (20 bins): {mi_values_ex2[2]:.4f}\")\n",
    "\n",
    "ax.set_xlabel(\"Number of Bins\", fontsize=12)\n",
    "ax.set_ylabel(\"Estimated MI (bits)\", fontsize=12)\n",
    "ax.set_title(\"Exercise 2: Effect of Binning on MI Estimation\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Add annotations\n",
    "for i, (nb, mi) in enumerate(zip(n_bins_list, mi_values_ex2)):\n",
    "    ax.annotate(f\"{mi:.3f}\", (nb, mi), textcoords=\"offset points\", \n",
    "                xytext=(0, 10), ha=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key observations:\")\n",
    "print(\"   - Too few bins (5): UNDERESTIMATES MI (discretization too coarse)\")\n",
    "print(\"   - Too many bins (200): BIAS increases (sparse histogram)\")\n",
    "print(\"   - Sweet spot: ~20-50 bins for n=1000 samples\")\n",
    "print(\"\\n   Rule of thumb: n_bins â‰ˆ âˆš(n_samples) or Sturges' formula\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b0ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Conditional MI\n",
    "# ============================\n",
    "# Conditional MI measures: I(X; Y | Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)\n",
    "# This tells us how much information X and Y share BEYOND what Z provides.\n",
    "#\n",
    "# Create three signals:\n",
    "# - Z: Common driver signal\n",
    "# - X = Z + noise_1\n",
    "# - Y = Z + noise_2\n",
    "#\n",
    "# Compare I(X; Y) with the expected behavior when conditioning on Z.\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 2000\n",
    "\n",
    "z = np.random.randn(n)  # Common driver\n",
    "x = z + 0.3 * np.random.randn(n)\n",
    "y = z + 0.3 * np.random.randn(n)\n",
    "\n",
    "# Unconditional MI between X and Y\n",
    "mi_xy = compute_mutual_information(x, y, n_bins=20)\n",
    "\n",
    "# Correlation between X, Y, and Z\n",
    "corr_xy = np.corrcoef(x, y)[0, 1]\n",
    "corr_xz = np.corrcoef(x, z)[0, 1]\n",
    "corr_yz = np.corrcoef(y, z)[0, 1]\n",
    "\n",
    "# For demonstration: compute MI after \"regressing out\" Z\n",
    "# This is a simplified approximation of conditional MI\n",
    "x_residual = x - np.polyval(np.polyfit(z, x, 1), z)\n",
    "y_residual = y - np.polyval(np.polyfit(z, y, 1), z)\n",
    "mi_xy_given_z_approx = compute_mutual_information(x_residual, y_residual, n_bins=20)\n",
    "\n",
    "print(\"ðŸ“Š Exercise 3: Conditional MI â€” Detecting Spurious Correlations\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"\\nCorrelations:\")\n",
    "print(f\"   r(X, Y) = {corr_xy:.4f}  â† High! But is it genuine?\")\n",
    "print(f\"   r(X, Z) = {corr_xz:.4f}  â† X follows Z\")\n",
    "print(f\"   r(Y, Z) = {corr_yz:.4f}  â† Y follows Z\")\n",
    "\n",
    "print(f\"\\nMutual Information:\")\n",
    "print(f\"   I(X; Y)     = {mi_xy:.4f} bits  â† Unconditional MI\")\n",
    "print(f\"   I(X; Y | Z) â‰ˆ {mi_xy_given_z_approx:.4f} bits  â† After removing Z influence\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key insight:\")\n",
    "print(\"   X and Y appear highly dependent (high I(X;Y))\")\n",
    "print(\"   But this is because BOTH depend on Z!\")\n",
    "print(\"   After conditioning on Z, the dependency (almost) disappears.\")\n",
    "print(\"\\nðŸ” This is the 'confounding variable' problem!\")\n",
    "print(\"   Conditional MI helps detect when apparent dependencies are spurious.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b40e2f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 15. Summary ðŸ“‹\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Formula | Meaning |\n",
    "|---------|---------|---------|\n",
    "| **Joint Entropy** | $H(X,Y) = -\\sum p(x,y) \\log_2 p(x,y)$ | Total uncertainty of both variables |\n",
    "| **Conditional Entropy** | $H(Y\\|X) = H(X,Y) - H(X)$ | Uncertainty in Y given X |\n",
    "| **Mutual Information** | $I(X;Y) = H(X) + H(Y) - H(X,Y)$ | Shared information |\n",
    "| **Normalized MI** | $NMI = \\frac{2 \\cdot I(X;Y)}{H(X) + H(Y)}$ | Bounded [0, 1] |\n",
    "\n",
    "### MI vs Correlation\n",
    "\n",
    "| Property | Correlation | Mutual Information |\n",
    "|----------|-------------|-------------------|\n",
    "| Range | [-1, 1] | [0, âˆž) |\n",
    "| Linear relationships | âœ“ | âœ“ |\n",
    "| Non-linear relationships | âœ— | âœ“ |\n",
    "| Interpretation | Direction + strength | Information shared |\n",
    "| Estimation | Simple | Requires binning/KNN |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **MI captures ALL dependencies**: Unlike correlation, MI detects any statistical relationship\n",
    "2. **Symmetric but not directional**: $I(X;Y) = I(Y;X)$ â€” use Transfer Entropy for directionality\n",
    "3. **Estimation matters**: Too few bins â†’ underestimate, too many â†’ bias/variance issues\n",
    "4. **Use surrogates**: Always validate significance with shuffled surrogates\n",
    "5. **Time-varying MI**: Sliding windows reveal dynamic coupling changes\n",
    "6. **Perfect for hyperscanning**: Captures complex inter-brain dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3656122",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 16. Discussion & Next Steps ðŸš€\n",
    "\n",
    "### Discussion Questions\n",
    "\n",
    "1. **Why might MI be preferred over correlation for EEG analysis?**\n",
    "   - Neural communication often involves non-linear dynamics\n",
    "   - Phase-amplitude coupling is inherently non-linear\n",
    "   - Information theory provides interpretable units (bits)\n",
    "\n",
    "2. **What are the limitations of histogram-based MI estimation?**\n",
    "   - Curse of dimensionality for multivariate data\n",
    "   - Bin size selection is somewhat arbitrary\n",
    "   - May require many samples for reliable estimates\n",
    "\n",
    "3. **How does MI relate to other connectivity metrics?**\n",
    "   - Coherence: captures linear frequency-specific dependencies\n",
    "   - Phase-Locking Value: captures phase synchronization\n",
    "   - MI: captures all statistical dependencies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook (**D03 - Transfer Entropy**), we'll learn:\n",
    "- How to measure **directed** information flow\n",
    "- The concept of **causal** coupling\n",
    "- Applications to detecting leader-follower dynamics in hyperscanning\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- Cover, T. M., & Thomas, J. A. (2006). *Elements of Information Theory*\n",
    "- Kraskov, A., et al. (2004). Estimating mutual information. *Physical Review E*\n",
    "- Jeong, J., et al. (2001). Mutual information analysis of EEG. *Clinical Neurophysiology*\n",
    "\n",
    "---\n",
    "\n",
    "**Estimated time**: 70 minutes\n",
    "\n",
    "**Prerequisites completed**: D01 (Entropy and Information)\n",
    "\n",
    "**Next notebook**: D03 - Transfer Entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectivity-metrics-tutorials-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
