{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af38adc0",
   "metadata": {},
   "source": [
    "## 1. Introduction â€” What is Information?\n",
    "\n",
    "In everyday language, \"information\" means facts, data, or knowledge. But in mathematics and signal processing, **information** has a precise, quantifiable meaning: it represents the **reduction of uncertainty**.\n",
    "\n",
    "This fundamental insight comes from **Claude Shannon's** groundbreaking 1948 paper \"A Mathematical Theory of Communication\", which established that:\n",
    "\n",
    "1. **Information can be quantified** â€” measured in precise units called **bits**\n",
    "2. **Information = Surprise** â€” the less expected an event, the more information it carries\n",
    "3. **Uncertainty can be measured** â€” via a quantity called **entropy**\n",
    "\n",
    "### Why Does This Matter for Neuroscience?\n",
    "\n",
    "Information theory provides powerful tools for analyzing neural signals:\n",
    "\n",
    "- **How much information** does a brain signal carry?\n",
    "- **How much information is shared** between two signals? â†’ This leads to connectivity measures!\n",
    "- **How much information flows** from one signal to another? â†’ This reveals causality!\n",
    "\n",
    "In this notebook, we'll build the foundational concepts: **entropy** and **information content**. These are the building blocks for mutual information (D02) and transfer entropy (D03), which are powerful connectivity metrics.\n",
    "\n",
    "> ðŸ’¡ **Key insight**: Information = Surprise. The less expected something is, the more information it carries when it occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff773f5",
   "metadata": {},
   "source": [
    "## 2. Intuition â€” Surprise and Uncertainty\n",
    "\n",
    "Before diving into mathematics, let's build intuition with thought experiments.\n",
    "\n",
    "### Thought Experiment 1: Weather Forecasts\n",
    "\n",
    "Consider the statement \"Tomorrow will be sunny\":\n",
    "\n",
    "- **In the Sahara desert**: This is expected â†’ carries **little information**\n",
    "- **In London**: This is less expected â†’ carries **more information**\n",
    "- **\"Tomorrow will snow\" in the Sahara**: Extremely surprising â†’ carries **a lot of information**!\n",
    "\n",
    "The key principle: **Rare events carry more information than common events.**\n",
    "\n",
    "### Thought Experiment 2: Coin Flips\n",
    "\n",
    "- **Fair coin** (50% heads, 50% tails): The outcome is maximally uncertain â†’ each flip provides **maximum information**\n",
    "- **Biased coin** (99% heads, 1% tails): The outcome is predictable â†’ each flip provides **little information** (usually just confirms what we expected)\n",
    "\n",
    "### The Connection\n",
    "\n",
    "- **Uncertainty** = how spread out the possibilities are\n",
    "- **Information** = how much uncertainty is reduced by an observation\n",
    "\n",
    "Now let's see how to **quantify** this precisely!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.typing import NDArray\n",
    "from typing import Tuple, Optional, Union, List\n",
    "from scipy import stats\n",
    "from scipy.signal import welch\n",
    "import sys\n",
    "sys.path.append(\"../../..\")\n",
    "\n",
    "from src.colors import COLORS\n",
    "from src.plotting import configure_plots\n",
    "\n",
    "configure_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Distributions with different uncertainty\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Distribution 1: Uniform (maximum uncertainty)\n",
    "probs_uniform = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "axes[0].bar(range(5), probs_uniform, color=COLORS[\"signal_1\"], edgecolor=\"white\", linewidth=2)\n",
    "axes[0].set_title(\"Uniform Distribution\\n(Maximum Uncertainty)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Outcome\")\n",
    "axes[0].set_ylabel(\"Probability\")\n",
    "axes[0].set_ylim(0, 0.8)\n",
    "axes[0].set_xticks(range(5))\n",
    "axes[0].set_xticklabels([\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "\n",
    "# Distribution 2: Slightly peaked (moderate uncertainty)\n",
    "probs_moderate = np.array([0.1, 0.15, 0.5, 0.15, 0.1])\n",
    "axes[1].bar(range(5), probs_moderate, color=COLORS[\"signal_2\"], edgecolor=\"white\", linewidth=2)\n",
    "axes[1].set_title(\"Peaked Distribution\\n(Moderate Uncertainty)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Outcome\")\n",
    "axes[1].set_ylim(0, 0.8)\n",
    "axes[1].set_xticks(range(5))\n",
    "axes[1].set_xticklabels([\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "\n",
    "# Distribution 3: Highly peaked (low uncertainty)\n",
    "probs_peaked = np.array([0.02, 0.03, 0.9, 0.03, 0.02])\n",
    "axes[2].bar(range(5), probs_peaked, color=COLORS[\"signal_3\"], edgecolor=\"white\", linewidth=2)\n",
    "axes[2].set_title(\"Highly Peaked Distribution\\n(Low Uncertainty)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[2].set_xlabel(\"Outcome\")\n",
    "axes[2].set_ylim(0, 0.8)\n",
    "axes[2].set_xticks(range(5))\n",
    "axes[2].set_xticklabels([\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "\n",
    "plt.suptitle(\"Which distribution has the most uncertainty?\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Answer: The UNIFORM distribution (left) has maximum uncertainty.\")\n",
    "print(\"When all outcomes are equally likely, we have no way to predict what will happen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c1764",
   "metadata": {},
   "source": [
    "## 3. Shannon Entropy â€” The Formula\n",
    "\n",
    "**Shannon entropy** quantifies the uncertainty of a random variable. For a discrete random variable $X$ with possible outcomes $x_1, x_2, ..., x_n$ and probabilities $p(x_i)$:\n",
    "\n",
    "$$H(X) = -\\sum_{i=1}^{n} p(x_i) \\log p(x_i)$$\n",
    "\n",
    "### Understanding the Components\n",
    "\n",
    "- **$p(x_i)$**: Probability of outcome $x_i$\n",
    "- **$\\log p(x_i)$**: Negative (since $p \\leq 1$), so the negative sign makes $H$ positive\n",
    "- **Sum**: Average over all possible outcomes, weighted by probability\n",
    "\n",
    "### Units\n",
    "\n",
    "- **Base 2** (logâ‚‚): Entropy in **bits** â€” most common in information theory\n",
    "- **Base e** (ln): Entropy in **nats** â€” common in physics and machine learning\n",
    "- Conversion: 1 nat â‰ˆ 1.44 bits\n",
    "\n",
    "### Convention\n",
    "\n",
    "When $p = 0$, we define $0 \\times \\log(0) = 0$ (the limit as $p \\to 0$).\n",
    "\n",
    "### Interpretations\n",
    "\n",
    "1. **Average surprise**: $H(X)$ = expected value of \"surprise\" $(-\\log p)$\n",
    "2. **Minimum encoding length**: Average bits needed to encode outcomes\n",
    "3. **Uncertainty measure**: How unpredictable is $X$ before we observe it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7af29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Contribution of each probability to entropy\n",
    "\n",
    "p_values = np.linspace(0.001, 0.999, 500)\n",
    "contribution = -p_values * np.log2(p_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(p_values, contribution, color=COLORS[\"signal_1\"], linewidth=2.5)\n",
    "ax.fill_between(p_values, contribution, alpha=0.3, color=COLORS[\"signal_1\"])\n",
    "\n",
    "# Mark maximum\n",
    "p_max = 1 / np.e  # â‰ˆ 0.368\n",
    "contrib_max = -p_max * np.log2(p_max)\n",
    "ax.axvline(p_max, color=COLORS[\"grid\"], linestyle=\"--\", linewidth=1.5, alpha=0.7)\n",
    "ax.scatter([p_max], [contrib_max], color=COLORS[\"signal_2\"], s=100, zorder=5)\n",
    "ax.annotate(f\"Maximum at p = 1/e â‰ˆ {p_max:.3f}\", xy=(p_max, contrib_max),\n",
    "            xytext=(p_max + 0.15, contrib_max),\n",
    "            fontsize=11, fontweight=\"bold\",\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "\n",
    "ax.set_xlabel(\"Probability p\", fontsize=12)\n",
    "ax.set_ylabel(\"-p Ã— logâ‚‚(p)\", fontsize=12)\n",
    "ax.set_title(\"Contribution of Each Probability to Total Entropy\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 0.55)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each outcome contributes -p Ã— logâ‚‚(p) to the total entropy.\")\n",
    "print(\"Very rare events (pâ‰ˆ0) and very common events (pâ‰ˆ1) contribute little.\")\n",
    "print(\"Events with intermediate probability contribute the most!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d395b6",
   "metadata": {},
   "source": [
    "## 4. Computing Entropy â€” Examples\n",
    "\n",
    "Let's compute entropy for concrete examples to build understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_discrete(\n",
    "    probabilities: NDArray[np.float64],\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute Shannon entropy of a discrete probability distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    probabilities : NDArray[np.float64]\n",
    "        Probability distribution (must sum to 1).\n",
    "    base : float, optional\n",
    "        Logarithm base. Use 2 for bits, np.e for nats. Default is 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Shannon entropy of the distribution.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> compute_entropy_discrete(np.array([0.5, 0.5]))\n",
    "    1.0  # Fair coin = 1 bit\n",
    "    \"\"\"\n",
    "    # Ensure probabilities sum to 1 (with tolerance)\n",
    "    assert np.abs(np.sum(probabilities) - 1.0) < 1e-9, \"Probabilities must sum to 1\"\n",
    "    \n",
    "    # Filter out zeros to avoid log(0)\n",
    "    p = probabilities[probabilities > 0]\n",
    "    \n",
    "    # Compute entropy\n",
    "    if base == np.e:\n",
    "        entropy = -np.sum(p * np.log(p))\n",
    "    else:\n",
    "        entropy = -np.sum(p * np.log(p) / np.log(base))\n",
    "    \n",
    "    return float(entropy)\n",
    "\n",
    "\n",
    "def compute_entropy_from_counts(\n",
    "    counts: NDArray[np.int64],\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Estimate entropy from observed counts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    counts : NDArray[np.int64]\n",
    "        Count of observations for each outcome.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2 (bits).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Estimated Shannon entropy.\n",
    "    \"\"\"\n",
    "    # Convert counts to probabilities\n",
    "    total = np.sum(counts)\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    probabilities = counts / total\n",
    "    return compute_entropy_discrete(probabilities, base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b4eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Fair coin\n",
    "p_fair_coin = np.array([0.5, 0.5])\n",
    "H_fair = compute_entropy_discrete(p_fair_coin)\n",
    "print(f\"Example 1: Fair coin\")\n",
    "print(f\"  P(heads) = P(tails) = 0.5\")\n",
    "print(f\"  H = {H_fair:.4f} bits\")\n",
    "print(f\"  â†’ Maximum entropy for 2 outcomes!\\n\")\n",
    "\n",
    "# Example 2: Biased coin (90% heads)\n",
    "p_biased = np.array([0.9, 0.1])\n",
    "H_biased = compute_entropy_discrete(p_biased)\n",
    "print(f\"Example 2: Biased coin (90% heads)\")\n",
    "print(f\"  P(heads) = 0.9, P(tails) = 0.1\")\n",
    "print(f\"  H = {H_biased:.4f} bits\")\n",
    "print(f\"  â†’ Less entropy (more predictable)\\n\")\n",
    "\n",
    "# Example 3: Fair die (6 sides)\n",
    "p_die = np.ones(6) / 6\n",
    "H_die = compute_entropy_discrete(p_die)\n",
    "print(f\"Example 3: Fair die (6 sides)\")\n",
    "print(f\"  P(each side) = 1/6\")\n",
    "print(f\"  H = {H_die:.4f} bits = logâ‚‚(6)\")\n",
    "print(f\"  â†’ Maximum entropy for 6 outcomes!\\n\")\n",
    "\n",
    "# Example 4: Loaded die\n",
    "p_loaded = np.array([0.4, 0.2, 0.15, 0.1, 0.1, 0.05])\n",
    "H_loaded = compute_entropy_discrete(p_loaded)\n",
    "print(f\"Example 4: Loaded die\")\n",
    "print(f\"  Non-uniform probabilities\")\n",
    "print(f\"  H = {H_loaded:.4f} bits\")\n",
    "print(f\"  â†’ Less than fair die ({H_die:.4f} bits)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Comparing distributions and their entropy\n",
    "\n",
    "examples = [\n",
    "    (\"Fair Coin\", np.array([0.5, 0.5]), [\"H\", \"T\"]),\n",
    "    (\"Biased Coin\\n(90% heads)\", np.array([0.9, 0.1]), [\"H\", \"T\"]),\n",
    "    (\"Fair Die\", np.ones(6)/6, [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]),\n",
    "    (\"Loaded Die\", np.array([0.4, 0.2, 0.15, 0.1, 0.1, 0.05]), [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "colors = [COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"], COLORS[\"signal_4\"]]\n",
    "\n",
    "for ax, (title, probs, labels), color in zip(axes, examples, colors):\n",
    "    H = compute_entropy_discrete(probs)\n",
    "    ax.bar(range(len(probs)), probs, color=color, edgecolor=\"white\", linewidth=2)\n",
    "    ax.set_title(f\"{title}\\nH = {H:.3f} bits\", fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Outcome\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(range(len(probs)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.axhline(1/len(probs), color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Uniform\")\n",
    "\n",
    "plt.suptitle(\"Entropy Reflects How 'Spread Out' a Distribution Is\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key observation: The more uniform (spread out) the distribution,\")\n",
    "print(\"   the HIGHER the entropy. Maximum entropy = uniform distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfbcec3",
   "metadata": {},
   "source": [
    "## 5. Maximum Entropy\n",
    "\n",
    "For $n$ possible outcomes, the **maximum entropy** is achieved when all outcomes are equally likely (uniform distribution):\n",
    "\n",
    "$$H_{max} = \\log(n)$$\n",
    "\n",
    "This makes intuitive sense: we're most uncertain when we have no reason to expect any outcome over another.\n",
    "\n",
    "### Normalized Entropy\n",
    "\n",
    "To compare entropies across systems with different numbers of states, we can **normalize**:\n",
    "\n",
    "$$H_{normalized} = \\frac{H}{H_{max}} = \\frac{H}{\\log(n)}$$\n",
    "\n",
    "- Range: 0 (deterministic) to 1 (maximum uncertainty)\n",
    "- Useful for comparing \"how random\" different systems are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_max_entropy(n_states: int, base: float = 2.0) -> float:\n",
    "    \"\"\"\n",
    "    Compute maximum possible entropy for n states.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_states : int\n",
    "        Number of possible states/outcomes.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2 (bits).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Maximum entropy = log(n_states).\n",
    "    \"\"\"\n",
    "    if n_states <= 0:\n",
    "        raise ValueError(\"n_states must be positive\")\n",
    "    \n",
    "    if base == np.e:\n",
    "        return float(np.log(n_states))\n",
    "    else:\n",
    "        return float(np.log(n_states) / np.log(base))\n",
    "\n",
    "\n",
    "def compute_normalized_entropy(\n",
    "    probabilities: NDArray[np.float64],\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute entropy normalized by maximum (range 0-1).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    probabilities : NDArray[np.float64]\n",
    "        Probability distribution.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Normalized entropy in range [0, 1].\n",
    "    \"\"\"\n",
    "    n_states = len(probabilities)\n",
    "    if n_states <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    H = compute_entropy_discrete(probabilities, base)\n",
    "    H_max = compute_max_entropy(n_states, base)\n",
    "    \n",
    "    return H / H_max if H_max > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5773485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Entropy vs distribution \"peakedness\"\n",
    "\n",
    "# Create distributions from uniform to very peaked using Dirichlet\n",
    "n_states = 5\n",
    "concentration_params = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 50.0]\n",
    "\n",
    "# For reproducibility, we'll create peaked distributions analytically\n",
    "def create_peaked_distribution(n: int, peak_strength: float) -> NDArray:\n",
    "    \"\"\"Create distribution with controllable peakedness.\"\"\"\n",
    "    if peak_strength == 0:\n",
    "        return np.ones(n) / n\n",
    "    weights = np.exp(-peak_strength * np.abs(np.arange(n) - n//2))\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "peak_values = np.linspace(0, 5, 50)\n",
    "entropies = []\n",
    "normalized_entropies = []\n",
    "\n",
    "for peak in peak_values:\n",
    "    probs = create_peaked_distribution(n_states, peak)\n",
    "    entropies.append(compute_entropy_discrete(probs))\n",
    "    normalized_entropies.append(compute_normalized_entropy(probs))\n",
    "\n",
    "H_max = compute_max_entropy(n_states)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Entropy vs peakedness\n",
    "axes[0].plot(peak_values, entropies, color=COLORS[\"signal_1\"], linewidth=2.5, label=\"Entropy H\")\n",
    "axes[0].axhline(H_max, color=COLORS[\"grid\"], linestyle=\"--\", linewidth=2, label=f\"H_max = {H_max:.3f} bits\")\n",
    "axes[0].axhline(0, color=\"gray\", linestyle=\":\", linewidth=1)\n",
    "axes[0].set_xlabel(\"Peakedness (concentration)\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Entropy (bits)\", fontsize=12)\n",
    "axes[0].set_title(\"Entropy Decreases as Distribution Becomes Peaked\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(-0.1, H_max + 0.3)\n",
    "\n",
    "# Right: Example distributions\n",
    "peak_examples = [0, 1, 3, 5]\n",
    "colors_ex = [COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"], COLORS[\"signal_4\"]]\n",
    "for peak, color in zip(peak_examples, colors_ex):\n",
    "    probs = create_peaked_distribution(n_states, peak)\n",
    "    H = compute_entropy_discrete(probs)\n",
    "    offset = peak_examples.index(peak) * 0.15\n",
    "    axes[1].bar(np.arange(n_states) + offset, probs, width=0.15, color=color, \n",
    "                label=f\"peak={peak}, H={H:.2f}\", alpha=0.8)\n",
    "\n",
    "axes[1].set_xlabel(\"Outcome\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Probability\", fontsize=12)\n",
    "axes[1].set_title(\"Distribution Shapes at Different Peakedness\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].set_xticks(np.arange(n_states) + 0.225)\n",
    "axes[1].set_xticklabels([\"A\", \"B\", \"C\", \"D\", \"E\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f6acce",
   "metadata": {},
   "source": [
    "## 6. Binary Entropy Function\n",
    "\n",
    "A special and commonly used case is the **binary entropy function**: the entropy of a Bernoulli variable (two outcomes) as a function of a single probability $p$:\n",
    "\n",
    "$$H(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$$\n",
    "\n",
    "### Properties\n",
    "\n",
    "- **H(0) = H(1) = 0**: Deterministic outcomes â†’ no uncertainty\n",
    "- **H(0.5) = 1 bit**: Maximum uncertainty for binary variable\n",
    "- **Symmetric**: H(p) = H(1-p)\n",
    "\n",
    "This function is useful for understanding information in yes/no questions and binary classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49616526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_entropy(p: Union[float, NDArray[np.float64]]) -> Union[float, NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute binary entropy function H(p) in bits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    p : float or NDArray\n",
    "        Probability value(s) in range [0, 1].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float or NDArray\n",
    "        Binary entropy H(p) = -p*log2(p) - (1-p)*log2(1-p).\n",
    "    \"\"\"\n",
    "    p = np.asarray(p)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    result = np.zeros_like(p, dtype=float)\n",
    "    \n",
    "    # Only compute for valid probabilities (not 0 or 1)\n",
    "    valid = (p > 0) & (p < 1)\n",
    "    p_valid = p[valid]\n",
    "    result[valid] = -p_valid * np.log2(p_valid) - (1 - p_valid) * np.log2(1 - p_valid)\n",
    "    \n",
    "    # Return scalar if input was scalar\n",
    "    if result.ndim == 0:\n",
    "        return float(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b04b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 5: Binary entropy function\n",
    "\n",
    "p_values = np.linspace(0, 1, 500)\n",
    "H_binary = binary_entropy(p_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.plot(p_values, H_binary, color=COLORS[\"signal_1\"], linewidth=3)\n",
    "ax.fill_between(p_values, H_binary, alpha=0.2, color=COLORS[\"signal_1\"])\n",
    "\n",
    "# Mark key points\n",
    "ax.scatter([0.5], [1.0], color=COLORS[\"signal_2\"], s=150, zorder=5, edgecolor=\"white\", linewidth=2)\n",
    "ax.annotate(\"Maximum: H(0.5) = 1 bit\", xy=(0.5, 1.0), xytext=(0.65, 0.9),\n",
    "            fontsize=11, fontweight=\"bold\",\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "\n",
    "# Mark zero points\n",
    "ax.scatter([0, 1], [0, 0], color=COLORS[\"signal_3\"], s=100, zorder=5)\n",
    "ax.annotate(\"H(0) = 0\\n(certain outcome)\", xy=(0, 0), xytext=(0.08, 0.15), fontsize=10)\n",
    "ax.annotate(\"H(1) = 0\\n(certain outcome)\", xy=(1, 0), xytext=(0.75, 0.15), fontsize=10)\n",
    "\n",
    "ax.set_xlabel(\"Probability p (of one outcome)\", fontsize=12)\n",
    "ax.set_ylabel(\"Binary Entropy H(p) [bits]\", fontsize=12)\n",
    "ax.set_title(\"Binary Entropy Function: Uncertainty in Yes/No Questions\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlim(-0.02, 1.02)\n",
    "ax.set_ylim(-0.05, 1.1)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add symmetry annotation\n",
    "ax.annotate(\"\", xy=(0.3, 0.88), xytext=(0.7, 0.88),\n",
    "            arrowprops=dict(arrowstyle=\"<->\", color=\"gray\", lw=1.5))\n",
    "ax.text(0.5, 0.92, \"Symmetric: H(p) = H(1-p)\", ha=\"center\", fontsize=10, color=\"gray\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ A fair coin flip gives exactly 1 bit of information.\")\n",
    "print(\"   This is why 'bit' is the fundamental unit of information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd419cdd",
   "metadata": {},
   "source": [
    "## 7. Entropy of Continuous Variables\n",
    "\n",
    "Neural signals like EEG are **continuous** â€” they can take any value in a range. How do we compute entropy for continuous variables?\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Continuous variables have infinitely many possible values. We can't directly sum over infinite outcomes!\n",
    "\n",
    "### Solution 1: Discretization (Binning)\n",
    "\n",
    "The practical approach:\n",
    "1. Divide the value range into **bins**\n",
    "2. Count samples in each bin\n",
    "3. Convert counts to probabilities\n",
    "4. Compute discrete entropy\n",
    "\n",
    "This is the most common approach for neural signal analysis.\n",
    "\n",
    "### Solution 2: Differential Entropy\n",
    "\n",
    "The theoretical continuous analogue:\n",
    "\n",
    "$$h(X) = -\\int p(x) \\log p(x) \\, dx$$\n",
    "\n",
    "âš ï¸ **Important differences** from discrete entropy:\n",
    "- Can be **negative** (unlike discrete entropy)\n",
    "- Depends on the **units** of measurement\n",
    "- Not directly comparable across different variable types\n",
    "\n",
    "For most practical neural signal analysis, we use the **binning approach**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c2c780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 6: Continuous distribution and discretization\n",
    "\n",
    "np.random.seed(42)\n",
    "continuous_data = np.random.normal(0, 1, 5000)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Continuous distribution (KDE)\n",
    "x_range = np.linspace(-4, 4, 200)\n",
    "kde = stats.gaussian_kde(continuous_data)\n",
    "axes[0].fill_between(x_range, kde(x_range), alpha=0.5, color=COLORS[\"signal_1\"])\n",
    "axes[0].plot(x_range, kde(x_range), color=COLORS[\"signal_1\"], linewidth=2)\n",
    "axes[0].set_xlabel(\"Value\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Probability Density\", fontsize=12)\n",
    "axes[0].set_title(\"Continuous Distribution\\n(Infinitely many possible values)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[0].set_xlim(-4, 4)\n",
    "\n",
    "# Right: Discretized (binned) version\n",
    "n_bins = 20\n",
    "counts, bin_edges, _ = axes[1].hist(continuous_data, bins=n_bins, color=COLORS[\"signal_2\"], \n",
    "                                     edgecolor=\"white\", linewidth=1, alpha=0.8, density=True)\n",
    "axes[1].set_xlabel(\"Value\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Probability Density\", fontsize=12)\n",
    "axes[1].set_title(f\"Discretized into {n_bins} Bins\\n(Finite number of states)\", fontsize=12, fontweight=\"bold\")\n",
    "axes[1].set_xlim(-4, 4)\n",
    "\n",
    "# Add annotation\n",
    "axes[1].annotate(\"Each bin becomes\\na discrete state\", xy=(1.5, 0.1), xytext=(2.5, 0.2),\n",
    "                 fontsize=10, arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
    "\n",
    "plt.suptitle(\"Binning Converts Continuous to Discrete\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ecff8",
   "metadata": {},
   "source": [
    "## 8. Binning Strategies\n",
    "\n",
    "The choice of binning strategy critically affects entropy estimation. Let's explore the options.\n",
    "\n",
    "### The Binning Process\n",
    "\n",
    "1. **Choose number of bins** (critical choice!)\n",
    "2. **Define bin edges** (uniform width or adaptive)\n",
    "3. **Count samples** in each bin\n",
    "4. **Convert counts to probabilities**\n",
    "5. **Compute discrete entropy**\n",
    "\n",
    "### Binning Methods\n",
    "\n",
    "1. **Uniform width**: Equal-sized bins across data range (most common)\n",
    "2. **Uniform count (equiprobable)**: Bins with equal number of samples\n",
    "3. **Adaptive**: Data-driven bin edges (e.g., based on data structure)\n",
    "\n",
    "### Choosing Number of Bins\n",
    "\n",
    "- **Too few bins**: Lose information, underestimate entropy\n",
    "- **Too many bins**: Sparse bins, biased estimation, overestimate entropy\n",
    "\n",
    "Common rules of thumb:\n",
    "- **âˆšn rule**: bins = âˆš(number of samples)\n",
    "- **Sturges' rule**: bins = 1 + logâ‚‚(n)\n",
    "- **Freedman-Diaconis**: bins based on interquartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_n_bins(n_samples: int, method: str = \"sturges\") -> int:\n",
    "    \"\"\"\n",
    "    Compute optimal number of bins for entropy estimation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Number of data samples.\n",
    "    method : str, optional\n",
    "        Method for determining bins: \"sturges\", \"sqrt\", \"rice\".\n",
    "        Default is \"sturges\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Recommended number of bins.\n",
    "    \"\"\"\n",
    "    if n_samples <= 0:\n",
    "        raise ValueError(\"n_samples must be positive\")\n",
    "    \n",
    "    if method == \"sturges\":\n",
    "        return max(1, int(np.ceil(1 + np.log2(n_samples))))\n",
    "    elif method == \"sqrt\":\n",
    "        return max(1, int(np.ceil(np.sqrt(n_samples))))\n",
    "    elif method == \"rice\":\n",
    "        return max(1, int(np.ceil(2 * n_samples ** (1/3))))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Use 'sturges', 'sqrt', or 'rice'.\")\n",
    "\n",
    "\n",
    "def compute_entropy_continuous(\n",
    "    signal: NDArray[np.float64],\n",
    "    n_bins: Union[int, str] = \"auto\",\n",
    "    method: str = \"uniform\"\n",
    ") -> Tuple[float, int]:\n",
    "    \"\"\"\n",
    "    Estimate entropy of a continuous signal via binning.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : NDArray[np.float64]\n",
    "        Continuous signal to analyze.\n",
    "    n_bins : int or str, optional\n",
    "        Number of bins, or \"auto\"/\"sturges\"/\"sqrt\" for automatic.\n",
    "        Default is \"auto\" (uses Sturges' rule).\n",
    "    method : str, optional\n",
    "        Binning method: \"uniform\" (equal width) or \"equiprobable\" (equal count).\n",
    "        Default is \"uniform\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, int]\n",
    "        (entropy in bits, actual number of bins used)\n",
    "    \"\"\"\n",
    "    n_samples = len(signal)\n",
    "    \n",
    "    # Determine number of bins\n",
    "    if isinstance(n_bins, str):\n",
    "        if n_bins == \"auto\" or n_bins == \"sturges\":\n",
    "            actual_bins = optimal_n_bins(n_samples, \"sturges\")\n",
    "        elif n_bins == \"sqrt\":\n",
    "            actual_bins = optimal_n_bins(n_samples, \"sqrt\")\n",
    "        else:\n",
    "            actual_bins = optimal_n_bins(n_samples, \"sturges\")\n",
    "    else:\n",
    "        actual_bins = int(n_bins)\n",
    "    \n",
    "    # Compute histogram\n",
    "    if method == \"uniform\":\n",
    "        counts, _ = np.histogram(signal, bins=actual_bins)\n",
    "    elif method == \"equiprobable\":\n",
    "        # Create bins with equal number of samples\n",
    "        percentiles = np.linspace(0, 100, actual_bins + 1)\n",
    "        bin_edges = np.percentile(signal, percentiles)\n",
    "        counts, _ = np.histogram(signal, bins=bin_edges)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "    \n",
    "    # Compute entropy from counts\n",
    "    entropy = compute_entropy_from_counts(counts)\n",
    "    \n",
    "    return entropy, actual_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 7: Effect of binning on entropy estimation\n",
    "\n",
    "np.random.seed(42)\n",
    "signal = np.random.normal(0, 1, 1000)\n",
    "\n",
    "bin_counts = [5, 20, 50, 200]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, n_bins in zip(axes, bin_counts):\n",
    "    entropy, _ = compute_entropy_continuous(signal, n_bins=n_bins)\n",
    "    H_max = compute_max_entropy(n_bins)\n",
    "    \n",
    "    ax.hist(signal, bins=n_bins, color=COLORS[\"signal_1\"], edgecolor=\"white\", alpha=0.8)\n",
    "    ax.set_title(f\"{n_bins} bins: H = {entropy:.3f} bits (H_max = {H_max:.2f})\", \n",
    "                 fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_xlim(-4, 4)\n",
    "\n",
    "plt.suptitle(\"How Binning Choice Affects Entropy Estimate\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show entropy vs bins\n",
    "print(\"\\nðŸ“Š Entropy vs Number of Bins:\")\n",
    "print(\"-\" * 40)\n",
    "for n_bins in [5, 10, 20, 50, 100, 200]:\n",
    "    H, _ = compute_entropy_continuous(signal, n_bins=n_bins)\n",
    "    H_max = compute_max_entropy(n_bins)\n",
    "    print(f\"  {n_bins:3d} bins: H = {H:.3f} bits  (H_max = {H_max:.2f}, ratio = {H/H_max:.3f})\")\n",
    "\n",
    "print(\"\\nâš ï¸ Note: Entropy increases with more bins, but normalized ratio stays similar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ade304",
   "metadata": {},
   "source": [
    "## 9. Bias in Entropy Estimation\n",
    "\n",
    "Entropy estimated from finite samples is **biased** â€” it tends to **underestimate** the true entropy.\n",
    "\n",
    "### Why the Bias?\n",
    "\n",
    "- With finite samples, we can't observe all possible outcomes\n",
    "- Empty bins are treated as zero probability\n",
    "- Underestimation is more severe with more bins or fewer samples\n",
    "\n",
    "### Miller-Madow Correction\n",
    "\n",
    "A simple bias correction:\n",
    "\n",
    "$$\\hat{H}_{corrected} = \\hat{H} + \\frac{m - 1}{2n \\ln(b)}$$\n",
    "\n",
    "Where:\n",
    "- $m$ = number of bins with non-zero counts\n",
    "- $n$ = total number of samples\n",
    "- $b$ = logarithm base\n",
    "\n",
    "This is a first-order correction that helps reduce bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy_miller_madow(\n",
    "    signal: NDArray[np.float64],\n",
    "    n_bins: int,\n",
    "    base: float = 2.0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute entropy with Miller-Madow bias correction.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : NDArray[np.float64]\n",
    "        Continuous signal to analyze.\n",
    "    n_bins : int\n",
    "        Number of bins for discretization.\n",
    "    base : float, optional\n",
    "        Logarithm base. Default is 2 (bits).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Bias-corrected entropy estimate.\n",
    "    \"\"\"\n",
    "    n_samples = len(signal)\n",
    "    \n",
    "    # Compute histogram\n",
    "    counts, _ = np.histogram(signal, bins=n_bins)\n",
    "    \n",
    "    # Number of non-empty bins\n",
    "    m = np.sum(counts > 0)\n",
    "    \n",
    "    # Raw entropy\n",
    "    H_raw = compute_entropy_from_counts(counts, base)\n",
    "    \n",
    "    # Miller-Madow correction\n",
    "    if base == np.e:\n",
    "        correction = (m - 1) / (2 * n_samples)\n",
    "    else:\n",
    "        correction = (m - 1) / (2 * n_samples * np.log(base))\n",
    "    \n",
    "    return H_raw + correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 8: Bias correction demonstration\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# For uniform distribution on [0, 1] with n_bins uniform bins,\n",
    "# true entropy = log2(n_bins)\n",
    "n_bins_sim = 20\n",
    "true_entropy = np.log2(n_bins_sim)\n",
    "\n",
    "sample_sizes = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "n_trials = 100\n",
    "\n",
    "raw_means, raw_stds = [], []\n",
    "corrected_means, corrected_stds = [], []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    raw_estimates = []\n",
    "    corrected_estimates = []\n",
    "    \n",
    "    for _ in range(n_trials):\n",
    "        data = np.random.uniform(0, 1, n)\n",
    "        H_raw, _ = compute_entropy_continuous(data, n_bins=n_bins_sim)\n",
    "        H_corrected = compute_entropy_miller_madow(data, n_bins_sim)\n",
    "        raw_estimates.append(H_raw)\n",
    "        corrected_estimates.append(H_corrected)\n",
    "    \n",
    "    raw_means.append(np.mean(raw_estimates))\n",
    "    raw_stds.append(np.std(raw_estimates))\n",
    "    corrected_means.append(np.mean(corrected_estimates))\n",
    "    corrected_stds.append(np.std(corrected_estimates))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot raw estimates\n",
    "ax.errorbar(sample_sizes, raw_means, yerr=raw_stds, \n",
    "            color=COLORS[\"signal_1\"], linewidth=2, marker=\"o\", markersize=8,\n",
    "            capsize=5, label=\"Raw estimate (biased)\")\n",
    "\n",
    "# Plot corrected estimates\n",
    "ax.errorbar(sample_sizes, corrected_means, yerr=corrected_stds,\n",
    "            color=COLORS[\"signal_2\"], linewidth=2, marker=\"s\", markersize=8,\n",
    "            capsize=5, label=\"Miller-Madow corrected\")\n",
    "\n",
    "# True value\n",
    "ax.axhline(true_entropy, color=COLORS[\"grid\"], linestyle=\"--\", linewidth=2, \n",
    "           label=f\"True entropy = {true_entropy:.3f} bits\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Number of Samples\", fontsize=12)\n",
    "ax.set_ylabel(\"Estimated Entropy (bits)\", fontsize=12)\n",
    "ax.set_title(\"Entropy Estimation Bias and Correction\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\", fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(true_entropy - 1, true_entropy + 0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š True entropy for uniform distribution with {n_bins_sim} bins: {true_entropy:.4f} bits\")\n",
    "print(f\"   With 100 samples: raw = {raw_means[1]:.4f}, corrected = {corrected_means[1]:.4f}\")\n",
    "print(f\"   With 5000 samples: raw = {raw_means[-1]:.4f}, corrected = {corrected_means[-1]:.4f}\")\n",
    "print(\"\\nâœ“ The correction helps, especially with fewer samples!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70adcc94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Excellent work so far! Let's continue to the second part of the notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6503ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 9: Entropy of different signal types\n",
    "\n",
    "np.random.seed(42)\n",
    "fs = 256  # Sampling rate (Hz)\n",
    "duration = 4  # seconds\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "n_samples = len(t)\n",
    "\n",
    "# Generate different signal types\n",
    "signals = {}\n",
    "\n",
    "# 1. Pure sine wave (very predictable)\n",
    "signals[\"Pure Sine (10 Hz)\"] = np.sin(2 * np.pi * 10 * t)\n",
    "\n",
    "# 2. Sine with noise (somewhat predictable)\n",
    "signals[\"Sine + Noise\"] = np.sin(2 * np.pi * 10 * t) + 0.3 * np.random.randn(n_samples)\n",
    "\n",
    "# 3. Mixed frequencies (more complex)\n",
    "signals[\"Mixed Frequencies\"] = (np.sin(2 * np.pi * 8 * t) + \n",
    "                                 0.7 * np.sin(2 * np.pi * 12 * t) + \n",
    "                                 0.5 * np.sin(2 * np.pi * 20 * t))\n",
    "\n",
    "# 4. White noise (maximally unpredictable)\n",
    "signals[\"White Noise\"] = np.random.randn(n_samples)\n",
    "\n",
    "# 5. Simulated alpha rhythm (realistic EEG-like)\n",
    "alpha = np.sin(2 * np.pi * 10 * t) * (1 + 0.3 * np.sin(2 * np.pi * 0.5 * t))\n",
    "signals[\"Alpha-like\"] = alpha + 0.2 * np.random.randn(n_samples)\n",
    "\n",
    "# Compute entropy for each\n",
    "n_bins_neural = optimal_n_bins(n_samples, \"sturges\")\n",
    "colors_signals = [COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"], \n",
    "                  COLORS[\"signal_4\"], COLORS[\"signal_5\"]]\n",
    "\n",
    "fig, axes = plt.subplots(len(signals), 2, figsize=(14, 12))\n",
    "\n",
    "for idx, (name, signal) in enumerate(signals.items()):\n",
    "    # Normalize signal\n",
    "    signal_norm = (signal - np.mean(signal)) / np.std(signal)\n",
    "    \n",
    "    # Compute entropy\n",
    "    H, actual_bins = compute_entropy_continuous(signal_norm, n_bins=n_bins_neural)\n",
    "    H_max = compute_max_entropy(actual_bins)\n",
    "    H_norm = H / H_max\n",
    "    \n",
    "    # Left: Time series\n",
    "    axes[idx, 0].plot(t[:256], signal_norm[:256], color=colors_signals[idx], linewidth=1)\n",
    "    axes[idx, 0].set_ylabel(name, fontsize=10, fontweight=\"bold\")\n",
    "    axes[idx, 0].set_xlim(0, 1)\n",
    "    if idx == len(signals) - 1:\n",
    "        axes[idx, 0].set_xlabel(\"Time (s)\", fontsize=11)\n",
    "    if idx == 0:\n",
    "        axes[idx, 0].set_title(\"Signal (1 second)\", fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    # Right: Distribution\n",
    "    axes[idx, 1].hist(signal_norm, bins=n_bins_neural, color=colors_signals[idx], \n",
    "                      edgecolor=\"white\", alpha=0.8, density=True)\n",
    "    axes[idx, 1].set_title(f\"H = {H:.2f} bits (normalized: {H_norm:.2f})\", fontsize=11)\n",
    "    if idx == len(signals) - 1:\n",
    "        axes[idx, 1].set_xlabel(\"Amplitude\", fontsize=11)\n",
    "    if idx == 0:\n",
    "        axes[idx, 1].set_title(f\"Distribution â€” H = {H:.2f} bits (norm: {H_norm:.2f})\", \n",
    "                               fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"Entropy of Different Signal Types\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Key observations:\")\n",
    "print(\"   â€¢ Pure sine has LOWER entropy (predictable, concentrated distribution)\")\n",
    "print(\"   â€¢ White noise has HIGHER entropy (unpredictable, uniform-like distribution)\")\n",
    "print(\"   â€¢ Real neural signals fall in between!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d5fa54",
   "metadata": {},
   "source": [
    "## 11. Spectral Entropy\n",
    "\n",
    "**Spectral entropy** applies the entropy concept to the **frequency domain**. Instead of measuring uncertainty in amplitude values, it measures uncertainty in the distribution of power across frequencies.\n",
    "\n",
    "### Definition\n",
    "\n",
    "Given a power spectral density (PSD) normalized to sum to 1 (making it a probability distribution):\n",
    "\n",
    "$$H_{spectral} = -\\sum_{f} P(f) \\log_2 P(f)$$\n",
    "\n",
    "Where $P(f)$ is the normalized power at frequency $f$.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **High spectral entropy**: Power spread across many frequencies â†’ \"broadband\" signal\n",
    "- **Low spectral entropy**: Power concentrated at few frequencies â†’ \"narrowband\" signal (e.g., strong oscillation)\n",
    "\n",
    "### Normalized Spectral Entropy\n",
    "\n",
    "$$H_{spectral,norm} = \\frac{H_{spectral}}{\\log_2(N_{freq})}$$\n",
    "\n",
    "Range: 0 (single frequency) to 1 (flat spectrum/white noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spectral_entropy(\n",
    "    signal: NDArray[np.float64],\n",
    "    fs: float,\n",
    "    nperseg: int = 256,\n",
    "    freq_range: Optional[Tuple[float, float]] = None,\n",
    "    normalize: bool = True\n",
    ") -> Tuple[float, NDArray[np.float64], NDArray[np.float64]]:\n",
    "    \"\"\"\n",
    "    Compute spectral entropy of a signal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : NDArray[np.float64]\n",
    "        Time series signal.\n",
    "    fs : float\n",
    "        Sampling frequency in Hz.\n",
    "    nperseg : int, optional\n",
    "        Length of each segment for Welch's method. Default is 256.\n",
    "    freq_range : Tuple[float, float], optional\n",
    "        Frequency range (fmin, fmax) to consider. Default is None (all frequencies).\n",
    "    normalize : bool, optional\n",
    "        Whether to normalize by maximum entropy. Default is True.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, NDArray, NDArray]\n",
    "        (spectral_entropy, frequencies, psd)\n",
    "    \"\"\"\n",
    "    # Compute PSD using Welch's method\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(nperseg, len(signal)))\n",
    "    \n",
    "    # Apply frequency range if specified\n",
    "    if freq_range is not None:\n",
    "        mask = (freqs >= freq_range[0]) & (freqs <= freq_range[1])\n",
    "        freqs = freqs[mask]\n",
    "        psd = psd[mask]\n",
    "    \n",
    "    # Normalize PSD to make it a probability distribution\n",
    "    psd_norm = psd / np.sum(psd)\n",
    "    \n",
    "    # Remove zeros\n",
    "    psd_valid = psd_norm[psd_norm > 0]\n",
    "    \n",
    "    # Compute entropy\n",
    "    H_spectral = -np.sum(psd_valid * np.log2(psd_valid))\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        H_max = np.log2(len(psd_valid))\n",
    "        if H_max > 0:\n",
    "            H_spectral = H_spectral / H_max\n",
    "    \n",
    "    return float(H_spectral), freqs, psd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a76edab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 10: Spectral entropy comparison\n",
    "\n",
    "np.random.seed(42)\n",
    "fs = 256\n",
    "duration = 10\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "n_samples = len(t)\n",
    "\n",
    "# Create signals with different spectral characteristics\n",
    "spectral_signals = {}\n",
    "\n",
    "# 1. Pure 10 Hz sine (narrowband)\n",
    "spectral_signals[\"Pure 10 Hz\"] = np.sin(2 * np.pi * 10 * t)\n",
    "\n",
    "# 2. Alpha band oscillation (8-12 Hz)\n",
    "alpha_signal = (np.sin(2 * np.pi * 8 * t) + \n",
    "                np.sin(2 * np.pi * 10 * t) + \n",
    "                np.sin(2 * np.pi * 12 * t)) / 3\n",
    "spectral_signals[\"Alpha Band\\n(8-12 Hz)\"] = alpha_signal\n",
    "\n",
    "# 3. Multiple bands\n",
    "multi_band = (np.sin(2 * np.pi * 10 * t) +  # alpha\n",
    "              0.7 * np.sin(2 * np.pi * 20 * t) +  # beta\n",
    "              0.5 * np.sin(2 * np.pi * 6 * t))  # theta\n",
    "spectral_signals[\"Multi-band\"] = multi_band\n",
    "\n",
    "# 4. White noise (flat spectrum)\n",
    "spectral_signals[\"White Noise\"] = np.random.randn(n_samples)\n",
    "\n",
    "# 5. Pink noise (1/f spectrum)\n",
    "# Generate 1/f noise using FFT\n",
    "fft_freqs = np.fft.rfftfreq(n_samples, 1/fs)\n",
    "fft_freqs[0] = 1  # Avoid division by zero\n",
    "pink_spectrum = 1 / np.sqrt(fft_freqs)\n",
    "pink_phase = np.random.uniform(0, 2*np.pi, len(fft_freqs))\n",
    "pink_fft = pink_spectrum * np.exp(1j * pink_phase)\n",
    "pink_noise = np.fft.irfft(pink_fft, n_samples)\n",
    "spectral_signals[\"Pink Noise (1/f)\"] = pink_noise\n",
    "\n",
    "# Compute and plot\n",
    "fig, axes = plt.subplots(len(spectral_signals), 2, figsize=(14, 14))\n",
    "colors_spec = [COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"], \n",
    "               COLORS[\"signal_4\"], COLORS[\"signal_5\"]]\n",
    "\n",
    "for idx, (name, signal) in enumerate(spectral_signals.items()):\n",
    "    H_spec, freqs, psd = compute_spectral_entropy(signal, fs, freq_range=(1, 50))\n",
    "    \n",
    "    # Left: PSD\n",
    "    axes[idx, 0].semilogy(freqs, psd, color=colors_spec[idx], linewidth=2)\n",
    "    axes[idx, 0].set_ylabel(name, fontsize=10, fontweight=\"bold\")\n",
    "    axes[idx, 0].set_xlim(0, 50)\n",
    "    axes[idx, 0].grid(True, alpha=0.3)\n",
    "    if idx == len(spectral_signals) - 1:\n",
    "        axes[idx, 0].set_xlabel(\"Frequency (Hz)\", fontsize=11)\n",
    "    if idx == 0:\n",
    "        axes[idx, 0].set_title(\"Power Spectral Density\", fontsize=12, fontweight=\"bold\")\n",
    "    \n",
    "    # Right: Normalized PSD (probability distribution)\n",
    "    psd_norm = psd / np.sum(psd)\n",
    "    axes[idx, 1].fill_between(freqs, psd_norm, alpha=0.5, color=colors_spec[idx])\n",
    "    axes[idx, 1].plot(freqs, psd_norm, color=colors_spec[idx], linewidth=2)\n",
    "    axes[idx, 1].set_xlim(0, 50)\n",
    "    axes[idx, 1].set_title(f\"H_spectral = {H_spec:.3f}\", fontsize=11)\n",
    "    if idx == len(spectral_signals) - 1:\n",
    "        axes[idx, 1].set_xlabel(\"Frequency (Hz)\", fontsize=11)\n",
    "    if idx == 0:\n",
    "        axes[idx, 1].set_title(f\"Normalized PSD â€” H = {H_spec:.3f}\", fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "plt.suptitle(\"Spectral Entropy: How Spread is Power Across Frequencies?\", \n",
    "             fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Spectral Entropy Interpretation:\")\n",
    "print(\"   â€¢ Pure tone (1 freq): LOW spectral entropy\")\n",
    "print(\"   â€¢ White noise (all freqs equally): HIGH spectral entropy\")\n",
    "print(\"   â€¢ Neural signals: Between extremes, depends on oscillatory content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554a4da",
   "metadata": {},
   "source": [
    "## 12. Sample Entropy\n",
    "\n",
    "**Sample entropy** is a measure of signal **complexity** that doesn't require binning. It measures how \"self-similar\" a signal is by comparing patterns at different time points.\n",
    "\n",
    "### Concept\n",
    "\n",
    "Sample entropy asks: *If two segments of the signal are similar now, how likely are they to remain similar if we extend them by one sample?*\n",
    "\n",
    "- **Low sample entropy**: Signal has repeating patterns â†’ predictable\n",
    "- **High sample entropy**: Signal patterns don't repeat â†’ unpredictable\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **m** (embedding dimension): Length of patterns to compare (typically 2)\n",
    "- **r** (tolerance): How close patterns must be to be \"similar\" (typically 0.2 Ã— std)\n",
    "\n",
    "### Advantages Over Binning-Based Entropy\n",
    "\n",
    "1. No binning required â†’ no bias from bin choice\n",
    "2. Captures temporal structure (pattern matching)\n",
    "3. Robust to noise and signal length\n",
    "4. Commonly used in physiological signal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c578099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_entropy(\n",
    "    signal: NDArray[np.float64],\n",
    "    m: int = 2,\n",
    "    r: Optional[float] = None\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute sample entropy of a time series.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : NDArray[np.float64]\n",
    "        Time series signal.\n",
    "    m : int, optional\n",
    "        Embedding dimension. Default is 2.\n",
    "    r : float, optional\n",
    "        Tolerance for pattern matching. Default is 0.2 * std(signal).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Sample entropy value.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Higher values indicate more complexity/irregularity.\n",
    "    Lower values indicate more self-similarity/regularity.\n",
    "    \"\"\"\n",
    "    N = len(signal)\n",
    "    \n",
    "    if r is None:\n",
    "        r = 0.2 * np.std(signal)\n",
    "    \n",
    "    def count_matches(template_length: int) -> int:\n",
    "        \"\"\"Count pairs of matching templates.\"\"\"\n",
    "        templates = np.array([\n",
    "            signal[i:i + template_length] \n",
    "            for i in range(N - template_length)\n",
    "        ])\n",
    "        \n",
    "        count = 0\n",
    "        n_templates = len(templates)\n",
    "        \n",
    "        for i in range(n_templates):\n",
    "            for j in range(i + 1, n_templates):\n",
    "                # Chebyshev distance (max absolute difference)\n",
    "                if np.max(np.abs(templates[i] - templates[j])) <= r:\n",
    "                    count += 1\n",
    "        \n",
    "        return count\n",
    "    \n",
    "    # Count matches for m and m+1\n",
    "    A = count_matches(m + 1)  # matches for length m+1\n",
    "    B = count_matches(m)      # matches for length m\n",
    "    \n",
    "    # Sample entropy\n",
    "    if A == 0 or B == 0:\n",
    "        return np.inf  # No matches found\n",
    "    \n",
    "    return -np.log(A / B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545b025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 11: Sample entropy for different signals\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create short signals for sample entropy (computation is O(nÂ²))\n",
    "n_samples_se = 500\n",
    "t_se = np.arange(n_samples_se) / 256\n",
    "\n",
    "sample_entropy_signals = {}\n",
    "\n",
    "# 1. Regular sine wave\n",
    "sample_entropy_signals[\"Regular Sine\"] = np.sin(2 * np.pi * 10 * t_se)\n",
    "\n",
    "# 2. Sine with amplitude modulation\n",
    "sample_entropy_signals[\"AM Sine\"] = np.sin(2 * np.pi * 10 * t_se) * (1 + 0.5 * np.sin(2 * np.pi * 0.5 * t_se))\n",
    "\n",
    "# 3. Chaotic signal (logistic map)\n",
    "x_logistic = np.zeros(n_samples_se)\n",
    "x_logistic[0] = 0.1\n",
    "r_param = 3.9  # Chaotic regime\n",
    "for i in range(1, n_samples_se):\n",
    "    x_logistic[i] = r_param * x_logistic[i-1] * (1 - x_logistic[i-1])\n",
    "sample_entropy_signals[\"Chaotic\\n(Logistic Map)\"] = x_logistic\n",
    "\n",
    "# 4. Random walk\n",
    "random_walk = np.cumsum(np.random.randn(n_samples_se))\n",
    "sample_entropy_signals[\"Random Walk\"] = random_walk\n",
    "\n",
    "# 5. White noise\n",
    "sample_entropy_signals[\"White Noise\"] = np.random.randn(n_samples_se)\n",
    "\n",
    "# Compute sample entropy\n",
    "fig, axes = plt.subplots(len(sample_entropy_signals), 1, figsize=(14, 12))\n",
    "colors_se = [COLORS[\"signal_1\"], COLORS[\"signal_2\"], COLORS[\"signal_3\"], \n",
    "             COLORS[\"signal_4\"], COLORS[\"signal_5\"]]\n",
    "\n",
    "sample_entropies = []\n",
    "\n",
    "for idx, (name, signal) in enumerate(sample_entropy_signals.items()):\n",
    "    # Normalize\n",
    "    signal_norm = (signal - np.mean(signal)) / np.std(signal)\n",
    "    \n",
    "    # Compute sample entropy\n",
    "    se = compute_sample_entropy(signal_norm, m=2)\n",
    "    sample_entropies.append((name.replace(\"\\n\", \" \"), se))\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].plot(t_se, signal_norm, color=colors_se[idx], linewidth=1)\n",
    "    axes[idx].set_ylabel(name, fontsize=10, fontweight=\"bold\")\n",
    "    axes[idx].set_xlim(0, t_se[-1])\n",
    "    \n",
    "    # Add sample entropy annotation\n",
    "    axes[idx].text(0.98, 0.95, f\"SampEn = {se:.3f}\", transform=axes[idx].transAxes,\n",
    "                   fontsize=11, fontweight=\"bold\", ha=\"right\", va=\"top\",\n",
    "                   bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    if idx == len(sample_entropy_signals) - 1:\n",
    "        axes[idx].set_xlabel(\"Time (s)\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\"Sample Entropy: Measuring Pattern Regularity\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary bar chart\n",
    "print(\"\\nðŸ“Š Sample Entropy Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for name, se in sample_entropies:\n",
    "    bar = \"â–ˆ\" * int(se * 10) if se < 3 else \"â–ˆ\" * 30 + \"...\"\n",
    "    print(f\"  {name:<24}: {se:.3f}  {bar}\")\n",
    "\n",
    "print(\"\\nâœ“ Regular signals have LOW sample entropy (patterns repeat)\")\n",
    "print(\"âœ“ Complex/random signals have HIGH sample entropy (no repeating patterns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae77136",
   "metadata": {},
   "source": [
    "## 13. Entropy in Hyperscanning Context\n",
    "\n",
    "In hyperscanning, we record brain signals from **multiple people** simultaneously. How does entropy help us understand inter-brain connectivity?\n",
    "\n",
    "### Individual Signal Entropy\n",
    "\n",
    "- Characterizes the **complexity** of each person's brain activity\n",
    "- Can reveal different cognitive states or engagement levels\n",
    "- Baseline measure before computing connectivity\n",
    "\n",
    "### Entropy and Connectivity (Preview)\n",
    "\n",
    "The real power of entropy for connectivity comes in the next notebooks:\n",
    "\n",
    "1. **Mutual Information (D02)**: How much entropy is *shared* between two signals?\n",
    "   - High MI = knowing one signal reduces uncertainty about the other\n",
    "   \n",
    "2. **Transfer Entropy (D03)**: How much does the past of one signal reduce entropy of another?\n",
    "   - Reveals *directional* information flow between brains\n",
    "\n",
    "### Entropy as a Preprocessing Tool\n",
    "\n",
    "- Identify noisy channels (unexpectedly high entropy)\n",
    "- Detect artifacts (abrupt entropy changes)\n",
    "- Characterize signal quality before connectivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d1f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 12: Entropy in a simulated hyperscanning scenario\n",
    "\n",
    "np.random.seed(42)\n",
    "fs = 256\n",
    "duration = 10\n",
    "t = np.arange(0, duration, 1/fs)\n",
    "n_samples = len(t)\n",
    "\n",
    "# Simulate two subjects with different engagement levels\n",
    "# Subject 1: Engaged (more structured alpha oscillation)\n",
    "alpha_1 = 2 * np.sin(2 * np.pi * 10 * t) * (1 + 0.3 * np.sin(2 * np.pi * 0.2 * t))\n",
    "noise_1 = 0.3 * np.random.randn(n_samples)\n",
    "subject_1 = alpha_1 + noise_1\n",
    "\n",
    "# Subject 2: Less engaged (weaker oscillation, more noise)\n",
    "alpha_2 = 0.8 * np.sin(2 * np.pi * 10 * t + np.pi/4)\n",
    "noise_2 = 0.8 * np.random.randn(n_samples)\n",
    "subject_2 = alpha_2 + noise_2\n",
    "\n",
    "# Normalize\n",
    "subject_1 = (subject_1 - np.mean(subject_1)) / np.std(subject_1)\n",
    "subject_2 = (subject_2 - np.mean(subject_2)) / np.std(subject_2)\n",
    "\n",
    "# Compute entropies\n",
    "n_bins = optimal_n_bins(n_samples)\n",
    "H1, _ = compute_entropy_continuous(subject_1, n_bins=n_bins)\n",
    "H2, _ = compute_entropy_continuous(subject_2, n_bins=n_bins)\n",
    "H_max = compute_max_entropy(n_bins)\n",
    "\n",
    "H1_spec, _, psd1 = compute_spectral_entropy(subject_1, fs)\n",
    "H2_spec, freqs, psd2 = compute_spectral_entropy(subject_2, fs)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Row 1: Subject 1\n",
    "axes[0, 0].plot(t[:512], subject_1[:512], color=COLORS[\"signal_1\"], linewidth=1)\n",
    "axes[0, 0].set_title(\"Subject 1: Engaged\\n(Strong Alpha)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0, 0].set_ylabel(\"Amplitude\", fontsize=10)\n",
    "axes[0, 0].set_xlabel(\"Time (s)\", fontsize=10)\n",
    "axes[0, 0].set_xlim(0, 2)\n",
    "\n",
    "axes[0, 1].hist(subject_1, bins=n_bins, color=COLORS[\"signal_1\"], edgecolor=\"white\", alpha=0.8, density=True)\n",
    "axes[0, 1].set_title(f\"Distribution\\nH = {H1:.2f} bits ({H1/H_max:.2f} norm)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0, 1].set_xlabel(\"Amplitude\", fontsize=10)\n",
    "\n",
    "psd1_norm = psd1 / np.sum(psd1)\n",
    "axes[0, 2].fill_between(freqs, psd1_norm, alpha=0.5, color=COLORS[\"signal_1\"])\n",
    "axes[0, 2].plot(freqs, psd1_norm, color=COLORS[\"signal_1\"], linewidth=2)\n",
    "axes[0, 2].set_title(f\"Spectrum\\nH_spec = {H1_spec:.3f}\", fontsize=11, fontweight=\"bold\")\n",
    "axes[0, 2].set_xlabel(\"Frequency (Hz)\", fontsize=10)\n",
    "axes[0, 2].set_xlim(0, 40)\n",
    "\n",
    "# Row 2: Subject 2\n",
    "axes[1, 0].plot(t[:512], subject_2[:512], color=COLORS[\"signal_2\"], linewidth=1)\n",
    "axes[1, 0].set_title(\"Subject 2: Less Engaged\\n(Weak Alpha, More Noise)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1, 0].set_ylabel(\"Amplitude\", fontsize=10)\n",
    "axes[1, 0].set_xlabel(\"Time (s)\", fontsize=10)\n",
    "axes[1, 0].set_xlim(0, 2)\n",
    "\n",
    "axes[1, 1].hist(subject_2, bins=n_bins, color=COLORS[\"signal_2\"], edgecolor=\"white\", alpha=0.8, density=True)\n",
    "axes[1, 1].set_title(f\"Distribution\\nH = {H2:.2f} bits ({H2/H_max:.2f} norm)\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1, 1].set_xlabel(\"Amplitude\", fontsize=10)\n",
    "\n",
    "psd2_norm = psd2 / np.sum(psd2)\n",
    "axes[1, 2].fill_between(freqs, psd2_norm, alpha=0.5, color=COLORS[\"signal_2\"])\n",
    "axes[1, 2].plot(freqs, psd2_norm, color=COLORS[\"signal_2\"], linewidth=2)\n",
    "axes[1, 2].set_title(f\"Spectrum\\nH_spec = {H2_spec:.3f}\", fontsize=11, fontweight=\"bold\")\n",
    "axes[1, 2].set_xlabel(\"Frequency (Hz)\", fontsize=10)\n",
    "axes[1, 2].set_xlim(0, 40)\n",
    "\n",
    "plt.suptitle(\"Hyperscanning: Characterizing Individual Signal Properties with Entropy\", \n",
    "             fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Entropy Comparison:\")\n",
    "print(f\"   Subject 1 (engaged):     H_amplitude = {H1:.3f}, H_spectral = {H1_spec:.3f}\")\n",
    "print(f\"   Subject 2 (less engaged): H_amplitude = {H2:.3f}, H_spectral = {H2_spec:.3f}\")\n",
    "print(\"\\nðŸ’¡ The less engaged subject shows:\")\n",
    "print(\"   â€¢ Higher amplitude entropy (more random)\")\n",
    "print(\"   â€¢ Higher spectral entropy (power spread across frequencies)\")\n",
    "print(\"\\nâ†’ Next step: Quantify SHARED information between subjects (Mutual Information, D02)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bf8175",
   "metadata": {},
   "source": [
    "## 14. Exercises\n",
    "\n",
    "Test your understanding with these exercises!\n",
    "\n",
    "### Exercise 1: Entropy Calculation\n",
    "Given a probability distribution [0.25, 0.25, 0.25, 0.25], calculate:\n",
    "1. The Shannon entropy in bits\n",
    "2. Is this maximum entropy for 4 states?\n",
    "3. What distribution would give minimum entropy?\n",
    "\n",
    "### Exercise 2: Binning Impact\n",
    "Generate a uniform random signal of 1000 samples. Compute entropy with 10, 50, and 100 bins.\n",
    "- How does entropy change with bin count?\n",
    "- What is the normalized entropy in each case?\n",
    "\n",
    "### Exercise 3: Spectral Entropy\n",
    "Create two signals: (a) pure 10 Hz sine, (b) sum of 5, 10, 15, 20 Hz sines.\n",
    "- Which has higher spectral entropy?\n",
    "- What does this tell you about the signals?\n",
    "\n",
    "### Exercise 4: Entropy and Neural States\n",
    "Think about how entropy might differ between:\n",
    "- Eyes open vs. eyes closed (alpha rhythm)\n",
    "- Rest vs. cognitive task\n",
    "- Awake vs. sleep stages\n",
    "\n",
    "What entropy patterns would you predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d803bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise solutions (try yourself first!)\n",
    "\n",
    "# Exercise 1\n",
    "print(\"=\" * 60)\n",
    "print(\"EXERCISE 1: Entropy Calculation\")\n",
    "print(\"=\" * 60)\n",
    "uniform_4 = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "H_uniform = compute_entropy_discrete(uniform_4)\n",
    "H_max_4 = compute_max_entropy(4)\n",
    "print(f\"1. H = {H_uniform:.4f} bits\")\n",
    "print(f\"2. H_max for 4 states = logâ‚‚(4) = {H_max_4:.4f} bits\")\n",
    "print(f\"   â†’ Yes! Uniform distribution achieves maximum entropy.\")\n",
    "print(f\"3. Minimum entropy: [1, 0, 0, 0] â†’ H = 0 (deterministic)\")\n",
    "\n",
    "# Exercise 2\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXERCISE 2: Binning Impact\")\n",
    "print(\"=\" * 60)\n",
    "np.random.seed(42)\n",
    "uniform_signal = np.random.uniform(0, 1, 1000)\n",
    "for n_bins in [10, 50, 100]:\n",
    "    H, _ = compute_entropy_continuous(uniform_signal, n_bins=n_bins)\n",
    "    H_max = compute_max_entropy(n_bins)\n",
    "    print(f\"  {n_bins:3d} bins: H = {H:.3f} bits, H_max = {H_max:.3f}, normalized = {H/H_max:.4f}\")\n",
    "print(\"  â†’ Entropy increases with bins, but normalized entropy stays ~constant (~1.0)\")\n",
    "\n",
    "# Exercise 3\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXERCISE 3: Spectral Entropy\")\n",
    "print(\"=\" * 60)\n",
    "t_ex = np.arange(0, 5, 1/256)\n",
    "pure_sine = np.sin(2 * np.pi * 10 * t_ex)\n",
    "multi_sine = (np.sin(2 * np.pi * 5 * t_ex) + np.sin(2 * np.pi * 10 * t_ex) + \n",
    "              np.sin(2 * np.pi * 15 * t_ex) + np.sin(2 * np.pi * 20 * t_ex))\n",
    "H_pure, _, _ = compute_spectral_entropy(pure_sine, 256)\n",
    "H_multi, _, _ = compute_spectral_entropy(multi_sine, 256)\n",
    "print(f\"  Pure 10 Hz sine:    H_spectral = {H_pure:.4f}\")\n",
    "print(f\"  Multi-frequency:    H_spectral = {H_multi:.4f}\")\n",
    "print(\"  â†’ Multi-frequency has HIGHER spectral entropy (power spread)\")\n",
    "\n",
    "# Exercise 4\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXERCISE 4: Entropy and Neural States (Discussion)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"  Expected patterns:\")\n",
    "print(\"  â€¢ Eyes closed: LOWER spectral entropy (strong alpha)\")\n",
    "print(\"  â€¢ Eyes open: HIGHER spectral entropy (alpha suppression)\")\n",
    "print(\"  â€¢ Cognitive task: Variable (depends on task demands)\")\n",
    "print(\"  â€¢ Deep sleep: LOWER entropy (slow, synchronized waves)\")\n",
    "print(\"  â€¢ REM sleep: HIGHER entropy (desynchronized, dream-like)\")\n",
    "print(\"  â€¢ Awake: Intermediate entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f562996",
   "metadata": {},
   "source": [
    "## 15. Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Concept | Definition | Key Property |\n",
    "|---------|------------|--------------|\n",
    "| **Information** | Reduction of uncertainty | Measured in bits |\n",
    "| **Entropy** | Average uncertainty of a random variable | $H = -\\sum p \\log p$ |\n",
    "| **Maximum Entropy** | When all outcomes equally likely | $H_{max} = \\log(n)$ |\n",
    "| **Binary Entropy** | Entropy for yes/no questions | Max at p = 0.5 |\n",
    "\n",
    "### Entropy Variants for Signals\n",
    "\n",
    "| Type | Domain | Measures |\n",
    "|------|--------|----------|\n",
    "| **Amplitude Entropy** | Time/Value | Uncertainty in signal values |\n",
    "| **Spectral Entropy** | Frequency | How spread is power across frequencies |\n",
    "| **Sample Entropy** | Time patterns | Regularity/complexity of patterns |\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "1. **Binning choice** affects discrete entropy estimation\n",
    "2. **Bias correction** (Miller-Madow) helps with small samples\n",
    "3. **Normalization** allows comparison across different state spaces\n",
    "4. **Sample entropy** avoids binning issues\n",
    "\n",
    "### Connection to Connectivity\n",
    "\n",
    "Entropy is the foundation for:\n",
    "- **Mutual Information** (D02): Shared entropy between signals\n",
    "- **Transfer Entropy** (D03): Directed information flow\n",
    "\n",
    "### Functions Defined\n",
    "\n",
    "```python\n",
    "compute_entropy_discrete(probabilities, base)      # Discrete entropy\n",
    "compute_entropy_from_counts(counts, base)          # From observations\n",
    "compute_max_entropy(n_states, base)                # Maximum possible\n",
    "compute_normalized_entropy(probabilities, base)    # Range 0-1\n",
    "binary_entropy(p)                                  # H(p) for binary\n",
    "optimal_n_bins(n_samples, method)                  # Bin selection\n",
    "compute_entropy_continuous(signal, n_bins, method) # For continuous signals\n",
    "compute_entropy_miller_madow(signal, n_bins)       # Bias-corrected\n",
    "compute_spectral_entropy(signal, fs, ...)          # Frequency domain\n",
    "compute_sample_entropy(signal, m, r)               # Pattern-based\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b06e5",
   "metadata": {},
   "source": [
    "## 16. Discussion Questions\n",
    "\n",
    "1. **Why is entropy measured in bits?**\n",
    "   - What's the connection to binary encoding?\n",
    "   - How does this relate to computer science?\n",
    "\n",
    "2. **Entropy and brain states**\n",
    "   - Would you expect higher or lower entropy during focused attention vs. mind wandering?\n",
    "   - How might anesthesia affect brain signal entropy?\n",
    "\n",
    "3. **Limitations of entropy**\n",
    "   - What information does entropy NOT capture about a signal?\n",
    "   - Why might two very different signals have the same entropy?\n",
    "\n",
    "4. **Choosing entropy measures**\n",
    "   - When would you prefer amplitude entropy vs. spectral entropy vs. sample entropy?\n",
    "   - What are the trade-offs of each approach?\n",
    "\n",
    "5. **Preview: From entropy to connectivity**\n",
    "   - How might we use entropy to measure \"shared information\" between two brains?\n",
    "   - What would it mean if two signals have high mutual entropy?\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook (**D02: Mutual Information**), we'll learn:\n",
    "- How to quantify **shared information** between signals\n",
    "- The relationship $I(X;Y) = H(X) + H(Y) - H(X,Y)$\n",
    "- How mutual information reveals **statistical dependencies**\n",
    "- Application to inter-brain connectivity in hyperscanning\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook completed! Entropy is the foundation of information theory â€” now you're ready to explore connectivity measures based on shared information.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectivity-metrics-tutorials-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
