{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# C03: Statistical Significance for Connectivity\n",
    "# ============================================================================\n",
    "#\n",
    "# This notebook covers how to determine whether connectivity values are\n",
    "# statistically significant. We'll learn to generate surrogate data,\n",
    "# build null distributions, and apply proper multiple comparisons correction.\n",
    "#\n",
    "# Duration: ~70 minutes\n",
    "# Prerequisites: C02 (Connectivity Matrices), basic statistics\n",
    "#\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "from scipy.fft import fft, ifft\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# Import from local src\n",
    "import sys\n",
    "sys.path.insert(0, '../../..')\n",
    "\n",
    "from src.colors import COLORS\n",
    "\n",
    "# Define color shortcuts for this notebook\n",
    "PRIMARY_BLUE = COLORS['signal_1']      # Sky Blue\n",
    "PRIMARY_RED = COLORS['negative']        # Coral Red\n",
    "PRIMARY_GREEN = COLORS['signal_3']      # Sage Green\n",
    "SECONDARY_PURPLE = COLORS['signal_5']   # Lavender\n",
    "SECONDARY_ORANGE = COLORS['signal_4']   # Golden\n",
    "SUBJECT_1 = COLORS['signal_1']          # Sky Blue\n",
    "SUBJECT_2 = COLORS['signal_2']          # Rose Pink\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Constants\n",
    "fs = 256  # Sampling frequency (Hz)\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Sampling frequency: {fs} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13231737",
   "metadata": {},
   "source": [
    "## Section 1: Introduction ‚Äî Why Significance Matters\n",
    "\n",
    "Connectivity metrics **always** give you a number. You compute PLV between two channels and get 0.35. But what does that mean? Is it \"high\"? \"Low\"? \"Significant\"?\n",
    "\n",
    "The answer depends on what you would expect **by chance**. Even two completely unrelated signals will show some non-zero connectivity due to random fluctuations. The critical question is: *Is our observed value unlikely to occur by chance alone?*\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Without proper statistical testing:\n",
    "- **False positives**: You claim connectivity that isn't really there\n",
    "- **False negatives**: You miss true connectivity\n",
    "- **Non-reproducible results**: Your findings won't replicate\n",
    "\n",
    "Scientific claims require statistical validation. This notebook teaches you how to do it **correctly**.\n",
    "\n",
    "> **Key message**: *\"A connectivity value without a p-value is just a number.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff89685",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: The Null Hypothesis for Connectivity\n",
    "\n",
    "In hypothesis testing, we define two competing hypotheses:\n",
    "\n",
    "- **Null hypothesis (H‚ÇÄ)**: There is NO true connectivity between the signals. Any measured connectivity is due to chance.\n",
    "- **Alternative hypothesis (H‚ÇÅ)**: True connectivity exists between the signals.\n",
    "\n",
    "### What Does \"No Connectivity\" Look Like?\n",
    "\n",
    "Under H‚ÇÄ, signals may have:\n",
    "- Similar spectral properties (same frequency content)\n",
    "- Similar amplitude characteristics\n",
    "- **But NO consistent phase or amplitude relationship**\n",
    "\n",
    "### The Testing Procedure\n",
    "\n",
    "1. Determine the **distribution of connectivity under H‚ÇÄ** (null distribution)\n",
    "2. Ask: Is our observed value unlikely under this distribution?\n",
    "3. If unlikely (p < Œ±) ‚Üí Reject H‚ÇÄ ‚Üí Claim significant connectivity\n",
    "\n",
    "The key challenge is: **How do we build the null distribution?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41dcb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 1: Conceptual Null Distribution\n",
    "# ============================================================================\n",
    "\n",
    "# Create a conceptual null distribution\n",
    "np.random.seed(42)\n",
    "null_distribution = np.random.beta(2, 5, 1000) * 0.5 + 0.1  # Skewed towards low values\n",
    "observed_value = 0.42\n",
    "\n",
    "# Compute p-value\n",
    "pvalue = np.mean(null_distribution >= observed_value)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot histogram\n",
    "n, bins, patches = ax.hist(null_distribution, bins=40, density=True, \n",
    "                            color=PRIMARY_BLUE, alpha=0.7, edgecolor='white')\n",
    "\n",
    "# Color the tail\n",
    "for i, (patch, left_edge) in enumerate(zip(patches, bins[:-1])):\n",
    "    if left_edge >= observed_value:\n",
    "        patch.set_facecolor(PRIMARY_RED)\n",
    "        patch.set_alpha(0.8)\n",
    "\n",
    "# Add observed value line\n",
    "ax.axvline(observed_value, color=PRIMARY_RED, linewidth=3, linestyle='--',\n",
    "           label=f'Observed = {observed_value}')\n",
    "\n",
    "# Annotations\n",
    "ax.annotate(f'p-value = {pvalue:.3f}\\n(shaded area)', \n",
    "            xy=(observed_value + 0.02, 1.5),\n",
    "            fontsize=12, fontweight='bold', color=PRIMARY_RED)\n",
    "\n",
    "ax.set_xlabel('Connectivity Value (PLV)', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Is Our Observation in the Tail of the Null Distribution?', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_xlim(0, 0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Observed value: {observed_value}\")\n",
    "print(f\"P-value: {pvalue:.3f}\")\n",
    "if pvalue < 0.05:\n",
    "    print(\"‚Üí Result is SIGNIFICANT at Œ± = 0.05\")\n",
    "else:\n",
    "    print(\"‚Üí Result is NOT significant at Œ± = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96484d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Surrogate Data Methods\n",
    "\n",
    "To build a null distribution, we need data that satisfies H‚ÇÄ ‚Äî signals with **no true connectivity**. We create this using **surrogate data**.\n",
    "\n",
    "### What is Surrogate Data?\n",
    "\n",
    "Surrogate data is artificial data that:\n",
    "- **Preserves** certain properties of the original (e.g., power spectrum)\n",
    "- **Destroys** the property we're testing (e.g., phase relationship)\n",
    "\n",
    "### Common Surrogate Methods\n",
    "\n",
    "| Method | Preserves | Destroys | Best For |\n",
    "|--------|-----------|----------|----------|\n",
    "| **Phase shuffling** | Power spectrum | Phase relationships | PLV, coherence |\n",
    "| **Time shifting** | Amplitude, approximate spectrum | Temporal alignment | Quick checks |\n",
    "| **Trial shuffling** | Individual trials | Trial pairing | Across-trial analyses |\n",
    "| **AAFT** | Amplitude distribution + spectrum | Phase relationships | Strict tests |\n",
    "\n",
    "### The Procedure\n",
    "\n",
    "1. Generate surrogate data (many times)\n",
    "2. Compute connectivity for each surrogate\n",
    "3. Build histogram of surrogate connectivity values\n",
    "4. This histogram IS the null distribution!\n",
    "\n",
    "Let's implement the most common methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838116b8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Phase Shuffling ‚Äî The Gold Standard\n",
    "\n",
    "Phase shuffling is the **most common** method for testing phase-based connectivity (PLV, coherence). The idea is elegant:\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Transform the signal to the frequency domain (FFT)\n",
    "2. **Randomly shuffle the phases** while keeping magnitudes intact\n",
    "3. Transform back to time domain (IFFT)\n",
    "\n",
    "### What This Preserves and Destroys\n",
    "\n",
    "‚úÖ **Preserves**: Power spectrum (all magnitudes unchanged)  \n",
    "‚ùå **Destroys**: Any phase relationship between signals\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "If there's true phase synchronization:\n",
    "- The original phases have a consistent relationship\n",
    "- Shuffling makes them random ‚Üí connectivity drops\n",
    "\n",
    "If there's NO true synchronization:\n",
    "- Phases were already random\n",
    "- Shuffling makes no difference ‚Üí similar connectivity values\n",
    "\n",
    "Let's implement it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 2: Understanding Phase Shuffling Step by Step\n",
    "# ============================================================================\n",
    "\n",
    "def phase_shuffle(signal: NDArray[np.floating]) -> NDArray[np.floating]:\n",
    "    \"\"\"\n",
    "    Create a phase-shuffled surrogate of a signal.\n",
    "    \n",
    "    Preserves the power spectrum while randomizing phase relationships.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : NDArray[np.floating]\n",
    "        Input signal (1D array).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    NDArray[np.floating]\n",
    "        Phase-shuffled surrogate signal.\n",
    "    \"\"\"\n",
    "    n = len(signal)\n",
    "    \n",
    "    # FFT\n",
    "    spectrum = fft(signal)\n",
    "    \n",
    "    # Get magnitude and phase\n",
    "    magnitude = np.abs(spectrum)\n",
    "    \n",
    "    # Generate random phases (symmetric for real output)\n",
    "    random_phases = np.random.uniform(0, 2 * np.pi, n // 2 + 1)\n",
    "    \n",
    "    # Build symmetric phase array for real signal\n",
    "    if n % 2 == 0:  # Even length\n",
    "        new_phases = np.concatenate([\n",
    "            [0],  # DC component (no phase)\n",
    "            random_phases[1:-1],\n",
    "            [0],  # Nyquist (no phase)\n",
    "            -random_phases[-2:0:-1]  # Negative frequencies\n",
    "        ])\n",
    "    else:  # Odd length\n",
    "        new_phases = np.concatenate([\n",
    "            [0],  # DC component\n",
    "            random_phases[1:],\n",
    "            -random_phases[-1:0:-1]  # Negative frequencies\n",
    "        ])\n",
    "    \n",
    "    # Reconstruct spectrum with new phases\n",
    "    surrogate_spectrum = magnitude * np.exp(1j * new_phases)\n",
    "    \n",
    "    # Inverse FFT\n",
    "    surrogate = np.real(ifft(surrogate_spectrum))\n",
    "    \n",
    "    return surrogate\n",
    "\n",
    "\n",
    "# Create example signal\n",
    "np.random.seed(42)\n",
    "t = np.arange(0, 2, 1/fs)\n",
    "original = np.sin(2 * np.pi * 10 * t) + 0.3 * np.sin(2 * np.pi * 25 * t)\n",
    "original += 0.2 * np.random.randn(len(t))\n",
    "\n",
    "# Create surrogate\n",
    "surrogate = phase_shuffle(original)\n",
    "\n",
    "# Compute spectra\n",
    "freq = np.fft.fftfreq(len(original), 1/fs)\n",
    "spectrum_original = np.abs(fft(original))\n",
    "spectrum_surrogate = np.abs(fft(surrogate))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Time domain - original\n",
    "axes[0, 0].plot(t[:256], original[:256], color=PRIMARY_BLUE, linewidth=1.5)\n",
    "axes[0, 0].set_title('Original Signal', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "axes[0, 0].set_xlim(0, 1)\n",
    "\n",
    "# Time domain - surrogate\n",
    "axes[0, 1].plot(t[:256], surrogate[:256], color=SECONDARY_ORANGE, linewidth=1.5)\n",
    "axes[0, 1].set_title('Phase-Shuffled Surrogate', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Amplitude')\n",
    "axes[0, 1].set_xlim(0, 1)\n",
    "\n",
    "# Frequency domain - original\n",
    "pos_freq = freq[:len(freq)//2]\n",
    "axes[1, 0].plot(pos_freq, spectrum_original[:len(freq)//2], \n",
    "                color=PRIMARY_BLUE, linewidth=1.5)\n",
    "axes[1, 0].set_title('Power Spectrum (Original)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 0].set_ylabel('Magnitude')\n",
    "axes[1, 0].set_xlim(0, 50)\n",
    "\n",
    "# Frequency domain - surrogate\n",
    "axes[1, 1].plot(pos_freq, spectrum_surrogate[:len(freq)//2], \n",
    "                color=SECONDARY_ORANGE, linewidth=1.5)\n",
    "axes[1, 1].set_title('Power Spectrum (Surrogate)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 1].set_ylabel('Magnitude')\n",
    "axes[1, 1].set_xlim(0, 50)\n",
    "\n",
    "plt.suptitle('Phase Shuffling: Time Domain Changes, Spectrum Preserved', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Original and surrogate look different in time domain\")\n",
    "print(\"‚úì But their power spectra are IDENTICAL!\")\n",
    "print(f\"  Correlation of spectra: {np.corrcoef(spectrum_original, spectrum_surrogate)[0,1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4802bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Time Shifting ‚Äî A Faster Alternative\n",
    "\n",
    "Time shifting is simpler and faster than phase shuffling. It's useful for quick exploratory analyses.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Shift one signal by a **random time lag**\n",
    "2. This breaks the temporal alignment between signals\n",
    "\n",
    "### What This Preserves and Destroys\n",
    "\n",
    "‚úÖ **Preserves**: Exact waveform, amplitude distribution  \n",
    "‚ùå **Destroys**: Temporal alignment (and thus phase relationships)\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Aspect | Assessment |\n",
    "|--------|------------|\n",
    "| Speed | ‚ö° Very fast |\n",
    "| Simplicity | üëç Easy to implement |\n",
    "| Spectrum preservation | ‚ö†Ô∏è Approximate (edge effects) |\n",
    "| Statistical rigor | ‚ö†Ô∏è Less rigorous than phase shuffling |\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Quick exploratory analyses\n",
    "- Very long signals where edge effects are negligible\n",
    "- When computational speed matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 3: Time Shifting Method\n",
    "# ============================================================================\n",
    "\n",
    "def time_shift(signal: NDArray[np.floating], \n",
    "               min_shift: int = None,\n",
    "               max_shift: int = None) -> NDArray[np.floating]:\n",
    "    \"\"\"\n",
    "    Create a time-shifted surrogate of a signal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : NDArray[np.floating]\n",
    "        Input signal (1D array).\n",
    "    min_shift : int, optional\n",
    "        Minimum shift (samples). Default: 10% of signal length.\n",
    "    max_shift : int, optional\n",
    "        Maximum shift (samples). Default: 90% of signal length.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    NDArray[np.floating]\n",
    "        Time-shifted surrogate signal (circular shift).\n",
    "    \"\"\"\n",
    "    n = len(signal)\n",
    "    \n",
    "    if min_shift is None:\n",
    "        min_shift = n // 10\n",
    "    if max_shift is None:\n",
    "        max_shift = 9 * n // 10\n",
    "    \n",
    "    # Random shift\n",
    "    shift = np.random.randint(min_shift, max_shift)\n",
    "    \n",
    "    # Circular shift (wraps around)\n",
    "    surrogate = np.roll(signal, shift)\n",
    "    \n",
    "    return surrogate\n",
    "\n",
    "\n",
    "# Create example with two related signals\n",
    "np.random.seed(42)\n",
    "t = np.arange(0, 2, 1/fs)\n",
    "\n",
    "# Signal 1\n",
    "signal1 = np.sin(2 * np.pi * 10 * t) + 0.3 * np.random.randn(len(t))\n",
    "\n",
    "# Signal 2: related to signal 1 (phase-locked)\n",
    "signal2 = np.sin(2 * np.pi * 10 * t + np.pi/4) + 0.3 * np.random.randn(len(t))\n",
    "\n",
    "# Create time-shifted version\n",
    "signal2_shifted = time_shift(signal2)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Signal 1\n",
    "axes[0].plot(t, signal1, color=SUBJECT_1, linewidth=1.2, label='Signal 1')\n",
    "axes[0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[0].set_title('Signal 1 (Reference)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_xlim(0, 1)\n",
    "\n",
    "# Signal 2 original\n",
    "axes[1].plot(t, signal2, color=SUBJECT_2, linewidth=1.2, label='Signal 2 (original)')\n",
    "axes[1].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[1].set_title('Signal 2 ‚Äî Original (Phase-Locked to Signal 1)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "# Signal 2 shifted\n",
    "axes[2].plot(t, signal2_shifted, color=SECONDARY_ORANGE, linewidth=1.2, \n",
    "             label='Signal 2 (time-shifted)')\n",
    "axes[2].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[2].set_xlabel('Time (s)', fontsize=11)\n",
    "axes[2].set_title('Signal 2 ‚Äî Time-Shifted (Phase Relationship Broken)', fontsize=12, fontweight='bold')\n",
    "axes[2].legend(loc='upper right')\n",
    "\n",
    "plt.suptitle('Time Shifting: Breaks Temporal Alignment', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Time shifting is a simple way to break phase relationships.\")\n",
    "print(\"The shifted signal has the exact same samples, just in a different order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3643a15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Building the Null Distribution\n",
    "\n",
    "Now we combine surrogate methods with a connectivity metric to build a proper null distribution.\n",
    "\n",
    "### The Process\n",
    "\n",
    "1. **Compute observed connectivity** between original signals\n",
    "2. **Generate N surrogates** (typically 1000-10000)\n",
    "3. **Compute connectivity** for each surrogate pair\n",
    "4. **The distribution of surrogate values = null distribution**\n",
    "\n",
    "### Why N = 1000?\n",
    "\n",
    "The number of surrogates determines the **precision** of your p-value:\n",
    "\n",
    "| N surrogates | Minimum p-value | Precision |\n",
    "|--------------|-----------------|-----------|\n",
    "| 100 | 0.01 | ¬±0.01 |\n",
    "| 1000 | 0.001 | ¬±0.001 |\n",
    "| 10000 | 0.0001 | ¬±0.0001 |\n",
    "\n",
    "For typical significance threshold Œ± = 0.05, N = 1000 is usually sufficient.\n",
    "\n",
    "Let's implement this with PLV (Phase Locking Value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 4: Building a Null Distribution with PLV\n",
    "# ============================================================================\n",
    "\n",
    "def compute_plv(signal1: NDArray[np.floating], \n",
    "                signal2: NDArray[np.floating]) -> float:\n",
    "    \"\"\"\n",
    "    Compute Phase Locking Value between two signals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal1 : NDArray[np.floating]\n",
    "        First signal.\n",
    "    signal2 : NDArray[np.floating]\n",
    "        Second signal.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        PLV value between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Get instantaneous phases\n",
    "    phase1 = np.angle(hilbert(signal1))\n",
    "    phase2 = np.angle(hilbert(signal2))\n",
    "    \n",
    "    # Compute phase difference\n",
    "    phase_diff = phase1 - phase2\n",
    "    \n",
    "    # PLV is the mean resultant length\n",
    "    plv = np.abs(np.mean(np.exp(1j * phase_diff)))\n",
    "    \n",
    "    return plv\n",
    "\n",
    "\n",
    "def build_null_distribution(signal1: NDArray[np.floating],\n",
    "                            signal2: NDArray[np.floating],\n",
    "                            n_surrogates: int = 1000,\n",
    "                            method: str = 'phase_shuffle') -> NDArray[np.floating]:\n",
    "    \"\"\"\n",
    "    Build null distribution of PLV using surrogate data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signal1 : NDArray[np.floating]\n",
    "        First signal.\n",
    "    signal2 : NDArray[np.floating]\n",
    "        Second signal.\n",
    "    n_surrogates : int\n",
    "        Number of surrogates to generate.\n",
    "    method : str\n",
    "        'phase_shuffle' or 'time_shift'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    NDArray[np.floating]\n",
    "        Array of PLV values under the null hypothesis.\n",
    "    \"\"\"\n",
    "    null_values = np.zeros(n_surrogates)\n",
    "    \n",
    "    for i in range(n_surrogates):\n",
    "        # Create surrogate of signal2\n",
    "        if method == 'phase_shuffle':\n",
    "            surrogate = phase_shuffle(signal2)\n",
    "        else:\n",
    "            surrogate = time_shift(signal2)\n",
    "        \n",
    "        # Compute PLV\n",
    "        null_values[i] = compute_plv(signal1, surrogate)\n",
    "    \n",
    "    return null_values\n",
    "\n",
    "\n",
    "# Create bandpass filter for alpha band (8-12 Hz)\n",
    "def bandpass_filter(signal: NDArray[np.floating], \n",
    "                    low: float, high: float, \n",
    "                    fs: int) -> NDArray[np.floating]:\n",
    "    \"\"\"Apply bandpass filter to signal.\"\"\"\n",
    "    nyq = fs / 2\n",
    "    b, a = butter(4, [low/nyq, high/nyq], btype='band')\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "\n",
    "# Create test signals: weakly phase-locked\n",
    "np.random.seed(42)\n",
    "t = np.arange(0, 5, 1/fs)  # 5 seconds\n",
    "\n",
    "# Base oscillation\n",
    "alpha = np.sin(2 * np.pi * 10 * t)\n",
    "\n",
    "# Signal 1: alpha + noise\n",
    "signal1 = alpha + 0.5 * np.random.randn(len(t))\n",
    "signal1 = bandpass_filter(signal1, 8, 12, fs)\n",
    "\n",
    "# Signal 2: phase-shifted alpha + noise (weak coupling)\n",
    "phase_jitter = 0.3 * np.random.randn(len(t))  # Add some phase jitter\n",
    "signal2 = np.sin(2 * np.pi * 10 * t + np.pi/3 + np.cumsum(phase_jitter) * 0.01)\n",
    "signal2 = signal2 + 0.5 * np.random.randn(len(t))\n",
    "signal2 = bandpass_filter(signal2, 8, 12, fs)\n",
    "\n",
    "# Compute observed PLV\n",
    "observed_plv = compute_plv(signal1, signal2)\n",
    "\n",
    "# Build null distribution (use fewer for demo speed)\n",
    "print(\"Building null distribution (500 surrogates)...\")\n",
    "null_dist = build_null_distribution(signal1, signal2, n_surrogates=500, method='phase_shuffle')\n",
    "\n",
    "# Compute p-value\n",
    "p_value = np.mean(null_dist >= observed_plv)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: signals\n",
    "axes[0].plot(t[:512], signal1[:512], color=SUBJECT_1, linewidth=1.2, label='Signal 1', alpha=0.8)\n",
    "axes[0].plot(t[:512], signal2[:512], color=SUBJECT_2, linewidth=1.2, label='Signal 2', alpha=0.8)\n",
    "axes[0].set_xlabel('Time (s)', fontsize=11)\n",
    "axes[0].set_ylabel('Amplitude', fontsize=11)\n",
    "axes[0].set_title('Filtered Signals (Alpha Band: 8-12 Hz)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper right')\n",
    "axes[0].set_xlim(0, 2)\n",
    "\n",
    "# Right: null distribution\n",
    "n, bins, patches = axes[1].hist(null_dist, bins=40, density=True, \n",
    "                                 color=PRIMARY_BLUE, alpha=0.7, edgecolor='white')\n",
    "\n",
    "# Color the tail\n",
    "for patch, left_edge in zip(patches, bins[:-1]):\n",
    "    if left_edge >= observed_plv:\n",
    "        patch.set_facecolor(PRIMARY_RED)\n",
    "        patch.set_alpha(0.8)\n",
    "\n",
    "axes[1].axvline(observed_plv, color=PRIMARY_RED, linewidth=3, linestyle='--',\n",
    "                label=f'Observed PLV = {observed_plv:.3f}')\n",
    "axes[1].set_xlabel('PLV (Phase Locking Value)', fontsize=11)\n",
    "axes[1].set_ylabel('Density', fontsize=11)\n",
    "axes[1].set_title('Null Distribution from Phase-Shuffled Surrogates', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "# Add p-value annotation\n",
    "significance = \"SIGNIFICANT\" if p_value < 0.05 else \"NOT significant\"\n",
    "axes[1].annotate(f'p = {p_value:.3f}\\n({significance} at Œ±=0.05)', \n",
    "                 xy=(observed_plv, axes[1].get_ylim()[1] * 0.8),\n",
    "                 fontsize=12, fontweight='bold', \n",
    "                 color=PRIMARY_GREEN if p_value < 0.05 else PRIMARY_RED)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nObserved PLV: {observed_plv:.4f}\")\n",
    "print(f\"Null distribution: mean = {np.mean(null_dist):.4f}, std = {np.std(null_dist):.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Result: {significance} at Œ± = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed23616",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Computing P-Values\n",
    "\n",
    "The **p-value** is the probability of observing a value at least as extreme as our observed value, assuming the null hypothesis is true.\n",
    "\n",
    "### Formula\n",
    "\n",
    "For connectivity (where higher = more evidence of connectivity):\n",
    "\n",
    "$$p = \\frac{\\text{Number of surrogates} \\geq \\text{observed}}{N_{\\text{surrogates}}}$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "| P-value | Interpretation |\n",
    "|---------|---------------|\n",
    "| p < 0.001 | Strong evidence against H‚ÇÄ |\n",
    "| p < 0.01 | Moderate evidence against H‚ÇÄ |\n",
    "| p < 0.05 | Weak evidence against H‚ÇÄ |\n",
    "| p ‚â• 0.05 | Insufficient evidence to reject H‚ÇÄ |\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "‚ö†Ô∏è **P-value is NOT the probability that H‚ÇÄ is true!**\n",
    "\n",
    "It's the probability of getting data this extreme IF H‚ÇÄ were true.\n",
    "\n",
    "‚ö†Ô∏è **Threshold Œ± is chosen BEFORE looking at data!**\n",
    "\n",
    "Common choices: 0.05, 0.01, 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 5: P-Value Computation\n",
    "# ============================================================================\n",
    "\n",
    "def compute_pvalue(observed: float, \n",
    "                   null_distribution: NDArray[np.floating],\n",
    "                   alternative: str = 'greater') -> float:\n",
    "    \"\"\"\n",
    "    Compute p-value from null distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observed : float\n",
    "        Observed connectivity value.\n",
    "    null_distribution : NDArray[np.floating]\n",
    "        Null distribution values.\n",
    "    alternative : str\n",
    "        'greater': test if observed > null (typical for connectivity)\n",
    "        'less': test if observed < null\n",
    "        'two-sided': test if observed differs from null\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        P-value.\n",
    "    \"\"\"\n",
    "    n = len(null_distribution)\n",
    "    \n",
    "    if alternative == 'greater':\n",
    "        p = (np.sum(null_distribution >= observed) + 1) / (n + 1)\n",
    "    elif alternative == 'less':\n",
    "        p = (np.sum(null_distribution <= observed) + 1) / (n + 1)\n",
    "    else:  # two-sided\n",
    "        mean_null = np.mean(null_distribution)\n",
    "        deviation = np.abs(observed - mean_null)\n",
    "        p = (np.sum(np.abs(null_distribution - mean_null) >= deviation) + 1) / (n + 1)\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "# Demonstrate with different observed values\n",
    "np.random.seed(42)\n",
    "demo_null = np.random.beta(2, 8, 1000) * 0.5  # Null distribution\n",
    "\n",
    "observed_values = [0.15, 0.25, 0.35, 0.45]\n",
    "colors = [PRIMARY_BLUE, SECONDARY_ORANGE, SECONDARY_PURPLE, PRIMARY_RED]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot null distribution\n",
    "ax.hist(demo_null, bins=50, density=True, color='lightgray', \n",
    "        edgecolor='white', alpha=0.7, label='Null distribution')\n",
    "\n",
    "# Add observed values\n",
    "for obs, color in zip(observed_values, colors):\n",
    "    pval = compute_pvalue(obs, demo_null)\n",
    "    ax.axvline(obs, color=color, linewidth=2.5, linestyle='--',\n",
    "               label=f'Obs = {obs:.2f}, p = {pval:.3f}')\n",
    "\n",
    "ax.set_xlabel('Connectivity Value', fontsize=12)\n",
    "ax.set_ylabel('Density', fontsize=12)\n",
    "ax.set_title('Different Observed Values ‚Üí Different P-Values', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "# Add significance threshold\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations further in the tail ‚Üí smaller p-values\")\n",
    "print(\"The further from the null, the more 'surprising' the result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf5ffb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: The Multiple Comparisons Problem\n",
    "\n",
    "In connectivity analysis, we often test **many pairs** simultaneously. With a 64-channel EEG:\n",
    "\n",
    "$$\\text{Number of pairs} = \\frac{64 \\times 63}{2} = 2016 \\text{ pairs}$$\n",
    "\n",
    "### The Problem\n",
    "\n",
    "If we test each pair at Œ± = 0.05:\n",
    "- Expected false positives = 2016 √ó 0.05 ‚âà **101 false connections!**\n",
    "\n",
    "This is called the **multiple comparisons problem** or **family-wise error rate (FWER)** inflation.\n",
    "\n",
    "### Visual Intuition\n",
    "\n",
    "Imagine flipping a fair coin 2016 times. You'd expect about 101 heads by chance alone. Similarly, testing 2016 pairs will give ~101 \"significant\" results even when there's NO true connectivity!\n",
    "\n",
    "### Solutions\n",
    "\n",
    "We need to **correct** our significance threshold. Two main approaches:\n",
    "\n",
    "1. **Bonferroni correction**: Control the family-wise error rate (FWER)\n",
    "2. **FDR correction**: Control the false discovery rate (FDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5730533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 6: The Multiple Comparisons Problem\n",
    "# ============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate testing 100 pairs with NO true connectivity\n",
    "n_tests = 100\n",
    "alpha = 0.05\n",
    "\n",
    "# Generate p-values under null (uniform distribution)\n",
    "# When H0 is true, p-values are uniformly distributed between 0 and 1\n",
    "pvalues_null = np.random.uniform(0, 1, n_tests)\n",
    "\n",
    "# Count false positives\n",
    "false_positives = np.sum(pvalues_null < alpha)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: p-value distribution\n",
    "axes[0].hist(pvalues_null, bins=20, color=PRIMARY_BLUE, edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(alpha, color=PRIMARY_RED, linewidth=2, linestyle='--', \n",
    "                label=f'Œ± = {alpha}')\n",
    "axes[0].fill_between([0, alpha], 0, axes[0].get_ylim()[1] + 5, \n",
    "                      color=PRIMARY_RED, alpha=0.2, label=f'Rejected ({false_positives})')\n",
    "axes[0].set_xlabel('P-value', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('P-Values When H‚ÇÄ is TRUE for ALL Tests', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 15)\n",
    "\n",
    "# Right: expected vs observed false positives\n",
    "n_simulations = 1000\n",
    "false_positive_counts = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    pvals = np.random.uniform(0, 1, n_tests)\n",
    "    fp = np.sum(pvals < alpha)\n",
    "    false_positive_counts.append(fp)\n",
    "\n",
    "axes[1].hist(false_positive_counts, bins=range(0, 20), color=PRIMARY_RED, \n",
    "             edgecolor='white', alpha=0.7, density=True)\n",
    "axes[1].axvline(n_tests * alpha, color='black', linewidth=2, linestyle='-',\n",
    "                label=f'Expected: {n_tests * alpha:.0f}')\n",
    "axes[1].set_xlabel('Number of False Positives', fontsize=12)\n",
    "axes[1].set_ylabel('Probability', fontsize=12)\n",
    "axes[1].set_title(f'False Positives Distribution ({n_tests} tests, Œ±={alpha})', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('The Multiple Comparisons Problem: False Positives Accumulate!', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"With {n_tests} tests at Œ± = {alpha}:\")\n",
    "print(f\"  Expected false positives: {n_tests * alpha:.0f}\")\n",
    "print(f\"  Actual false positives (this simulation): {false_positives}\")\n",
    "print(f\"\\nWith 2016 EEG pairs: expected ~{int(2016 * 0.05)} false connections!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de18f3c7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Bonferroni Correction\n",
    "\n",
    "The **Bonferroni correction** is the simplest and most conservative approach.\n",
    "\n",
    "### The Idea\n",
    "\n",
    "Divide the significance threshold by the number of tests:\n",
    "\n",
    "$$\\alpha_{\\text{corrected}} = \\frac{\\alpha}{N_{\\text{tests}}}$$\n",
    "\n",
    "### Example\n",
    "\n",
    "With 100 tests and Œ± = 0.05:\n",
    "\n",
    "$$\\alpha_{\\text{corrected}} = \\frac{0.05}{100} = 0.0005$$\n",
    "\n",
    "### Properties\n",
    "\n",
    "| Aspect | Assessment |\n",
    "|--------|------------|\n",
    "| Controls | Family-Wise Error Rate (FWER) |\n",
    "| Conservative? | ‚úì Very conservative |\n",
    "| Power | ‚Üì Reduced (misses true effects) |\n",
    "| Best for | Few tests, need strict control |\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- You have **few** tests (< 20)\n",
    "- False positives are **very costly**\n",
    "- You need to claim \"at least one\" significant result\n",
    "\n",
    "### When NOT to Use\n",
    "\n",
    "- Many tests (> 100) ‚Äî too conservative\n",
    "- Exploratory analysis ‚Äî misses true effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79786a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 7: Bonferroni Correction\n",
    "# ============================================================================\n",
    "\n",
    "def bonferroni_correction(pvalues: NDArray[np.floating], \n",
    "                          alpha: float = 0.05) -> Tuple[NDArray[np.bool_], float]:\n",
    "    \"\"\"\n",
    "    Apply Bonferroni correction to p-values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pvalues : NDArray[np.floating]\n",
    "        Array of p-values.\n",
    "    alpha : float\n",
    "        Desired family-wise error rate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[NDArray[np.bool_], float]\n",
    "        Boolean mask of significant tests, and corrected alpha.\n",
    "    \"\"\"\n",
    "    n_tests = len(pvalues)\n",
    "    alpha_corrected = alpha / n_tests\n",
    "    significant = pvalues < alpha_corrected\n",
    "    \n",
    "    return significant, alpha_corrected\n",
    "\n",
    "\n",
    "# Simulate scenario with some true effects\n",
    "np.random.seed(42)\n",
    "n_tests = 50\n",
    "n_true_effects = 5  # 5 pairs with real connectivity\n",
    "\n",
    "# Generate p-values\n",
    "pvalues = np.random.uniform(0, 1, n_tests)\n",
    "# Make some small (true effects)\n",
    "true_effect_indices = np.random.choice(n_tests, n_true_effects, replace=False)\n",
    "pvalues[true_effect_indices] = np.random.uniform(0.001, 0.03, n_true_effects)\n",
    "\n",
    "# Apply corrections\n",
    "alpha = 0.05\n",
    "alpha_bonf = alpha / n_tests\n",
    "\n",
    "# Uncorrected\n",
    "sig_uncorrected = pvalues < alpha\n",
    "# Bonferroni\n",
    "sig_bonf, _ = bonferroni_correction(pvalues, alpha)\n",
    "\n",
    "# Sort for visualization\n",
    "sort_idx = np.argsort(pvalues)\n",
    "pvalues_sorted = pvalues[sort_idx]\n",
    "\n",
    "# Track which are true effects\n",
    "is_true_effect = np.isin(np.arange(n_tests), true_effect_indices)\n",
    "is_true_effect_sorted = is_true_effect[sort_idx]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(n_tests)\n",
    "\n",
    "# Plot all p-values\n",
    "colors_bars = [PRIMARY_GREEN if te else PRIMARY_BLUE for te in is_true_effect_sorted]\n",
    "bars = ax.bar(x, pvalues_sorted, color=colors_bars, edgecolor='white', alpha=0.7)\n",
    "\n",
    "# Threshold lines\n",
    "ax.axhline(alpha, color=SECONDARY_ORANGE, linewidth=2, linestyle='--',\n",
    "           label=f'Uncorrected Œ± = {alpha}')\n",
    "ax.axhline(alpha_bonf, color=PRIMARY_RED, linewidth=2, linestyle='-',\n",
    "           label=f'Bonferroni Œ± = {alpha_bonf:.4f}')\n",
    "\n",
    "ax.set_xlabel('Test (sorted by p-value)', fontsize=12)\n",
    "ax.set_ylabel('P-value', fontsize=12)\n",
    "ax.set_title('Bonferroni Correction: Very Conservative Threshold', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "ax.set_ylim(0, 0.1)\n",
    "ax.set_xlim(-1, n_tests)\n",
    "\n",
    "# Add legend for bar colors\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=PRIMARY_GREEN, alpha=0.7, label='True effect'),\n",
    "                   Patch(facecolor=PRIMARY_BLUE, alpha=0.7, label='Null (no effect)')]\n",
    "ax.legend(handles=legend_elements + ax.get_legend_handles_labels()[0], \n",
    "          loc='upper right', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"Total tests: {n_tests}\")\n",
    "print(f\"True effects: {n_true_effects}\")\n",
    "print(f\"\\nUncorrected (Œ± = {alpha}):\")\n",
    "print(f\"  Significant: {np.sum(sig_uncorrected)}\")\n",
    "print(f\"  True positives: {np.sum(sig_uncorrected & is_true_effect)}\")\n",
    "print(f\"  False positives: {np.sum(sig_uncorrected & ~is_true_effect)}\")\n",
    "print(f\"\\nBonferroni (Œ± = {alpha_bonf:.5f}):\")\n",
    "print(f\"  Significant: {np.sum(sig_bonf)}\")\n",
    "print(f\"  True positives: {np.sum(sig_bonf & is_true_effect)}\")\n",
    "print(f\"  False positives: {np.sum(sig_bonf & ~is_true_effect)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62696ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: False Discovery Rate (FDR) Correction\n",
    "\n",
    "**FDR correction** (Benjamini-Hochberg procedure) is less conservative and more commonly used in neuroimaging.\n",
    "\n",
    "### What Does FDR Control?\n",
    "\n",
    "Instead of controlling the probability of **any** false positive (FWER), FDR controls the **proportion** of false positives among all discoveries.\n",
    "\n",
    "$$\\text{FDR} = \\mathbb{E}\\left[\\frac{\\text{False Positives}}{\\text{Total Discoveries}}\\right]$$\n",
    "\n",
    "### The Benjamini-Hochberg Procedure\n",
    "\n",
    "1. **Sort** p-values from smallest to largest: $p_{(1)} \\leq p_{(2)} \\leq ... \\leq p_{(N)}$\n",
    "2. **Find** the largest $k$ such that: $p_{(k)} \\leq \\frac{k}{N} \\cdot \\alpha$\n",
    "3. **Reject** all hypotheses with $p_{(i)} \\leq p_{(k)}$\n",
    "\n",
    "### Comparison with Bonferroni\n",
    "\n",
    "| Aspect | Bonferroni | FDR (BH) |\n",
    "|--------|------------|----------|\n",
    "| Controls | FWER | FDR |\n",
    "| Stringency | Very conservative | Moderate |\n",
    "| Power | Low | Higher |\n",
    "| Interpretation | \"No false positives\" | \"‚â§5% of discoveries are false\" |\n",
    "| Best for | Confirmatory | Exploratory |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 8: FDR Correction (Benjamini-Hochberg)\n",
    "# ============================================================================\n",
    "\n",
    "def fdr_correction(pvalues: NDArray[np.floating], \n",
    "                   alpha: float = 0.05) -> Tuple[NDArray[np.bool_], NDArray[np.floating]]:\n",
    "    \"\"\"\n",
    "    Apply FDR correction using Benjamini-Hochberg procedure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pvalues : NDArray[np.floating]\n",
    "        Array of p-values.\n",
    "    alpha : float\n",
    "        Desired false discovery rate.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[NDArray[np.bool_], NDArray[np.floating]]\n",
    "        Boolean mask of significant tests, and adjusted p-values.\n",
    "    \"\"\"\n",
    "    n = len(pvalues)\n",
    "    \n",
    "    # Sort p-values and keep track of original order\n",
    "    sorted_idx = np.argsort(pvalues)\n",
    "    sorted_pvals = pvalues[sorted_idx]\n",
    "    \n",
    "    # Compute BH threshold for each rank\n",
    "    ranks = np.arange(1, n + 1)\n",
    "    bh_threshold = ranks / n * alpha\n",
    "    \n",
    "    # Find the largest p-value below its threshold\n",
    "    below_threshold = sorted_pvals <= bh_threshold\n",
    "    if np.any(below_threshold):\n",
    "        max_below = np.max(np.where(below_threshold)[0])\n",
    "        reject_sorted = np.arange(n) <= max_below\n",
    "    else:\n",
    "        reject_sorted = np.zeros(n, dtype=bool)\n",
    "    \n",
    "    # Map back to original order\n",
    "    reject = np.zeros(n, dtype=bool)\n",
    "    reject[sorted_idx] = reject_sorted\n",
    "    \n",
    "    # Compute adjusted p-values\n",
    "    adjusted = np.zeros(n)\n",
    "    adjusted[sorted_idx] = np.minimum.accumulate(\n",
    "        (sorted_pvals * n / ranks)[::-1]\n",
    "    )[::-1]\n",
    "    adjusted = np.minimum(adjusted, 1.0)\n",
    "    \n",
    "    return reject, adjusted\n",
    "\n",
    "\n",
    "# Use same data as before\n",
    "np.random.seed(42)\n",
    "n_tests = 50\n",
    "n_true_effects = 5\n",
    "\n",
    "pvalues = np.random.uniform(0, 1, n_tests)\n",
    "true_effect_indices = np.random.choice(n_tests, n_true_effects, replace=False)\n",
    "pvalues[true_effect_indices] = np.random.uniform(0.001, 0.03, n_true_effects)\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "# Apply corrections\n",
    "sig_uncorr = pvalues < alpha\n",
    "sig_bonf, alpha_bonf = bonferroni_correction(pvalues, alpha)\n",
    "sig_fdr, adj_pvals = fdr_correction(pvalues, alpha)\n",
    "\n",
    "# Track true effects\n",
    "is_true_effect = np.isin(np.arange(n_tests), true_effect_indices)\n",
    "\n",
    "# Sort for visualization\n",
    "sort_idx = np.argsort(pvalues)\n",
    "pvalues_sorted = pvalues[sort_idx]\n",
    "is_true_effect_sorted = is_true_effect[sort_idx]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: BH procedure visualization\n",
    "x = np.arange(n_tests)\n",
    "bh_line = (x + 1) / n_tests * alpha\n",
    "\n",
    "colors_bars = [PRIMARY_GREEN if te else PRIMARY_BLUE for te in is_true_effect_sorted]\n",
    "axes[0].bar(x, pvalues_sorted, color=colors_bars, edgecolor='white', alpha=0.7)\n",
    "axes[0].plot(x, bh_line, color=PRIMARY_RED, linewidth=2, label='BH threshold line')\n",
    "axes[0].axhline(alpha_bonf, color=SECONDARY_ORANGE, linewidth=2, linestyle='--',\n",
    "                label=f'Bonferroni (Œ± = {alpha_bonf:.4f})')\n",
    "\n",
    "axes[0].set_xlabel('Rank', fontsize=12)\n",
    "axes[0].set_ylabel('P-value', fontsize=12)\n",
    "axes[0].set_title('Benjamini-Hochberg: Adaptive Threshold', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='upper left', fontsize=10)\n",
    "axes[0].set_ylim(0, 0.15)\n",
    "\n",
    "# Right: Comparison of methods\n",
    "methods = ['Uncorrected', 'Bonferroni', 'FDR (BH)']\n",
    "true_positives = [np.sum(sig_uncorr & is_true_effect),\n",
    "                  np.sum(sig_bonf & is_true_effect),\n",
    "                  np.sum(sig_fdr & is_true_effect)]\n",
    "false_positives = [np.sum(sig_uncorr & ~is_true_effect),\n",
    "                   np.sum(sig_bonf & ~is_true_effect),\n",
    "                   np.sum(sig_fdr & ~is_true_effect)]\n",
    "\n",
    "x_bar = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1].bar(x_bar - width/2, true_positives, width, \n",
    "                    label='True Positives', color=PRIMARY_GREEN, alpha=0.8)\n",
    "bars2 = axes[1].bar(x_bar + width/2, false_positives, width, \n",
    "                    label='False Positives', color=PRIMARY_RED, alpha=0.8)\n",
    "\n",
    "axes[1].axhline(n_true_effects, color='black', linestyle='--', alpha=0.5,\n",
    "                label=f'Max possible TP = {n_true_effects}')\n",
    "\n",
    "axes[1].set_xlabel('Correction Method', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Comparison: True vs False Positives', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(x_bar)\n",
    "axes[1].set_xticklabels(methods)\n",
    "axes[1].legend(loc='upper right', fontsize=10)\n",
    "axes[1].set_ylim(0, max(true_positives + false_positives) + 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[1].annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                     ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1].annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                     ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('FDR vs Bonferroni: Better Balance of Power vs Control', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"True effects in data: {n_true_effects}\")\n",
    "print(f\"\\nMethod comparison:\")\n",
    "print(f\"  Uncorrected: {np.sum(sig_uncorr)} significant ({np.sum(sig_uncorr & is_true_effect)} TP, {np.sum(sig_uncorr & ~is_true_effect)} FP)\")\n",
    "print(f\"  Bonferroni:  {np.sum(sig_bonf)} significant ({np.sum(sig_bonf & is_true_effect)} TP, {np.sum(sig_bonf & ~is_true_effect)} FP)\")\n",
    "print(f\"  FDR (BH):    {np.sum(sig_fdr)} significant ({np.sum(sig_fdr & is_true_effect)} TP, {np.sum(sig_fdr & ~is_true_effect)} FP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d57d1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 11: Permutation Testing for Group Comparisons\n",
    "\n",
    "So far we tested if connectivity is **different from zero**. But often we want to compare **two groups**:\n",
    "\n",
    "- Is connectivity higher in patients than controls?\n",
    "- Is there a difference between conditions?\n",
    "\n",
    "### Permutation Testing\n",
    "\n",
    "Permutation testing is a **non-parametric** approach that makes no assumptions about the data distribution.\n",
    "\n",
    "### The Procedure\n",
    "\n",
    "1. Compute the **observed difference** between groups (e.g., mean connectivity)\n",
    "2. **Pool** all observations together\n",
    "3. **Randomly permute** group labels (shuffle who belongs to which group)\n",
    "4. Compute the difference for this permuted data\n",
    "5. Repeat steps 3-4 many times (e.g., 1000)\n",
    "6. The distribution of permuted differences = **null distribution**\n",
    "7. Compare observed difference to null distribution ‚Üí p-value\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "Under H‚ÇÄ, group membership doesn't matter. So shuffling labels should give similar results to the true labels. If the observed difference is extreme, it's unlikely to be due to chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15199bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 9: Permutation Testing for Group Comparison\n",
    "# ============================================================================\n",
    "\n",
    "def permutation_test(group1: NDArray[np.floating],\n",
    "                     group2: NDArray[np.floating],\n",
    "                     n_permutations: int = 1000,\n",
    "                     statistic: str = 'mean_diff') -> Tuple[float, float, NDArray[np.floating]]:\n",
    "    \"\"\"\n",
    "    Perform a permutation test comparing two groups.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group1 : NDArray[np.floating]\n",
    "        Connectivity values for group 1.\n",
    "    group2 : NDArray[np.floating]\n",
    "        Connectivity values for group 2.\n",
    "    n_permutations : int\n",
    "        Number of permutations.\n",
    "    statistic : str\n",
    "        'mean_diff' or 't_stat'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, NDArray[np.floating]]\n",
    "        Observed statistic, p-value, and null distribution.\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    pooled = np.concatenate([group1, group2])\n",
    "    \n",
    "    # Compute observed statistic\n",
    "    if statistic == 'mean_diff':\n",
    "        observed = np.mean(group1) - np.mean(group2)\n",
    "    else:\n",
    "        observed = stats.ttest_ind(group1, group2)[0]\n",
    "    \n",
    "    # Generate null distribution\n",
    "    null_stats = np.zeros(n_permutations)\n",
    "    \n",
    "    for i in range(n_permutations):\n",
    "        # Shuffle and split\n",
    "        np.random.shuffle(pooled)\n",
    "        perm_g1 = pooled[:n1]\n",
    "        perm_g2 = pooled[n1:]\n",
    "        \n",
    "        if statistic == 'mean_diff':\n",
    "            null_stats[i] = np.mean(perm_g1) - np.mean(perm_g2)\n",
    "        else:\n",
    "            null_stats[i] = stats.ttest_ind(perm_g1, perm_g2)[0]\n",
    "    \n",
    "    # Two-sided p-value\n",
    "    p_value = np.mean(np.abs(null_stats) >= np.abs(observed))\n",
    "    \n",
    "    return observed, p_value, null_stats\n",
    "\n",
    "\n",
    "# Simulate connectivity data from two groups\n",
    "np.random.seed(42)\n",
    "\n",
    "# Group 1 (e.g., controls): lower connectivity\n",
    "group1_plv = np.random.beta(2, 5, 20) * 0.5 + 0.15\n",
    "# Group 2 (e.g., patients): higher connectivity\n",
    "group2_plv = np.random.beta(3, 4, 20) * 0.5 + 0.25\n",
    "\n",
    "# Permutation test\n",
    "observed_diff, pvalue, null_dist = permutation_test(group2_plv, group1_plv, n_permutations=1000)\n",
    "\n",
    "# Also do parametric t-test for comparison\n",
    "t_stat, t_pvalue = stats.ttest_ind(group2_plv, group1_plv)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Group comparison\n",
    "positions = [1, 2]\n",
    "bp = axes[0].boxplot([group1_plv, group2_plv], positions=positions, widths=0.5,\n",
    "                      patch_artist=True)\n",
    "bp['boxes'][0].set_facecolor(SUBJECT_1)\n",
    "bp['boxes'][1].set_facecolor(SUBJECT_2)\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "\n",
    "axes[0].scatter(np.ones(len(group1_plv)) + np.random.randn(len(group1_plv)) * 0.05, \n",
    "                group1_plv, color=SUBJECT_1, alpha=0.6, s=50)\n",
    "axes[0].scatter(np.ones(len(group2_plv)) * 2 + np.random.randn(len(group2_plv)) * 0.05, \n",
    "                group2_plv, color=SUBJECT_2, alpha=0.6, s=50)\n",
    "\n",
    "axes[0].set_xticks([1, 2])\n",
    "axes[0].set_xticklabels(['Group 1\\n(Controls)', 'Group 2\\n(Patients)'])\n",
    "axes[0].set_ylabel('PLV', fontsize=12)\n",
    "axes[0].set_title('Connectivity by Group', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add means\n",
    "axes[0].scatter([1, 2], [np.mean(group1_plv), np.mean(group2_plv)], \n",
    "                color='black', s=100, marker='D', zorder=5, label='Mean')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: Null distribution\n",
    "axes[1].hist(null_dist, bins=40, density=True, color=PRIMARY_BLUE, \n",
    "             edgecolor='white', alpha=0.7)\n",
    "axes[1].axvline(observed_diff, color=PRIMARY_RED, linewidth=3, linestyle='--',\n",
    "                label=f'Observed diff = {observed_diff:.3f}')\n",
    "axes[1].axvline(-observed_diff, color=PRIMARY_RED, linewidth=3, linestyle='--', alpha=0.5)\n",
    "\n",
    "axes[1].set_xlabel('Difference (Group 2 - Group 1)', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Permutation Null Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "plt.suptitle(f'Permutation Test: p = {pvalue:.3f} (t-test p = {t_pvalue:.3f})', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Group 1 (Controls): mean = {np.mean(group1_plv):.3f}\")\n",
    "print(f\"Group 2 (Patients): mean = {np.mean(group2_plv):.3f}\")\n",
    "print(f\"Observed difference: {observed_diff:.3f}\")\n",
    "print(f\"\\nPermutation test p-value: {pvalue:.3f}\")\n",
    "print(f\"Parametric t-test p-value: {t_pvalue:.3f}\")\n",
    "print(f\"\\n‚Üí {'Significant' if pvalue < 0.05 else 'Not significant'} difference at Œ± = 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71afc0c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 12: Bootstrap Confidence Intervals\n",
    "\n",
    "P-values tell us whether an effect exists. **Confidence intervals** tell us **how big** it is.\n",
    "\n",
    "### What is Bootstrapping?\n",
    "\n",
    "Bootstrapping is a resampling technique:\n",
    "\n",
    "1. **Resample with replacement** from your data (same size as original)\n",
    "2. Compute the statistic of interest\n",
    "3. Repeat many times (e.g., 1000)\n",
    "4. The distribution of statistics gives you uncertainty estimates\n",
    "\n",
    "### Confidence Interval\n",
    "\n",
    "A 95% confidence interval is the range from the 2.5th to 97.5th percentile of bootstrap samples.\n",
    "\n",
    "**Interpretation**: If we repeated the experiment many times, 95% of the intervals would contain the true value.\n",
    "\n",
    "### Why Bootstrapping for Connectivity?\n",
    "\n",
    "- No assumptions about the distribution\n",
    "- Works for complex statistics\n",
    "- Provides intuitive uncertainty quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 10: Bootstrap Confidence Intervals\n",
    "# ============================================================================\n",
    "\n",
    "def bootstrap_ci(data: NDArray[np.floating],\n",
    "                 statistic: callable = np.mean,\n",
    "                 n_bootstrap: int = 1000,\n",
    "                 ci: float = 0.95) -> Tuple[float, float, float, NDArray[np.floating]]:\n",
    "    \"\"\"\n",
    "    Compute bootstrap confidence interval for a statistic.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : NDArray[np.floating]\n",
    "        Input data array.\n",
    "    statistic : callable\n",
    "        Function to compute the statistic (e.g., np.mean).\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap resamples.\n",
    "    ci : float\n",
    "        Confidence level (e.g., 0.95 for 95%).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[float, float, float, NDArray[np.floating]]\n",
    "        Point estimate, CI lower, CI upper, and bootstrap distribution.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    point_estimate = statistic(data)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    bootstrap_stats = np.zeros(n_bootstrap)\n",
    "    for i in range(n_bootstrap):\n",
    "        resample = np.random.choice(data, size=n, replace=True)\n",
    "        bootstrap_stats[i] = statistic(resample)\n",
    "    \n",
    "    # Percentile method for CI\n",
    "    alpha = (1 - ci) / 2\n",
    "    ci_lower = np.percentile(bootstrap_stats, alpha * 100)\n",
    "    ci_upper = np.percentile(bootstrap_stats, (1 - alpha) * 100)\n",
    "    \n",
    "    return point_estimate, ci_lower, ci_upper, bootstrap_stats\n",
    "\n",
    "\n",
    "# Simulate PLV measurements from multiple trials\n",
    "np.random.seed(42)\n",
    "n_trials = 30\n",
    "\n",
    "# Subject 1: moderate connectivity\n",
    "plv_trials_s1 = np.random.beta(4, 3, n_trials) * 0.6 + 0.2\n",
    "\n",
    "# Subject 2: lower connectivity\n",
    "plv_trials_s2 = np.random.beta(2, 4, n_trials) * 0.5 + 0.1\n",
    "\n",
    "# Bootstrap for each subject\n",
    "mean1, ci1_low, ci1_up, boot1 = bootstrap_ci(plv_trials_s1, n_bootstrap=1000)\n",
    "mean2, ci2_low, ci2_up, boot2 = bootstrap_ci(plv_trials_s2, n_bootstrap=1000)\n",
    "\n",
    "# Bootstrap for the difference\n",
    "diff_trials = plv_trials_s1 - plv_trials_s2\n",
    "mean_diff, ci_diff_low, ci_diff_up, boot_diff = bootstrap_ci(diff_trials, n_bootstrap=1000)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Left: Raw data with CIs\n",
    "x = [1, 2]\n",
    "means = [mean1, mean2]\n",
    "ci_lows = [ci1_low, ci2_low]\n",
    "ci_ups = [ci1_up, ci2_up]\n",
    "\n",
    "for i, (m, low, up, color) in enumerate(zip(means, ci_lows, ci_ups, [SUBJECT_1, SUBJECT_2])):\n",
    "    axes[0].bar(x[i], m, color=color, alpha=0.7, width=0.5)\n",
    "    axes[0].errorbar(x[i], m, yerr=[[m - low], [up - m]], \n",
    "                     color='black', capsize=5, capthick=2, linewidth=2)\n",
    "\n",
    "axes[0].set_xticks([1, 2])\n",
    "axes[0].set_xticklabels(['Subject 1', 'Subject 2'])\n",
    "axes[0].set_ylabel('Mean PLV', fontsize=12)\n",
    "axes[0].set_title('Mean Connectivity with 95% CI', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim(0, 0.8)\n",
    "\n",
    "# Middle: Bootstrap distributions\n",
    "axes[1].hist(boot1, bins=40, density=True, color=SUBJECT_1, alpha=0.6, label='Subject 1')\n",
    "axes[1].hist(boot2, bins=40, density=True, color=SUBJECT_2, alpha=0.6, label='Subject 2')\n",
    "axes[1].axvline(mean1, color=SUBJECT_1, linewidth=2, linestyle='--')\n",
    "axes[1].axvline(mean2, color=SUBJECT_2, linewidth=2, linestyle='--')\n",
    "axes[1].set_xlabel('Mean PLV', fontsize=12)\n",
    "axes[1].set_ylabel('Density', fontsize=12)\n",
    "axes[1].set_title('Bootstrap Distributions', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Right: Difference distribution\n",
    "axes[2].hist(boot_diff, bins=40, density=True, color=SECONDARY_PURPLE, \n",
    "             alpha=0.7, edgecolor='white')\n",
    "axes[2].axvline(mean_diff, color=PRIMARY_RED, linewidth=3, linestyle='--',\n",
    "                label=f'Mean diff = {mean_diff:.3f}')\n",
    "axes[2].axvline(0, color='black', linewidth=2, linestyle='-', alpha=0.5)\n",
    "axes[2].axvspan(ci_diff_low, ci_diff_up, color=SECONDARY_PURPLE, alpha=0.2,\n",
    "                label=f'95% CI: [{ci_diff_low:.3f}, {ci_diff_up:.3f}]')\n",
    "axes[2].set_xlabel('Difference (S1 - S2)', fontsize=12)\n",
    "axes[2].set_ylabel('Density', fontsize=12)\n",
    "axes[2].set_title('Difference: Does CI Include 0?', fontsize=12, fontweight='bold')\n",
    "axes[2].legend(loc='upper left', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Subject 1: mean = {mean1:.3f}, 95% CI = [{ci1_low:.3f}, {ci1_up:.3f}]\")\n",
    "print(f\"Subject 2: mean = {mean2:.3f}, 95% CI = [{ci2_low:.3f}, {ci2_up:.3f}]\")\n",
    "print(f\"\\nDifference (S1 - S2):\")\n",
    "print(f\"  Mean = {mean_diff:.3f}\")\n",
    "print(f\"  95% CI = [{ci_diff_low:.3f}, {ci_diff_up:.3f}]\")\n",
    "if ci_diff_low > 0 or ci_diff_up < 0:\n",
    "    print(\"  ‚Üí CI does NOT include 0 ‚Üí Significant difference!\")\n",
    "else:\n",
    "    print(\"  ‚Üí CI includes 0 ‚Üí NOT significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589355a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 13: Effect Size ‚Äî Beyond P-Values\n",
    "\n",
    "P-values can be misleading! With large samples, even **tiny** effects become \"significant.\" Effect size tells us **how meaningful** the effect is.\n",
    "\n",
    "### Common Effect Size Measures\n",
    "\n",
    "| Measure | Formula | Interpretation |\n",
    "|---------|---------|----------------|\n",
    "| Cohen's d | $d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}$ | 0.2 small, 0.5 medium, 0.8 large |\n",
    "| Pearson's r | Correlation coefficient | Strength of relationship |\n",
    "| Hedge's g | Corrected Cohen's d | Better for small samples |\n",
    "\n",
    "### Why Effect Size Matters\n",
    "\n",
    "- **Statistical significance ‚â† Practical significance**\n",
    "- A \"significant\" PLV difference of 0.01 may be meaningless\n",
    "- Always report both p-values AND effect sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 11: Effect Size Demonstration\n",
    "# ============================================================================\n",
    "\n",
    "def cohens_d(group1: NDArray[np.floating], \n",
    "             group2: NDArray[np.floating]) -> float:\n",
    "    \"\"\"\n",
    "    Compute Cohen's d effect size.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group1 : NDArray[np.floating]\n",
    "        First group values.\n",
    "    group2 : NDArray[np.floating]\n",
    "        Second group values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Cohen's d effect size.\n",
    "    \"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    \n",
    "    # Pooled standard deviation\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "\n",
    "# Demonstrate: same p-value, different effect sizes\n",
    "np.random.seed(42)\n",
    "\n",
    "# Scenario 1: Small sample, large effect\n",
    "g1_small = np.random.normal(0.5, 0.1, 15)\n",
    "g2_small = np.random.normal(0.3, 0.1, 15)\n",
    "\n",
    "# Scenario 2: Large sample, small effect\n",
    "g1_large = np.random.normal(0.35, 0.1, 200)\n",
    "g2_large = np.random.normal(0.32, 0.1, 200)\n",
    "\n",
    "# Statistics\n",
    "_, p_small = stats.ttest_ind(g1_small, g2_small)\n",
    "_, p_large = stats.ttest_ind(g1_large, g2_large)\n",
    "\n",
    "d_small = cohens_d(g1_small, g2_small)\n",
    "d_large = cohens_d(g1_large, g2_large)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Small sample, large effect\n",
    "bp1 = axes[0].boxplot([g1_small, g2_small], patch_artist=True)\n",
    "bp1['boxes'][0].set_facecolor(SUBJECT_1)\n",
    "bp1['boxes'][1].set_facecolor(SUBJECT_2)\n",
    "for box in bp1['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "axes[0].set_xticklabels(['Group 1', 'Group 2'])\n",
    "axes[0].set_ylabel('Connectivity', fontsize=12)\n",
    "axes[0].set_title(f'Small Sample (n=15 per group)\\np = {p_small:.3f}, Cohen\\'s d = {d_small:.2f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylim(0, 0.8)\n",
    "\n",
    "# Right: Large sample, small effect\n",
    "bp2 = axes[1].boxplot([g1_large, g2_large], patch_artist=True)\n",
    "bp2['boxes'][0].set_facecolor(SUBJECT_1)\n",
    "bp2['boxes'][1].set_facecolor(SUBJECT_2)\n",
    "for box in bp2['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "axes[1].set_xticklabels(['Group 1', 'Group 2'])\n",
    "axes[1].set_ylabel('Connectivity', fontsize=12)\n",
    "axes[1].set_title(f'Large Sample (n=200 per group)\\np = {p_large:.3f}, Cohen\\'s d = {d_large:.2f}', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylim(0, 0.8)\n",
    "\n",
    "plt.suptitle('Same P-Value, Different Effect Sizes!', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Scenario 1 (small sample):\")\n",
    "print(f\"  p = {p_small:.3f}, Cohen's d = {d_small:.2f} ‚Üí LARGE effect\")\n",
    "print(f\"\\nScenario 2 (large sample):\")\n",
    "print(f\"  p = {p_large:.3f}, Cohen's d = {d_large:.2f} ‚Üí SMALL effect\")\n",
    "print(\"\\n‚ö†Ô∏è Both have similar p-values, but very different practical importance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44ed747",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 14: Best Practices Checklist\n",
    "\n",
    "### Before Analysis\n",
    "\n",
    "- [ ] Define your hypothesis BEFORE looking at data\n",
    "- [ ] Pre-register your analysis plan if possible\n",
    "- [ ] Choose Œ± level (typically 0.05) BEFORE analysis\n",
    "- [ ] Decide on correction method (FDR vs Bonferroni) BEFORE analysis\n",
    "\n",
    "### During Analysis\n",
    "\n",
    "- [ ] Use appropriate surrogate method for your connectivity metric\n",
    "- [ ] Generate enough surrogates (‚â•1000 for Œ±=0.05)\n",
    "- [ ] Apply multiple comparisons correction\n",
    "- [ ] Compute effect sizes alongside p-values\n",
    "- [ ] Report confidence intervals\n",
    "\n",
    "### Reporting Results\n",
    "\n",
    "- [ ] Report exact p-values (not just \"p < 0.05\")\n",
    "- [ ] Report correction method used\n",
    "- [ ] Report effect sizes (Cohen's d, etc.)\n",
    "- [ ] Report confidence intervals\n",
    "- [ ] Be transparent about number of tests performed\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "‚ùå **P-hacking**: Running many analyses until you find p < 0.05  \n",
    "‚ùå **HARKing**: Hypothesizing After Results are Known  \n",
    "‚ùå **Ignoring multiple comparisons**  \n",
    "‚ùå **Confusing statistical and practical significance**  \n",
    "‚ùå **Reporting only \"significant\" results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090db6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 15: Complete Significance Testing Pipeline\n",
    "\n",
    "Let's put everything together in a complete pipeline for testing connectivity significance across multiple channel pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION 12: Complete Significance Testing Pipeline\n",
    "# ============================================================================\n",
    "\n",
    "def significance_pipeline(signals: List[NDArray[np.floating]],\n",
    "                          n_surrogates: int = 500,\n",
    "                          alpha: float = 0.05,\n",
    "                          correction: str = 'fdr') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete pipeline for connectivity significance testing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signals : List[NDArray[np.floating]]\n",
    "        List of signals (channels).\n",
    "    n_surrogates : int\n",
    "        Number of surrogates for null distribution.\n",
    "    alpha : float\n",
    "        Significance level.\n",
    "    correction : str\n",
    "        'bonferroni', 'fdr', or 'none'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Results dictionary with PLV values, p-values, and significance masks.\n",
    "    \"\"\"\n",
    "    n_channels = len(signals)\n",
    "    n_pairs = n_channels * (n_channels - 1) // 2\n",
    "    \n",
    "    # Initialize arrays\n",
    "    plv_matrix = np.zeros((n_channels, n_channels))\n",
    "    pvalue_matrix = np.ones((n_channels, n_channels))\n",
    "    \n",
    "    # Compute PLV and p-values for all pairs\n",
    "    pair_idx = 0\n",
    "    pvalues_flat = []\n",
    "    pairs_list = []\n",
    "    \n",
    "    for i in range(n_channels):\n",
    "        for j in range(i + 1, n_channels):\n",
    "            # Observed PLV\n",
    "            plv = compute_plv(signals[i], signals[j])\n",
    "            plv_matrix[i, j] = plv\n",
    "            plv_matrix[j, i] = plv\n",
    "            \n",
    "            # Null distribution\n",
    "            null_values = build_null_distribution(signals[i], signals[j], \n",
    "                                                  n_surrogates=n_surrogates,\n",
    "                                                  method='phase_shuffle')\n",
    "            \n",
    "            # P-value\n",
    "            pval = compute_pvalue(plv, null_values)\n",
    "            pvalue_matrix[i, j] = pval\n",
    "            pvalue_matrix[j, i] = pval\n",
    "            \n",
    "            pvalues_flat.append(pval)\n",
    "            pairs_list.append((i, j))\n",
    "            pair_idx += 1\n",
    "    \n",
    "    pvalues_flat = np.array(pvalues_flat)\n",
    "    \n",
    "    # Apply correction\n",
    "    if correction == 'bonferroni':\n",
    "        significant_flat, alpha_corr = bonferroni_correction(pvalues_flat, alpha)\n",
    "    elif correction == 'fdr':\n",
    "        significant_flat, _ = fdr_correction(pvalues_flat, alpha)\n",
    "        alpha_corr = alpha  # Not directly applicable for FDR\n",
    "    else:\n",
    "        significant_flat = pvalues_flat < alpha\n",
    "        alpha_corr = alpha\n",
    "    \n",
    "    # Build significance matrix\n",
    "    sig_matrix = np.zeros((n_channels, n_channels), dtype=bool)\n",
    "    for k, (i, j) in enumerate(pairs_list):\n",
    "        sig_matrix[i, j] = significant_flat[k]\n",
    "        sig_matrix[j, i] = significant_flat[k]\n",
    "    \n",
    "    return {\n",
    "        'plv_matrix': plv_matrix,\n",
    "        'pvalue_matrix': pvalue_matrix,\n",
    "        'significant_matrix': sig_matrix,\n",
    "        'n_significant': np.sum(significant_flat),\n",
    "        'correction': correction\n",
    "    }\n",
    "\n",
    "\n",
    "# Create simulated multi-channel data\n",
    "np.random.seed(42)\n",
    "n_channels = 6\n",
    "n_samples = 5 * fs  # 5 seconds\n",
    "channel_names = ['Fz', 'Cz', 'Pz', 'F3', 'F4', 'Oz']\n",
    "\n",
    "# Generate signals with some true connectivity\n",
    "signals = []\n",
    "base_alpha = np.sin(2 * np.pi * 10 * np.arange(n_samples) / fs)\n",
    "\n",
    "for i in range(n_channels):\n",
    "    noise = 0.5 * np.random.randn(n_samples)\n",
    "    if i < 3:  # First 3 channels share some phase\n",
    "        phase_shift = np.random.uniform(0, np.pi/4)\n",
    "        sig = np.sin(2 * np.pi * 10 * np.arange(n_samples) / fs + phase_shift) + noise\n",
    "    else:  # Last 3 channels are independent\n",
    "        sig = np.sin(2 * np.pi * 10 * np.arange(n_samples) / fs + np.random.uniform(0, 2*np.pi)) + noise\n",
    "    sig = bandpass_filter(sig, 8, 12, fs)\n",
    "    signals.append(sig)\n",
    "\n",
    "# Run pipeline\n",
    "print(\"Running significance pipeline (this may take a moment)...\")\n",
    "results = significance_pipeline(signals, n_surrogates=200, alpha=0.05, correction='fdr')\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# PLV matrix\n",
    "im1 = axes[0].imshow(results['plv_matrix'], cmap='Blues', vmin=0, vmax=1)\n",
    "axes[0].set_xticks(range(n_channels))\n",
    "axes[0].set_yticks(range(n_channels))\n",
    "axes[0].set_xticklabels(channel_names)\n",
    "axes[0].set_yticklabels(channel_names)\n",
    "axes[0].set_title('PLV Matrix', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0], shrink=0.8)\n",
    "\n",
    "# P-value matrix\n",
    "im2 = axes[1].imshow(results['pvalue_matrix'], cmap='Reds_r', vmin=0, vmax=0.1)\n",
    "axes[1].set_xticks(range(n_channels))\n",
    "axes[1].set_yticks(range(n_channels))\n",
    "axes[1].set_xticklabels(channel_names)\n",
    "axes[1].set_yticklabels(channel_names)\n",
    "axes[1].set_title('P-Value Matrix', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1], shrink=0.8, label='p-value')\n",
    "\n",
    "# Significance matrix\n",
    "im3 = axes[2].imshow(results['significant_matrix'].astype(float), cmap='Greens', vmin=0, vmax=1)\n",
    "axes[2].set_xticks(range(n_channels))\n",
    "axes[2].set_yticks(range(n_channels))\n",
    "axes[2].set_xticklabels(channel_names)\n",
    "axes[2].set_yticklabels(channel_names)\n",
    "axes[2].set_title(f'Significant (FDR, Œ±=0.05)\\n{results[\"n_significant\"]} pairs', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add text annotations for significance\n",
    "for i in range(n_channels):\n",
    "    for j in range(n_channels):\n",
    "        if results['significant_matrix'][i, j]:\n",
    "            axes[2].text(j, i, '‚úì', ha='center', va='center', \n",
    "                        fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "plt.suptitle('Complete Significance Testing Pipeline', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total pairs tested: {n_channels * (n_channels - 1) // 2}\")\n",
    "print(f\"  Significant pairs (FDR corrected): {results['n_significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92fc542",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 16: Exercises\n",
    "\n",
    "### Exercise 1: Surrogate Comparison\n",
    "\n",
    "Compare phase shuffling and time shifting for the same signal pair:\n",
    "- Generate 500 surrogates with each method\n",
    "- Plot both null distributions\n",
    "- Are the p-values similar?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "# Hint: Use phase_shuffle() and time_shift() functions\n",
    "```\n",
    "\n",
    "### Exercise 2: Effect of Number of Surrogates\n",
    "\n",
    "Investigate how the number of surrogates affects p-value stability:\n",
    "- Test with N = 100, 500, 1000, 5000\n",
    "- Repeat each 10 times and compute the standard deviation of p-values\n",
    "- At what N does the p-value stabilize?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "### Exercise 3: Multiple Comparisons Impact\n",
    "\n",
    "Simulate the multiple comparisons problem:\n",
    "- Generate 100 pairs of independent signals (no true connectivity)\n",
    "- Test all pairs at Œ± = 0.05\n",
    "- How many false positives with no correction?\n",
    "- How many with Bonferroni? With FDR?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```\n",
    "\n",
    "### Exercise 4: Power Analysis\n",
    "\n",
    "Investigate statistical power:\n",
    "- Generate pairs with known connectivity (PLV = 0.3, 0.5, 0.7)\n",
    "- For each, run the significance test 100 times\n",
    "- What proportion of tests correctly reject H‚ÇÄ?\n",
    "- How does signal length affect power?\n",
    "\n",
    "```python\n",
    "# Your code here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86721c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 1 SOLUTION: Surrogate Comparison\n",
    "# ============================================================================\n",
    "\n",
    "# Generate test signals\n",
    "np.random.seed(42)\n",
    "t_ex = np.arange(0, 3, 1/fs)\n",
    "\n",
    "# Create two weakly coupled signals\n",
    "sig1_ex = np.sin(2 * np.pi * 10 * t_ex) + 0.3 * np.random.randn(len(t_ex))\n",
    "sig2_ex = np.sin(2 * np.pi * 10 * t_ex + np.pi/4) + 0.3 * np.random.randn(len(t_ex))\n",
    "\n",
    "# Filter to alpha band\n",
    "sig1_ex = bandpass_filter(sig1_ex, 8, 12, fs)\n",
    "sig2_ex = bandpass_filter(sig2_ex, 8, 12, fs)\n",
    "\n",
    "# Observed PLV\n",
    "plv_observed_ex = compute_plv(sig1_ex, sig2_ex)\n",
    "\n",
    "# Generate null distributions with both methods\n",
    "n_surr = 500\n",
    "\n",
    "null_phase_shuffle = np.zeros(n_surr)\n",
    "null_time_shift = np.zeros(n_surr)\n",
    "\n",
    "for i in range(n_surr):\n",
    "    # Phase shuffle\n",
    "    surr_ps = phase_shuffle(sig2_ex)\n",
    "    null_phase_shuffle[i] = compute_plv(sig1_ex, surr_ps)\n",
    "    \n",
    "    # Time shift\n",
    "    surr_ts = time_shift(sig2_ex)\n",
    "    null_time_shift[i] = compute_plv(sig1_ex, surr_ts)\n",
    "\n",
    "# Compute p-values\n",
    "pval_ps = compute_pvalue(plv_observed_ex, null_phase_shuffle)\n",
    "pval_ts = compute_pvalue(plv_observed_ex, null_time_shift)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].hist(null_phase_shuffle, bins=30, alpha=0.7, color=PRIMARY_BLUE, \n",
    "             edgecolor='white', label='Phase Shuffle')\n",
    "axes[0].axvline(plv_observed_ex, color=PRIMARY_RED, linewidth=2, linestyle='--',\n",
    "                label=f'Observed PLV = {plv_observed_ex:.3f}')\n",
    "axes[0].set_xlabel('PLV', fontsize=11)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].set_title(f'Phase Shuffling\\np = {pval_ps:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(null_time_shift, bins=30, alpha=0.7, color=SECONDARY_ORANGE, \n",
    "             edgecolor='white', label='Time Shift')\n",
    "axes[1].axvline(plv_observed_ex, color=PRIMARY_RED, linewidth=2, linestyle='--',\n",
    "                label=f'Observed PLV = {plv_observed_ex:.3f}')\n",
    "axes[1].set_xlabel('PLV', fontsize=11)\n",
    "axes[1].set_ylabel('Count', fontsize=11)\n",
    "axes[1].set_title(f'Time Shifting\\np = {pval_ts:.4f}', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle('Exercise 1: Comparing Surrogate Methods', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Observed PLV: {plv_observed_ex:.4f}\")\n",
    "print(f\"Phase shuffling: mean null = {np.mean(null_phase_shuffle):.4f}, p = {pval_ps:.4f}\")\n",
    "print(f\"Time shifting:   mean null = {np.mean(null_time_shift):.4f}, p = {pval_ts:.4f}\")\n",
    "print(\"\\n‚Üí Both methods give similar results, but phase shuffling is more rigorous.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd49230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 2 SOLUTION: Effect of Number of Surrogates\n",
    "# ============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Test different numbers of surrogates\n",
    "n_surrogates_list = [100, 500, 1000, 5000]\n",
    "n_repetitions = 10\n",
    "\n",
    "# Store results\n",
    "pvalue_results = {n: [] for n in n_surrogates_list}\n",
    "\n",
    "# Use same signals as before\n",
    "for n_surr in n_surrogates_list:\n",
    "    for rep in range(n_repetitions):\n",
    "        # Build null distribution\n",
    "        null_vals = np.zeros(n_surr)\n",
    "        for i in range(n_surr):\n",
    "            surr = phase_shuffle(sig2_ex)\n",
    "            null_vals[i] = compute_plv(sig1_ex, surr)\n",
    "        \n",
    "        # Compute p-value\n",
    "        pval = compute_pvalue(plv_observed_ex, null_vals)\n",
    "        pvalue_results[n_surr].append(pval)\n",
    "\n",
    "# Compute statistics\n",
    "means = [np.mean(pvalue_results[n]) for n in n_surrogates_list]\n",
    "stds = [np.std(pvalue_results[n]) for n in n_surrogates_list]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Boxplot of p-values\n",
    "bp = axes[0].boxplot([pvalue_results[n] for n in n_surrogates_list], \n",
    "                      patch_artist=True, labels=[str(n) for n in n_surrogates_list])\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor(PRIMARY_BLUE)\n",
    "    patch.set_alpha(0.7)\n",
    "axes[0].set_xlabel('Number of Surrogates', fontsize=11)\n",
    "axes[0].set_ylabel('P-value', fontsize=11)\n",
    "axes[0].set_title('P-value Variability vs N Surrogates', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Right: Standard deviation\n",
    "axes[1].bar(range(len(n_surrogates_list)), stds, color=SECONDARY_ORANGE, alpha=0.7)\n",
    "axes[1].set_xticks(range(len(n_surrogates_list)))\n",
    "axes[1].set_xticklabels([str(n) for n in n_surrogates_list])\n",
    "axes[1].set_xlabel('Number of Surrogates', fontsize=11)\n",
    "axes[1].set_ylabel('Std of P-values', fontsize=11)\n",
    "axes[1].set_title('P-value Stability', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Exercise 2: More Surrogates = More Stable P-values', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"P-value statistics by number of surrogates:\")\n",
    "for n, m, s in zip(n_surrogates_list, means, stds):\n",
    "    print(f\"  N = {n:4d}: mean = {m:.4f}, std = {s:.4f}\")\n",
    "print(\"\\n‚Üí P-values stabilize around N = 1000. Use N ‚â• 1000 for reliable results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f665ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 3 SOLUTION: Multiple Comparisons Impact\n",
    "# ============================================================================\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n_pairs = 100\n",
    "alpha_test = 0.05\n",
    "n_surrogates_ex3 = 200  # Fewer for speed\n",
    "\n",
    "# Generate independent signal pairs (no true connectivity)\n",
    "pvalues_ex3 = []\n",
    "\n",
    "print(\"Testing 100 independent signal pairs (no true connectivity)...\")\n",
    "for pair in range(n_pairs):\n",
    "    # Generate two independent signals\n",
    "    s1 = np.random.randn(len(t_ex))\n",
    "    s2 = np.random.randn(len(t_ex))\n",
    "    s1 = bandpass_filter(s1, 8, 12, fs)\n",
    "    s2 = bandpass_filter(s2, 8, 12, fs)\n",
    "    \n",
    "    # Observed PLV\n",
    "    plv = compute_plv(s1, s2)\n",
    "    \n",
    "    # Null distribution (fast: time shifting)\n",
    "    null_vals = np.zeros(n_surrogates_ex3)\n",
    "    for i in range(n_surrogates_ex3):\n",
    "        null_vals[i] = compute_plv(s1, time_shift(s2))\n",
    "    \n",
    "    # P-value\n",
    "    pval = compute_pvalue(plv, null_vals)\n",
    "    pvalues_ex3.append(pval)\n",
    "\n",
    "pvalues_ex3 = np.array(pvalues_ex3)\n",
    "\n",
    "# Apply corrections\n",
    "sig_none = pvalues_ex3 < alpha_test\n",
    "sig_bonf, _ = bonferroni_correction(pvalues_ex3, alpha_test)\n",
    "sig_fdr, _ = fdr_correction(pvalues_ex3, alpha_test)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: P-value distribution\n",
    "axes[0].hist(pvalues_ex3, bins=20, color=PRIMARY_BLUE, edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(alpha_test, color=PRIMARY_RED, linewidth=2, linestyle='--',\n",
    "                label=f'Œ± = {alpha_test}')\n",
    "axes[0].set_xlabel('P-value', fontsize=11)\n",
    "axes[0].set_ylabel('Count', fontsize=11)\n",
    "axes[0].set_title('P-value Distribution (H‚ÇÄ true for all)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Right: False positives by method\n",
    "methods_ex3 = ['No correction', 'Bonferroni', 'FDR']\n",
    "fp_counts = [np.sum(sig_none), np.sum(sig_bonf), np.sum(sig_fdr)]\n",
    "colors_ex3 = [PRIMARY_RED, PRIMARY_GREEN, SECONDARY_ORANGE]\n",
    "\n",
    "bars = axes[1].bar(methods_ex3, fp_counts, color=colors_ex3, alpha=0.7)\n",
    "axes[1].axhline(n_pairs * alpha_test, color='black', linestyle='--', \n",
    "                label=f'Expected FP = {n_pairs * alpha_test:.0f}')\n",
    "axes[1].set_ylabel('False Positives', fontsize=11)\n",
    "axes[1].set_title('False Positives by Correction Method', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, fp_counts):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                 str(count), ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Exercise 3: Multiple Comparisons Problem Demonstrated', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResults ({n_pairs} pairs tested, ALL with NO true connectivity):\")\n",
    "print(f\"  No correction:  {np.sum(sig_none)} false positives (expected: {n_pairs * alpha_test:.0f})\")\n",
    "print(f\"  Bonferroni:     {np.sum(sig_bonf)} false positives\")\n",
    "print(f\"  FDR:            {np.sum(sig_fdr)} false positives\")\n",
    "print(\"\\n‚Üí Without correction, ~5% of null pairs are falsely declared significant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXERCISE 4 SOLUTION: Power Analysis\n",
    "# ============================================================================\n",
    "\n",
    "def generate_coupled_signals(plv_target: float, \n",
    "                              n_samples: int,\n",
    "                              fs: int) -> Tuple[NDArray, NDArray]:\n",
    "    \"\"\"\n",
    "    Generate two signals with approximately the target PLV.\n",
    "    \n",
    "    The coupling strength is controlled by mixing a shared oscillation\n",
    "    with independent noise.\n",
    "    \"\"\"\n",
    "    t = np.arange(n_samples) / fs\n",
    "    \n",
    "    # Shared oscillation (10 Hz)\n",
    "    shared = np.sin(2 * np.pi * 10 * t)\n",
    "    \n",
    "    # Coupling factor (empirically tuned)\n",
    "    coupling = plv_target ** 0.5  # Approximate\n",
    "    \n",
    "    # Signal 1: shared + noise\n",
    "    s1 = coupling * shared + (1 - coupling) * np.random.randn(n_samples)\n",
    "    \n",
    "    # Signal 2: phase-shifted shared + noise\n",
    "    phase_shift = np.random.uniform(0, np.pi / 4)\n",
    "    s2 = coupling * np.sin(2 * np.pi * 10 * t + phase_shift) + (1 - coupling) * np.random.randn(n_samples)\n",
    "    \n",
    "    # Bandpass filter\n",
    "    s1 = bandpass_filter(s1, 8, 12, fs)\n",
    "    s2 = bandpass_filter(s2, 8, 12, fs)\n",
    "    \n",
    "    return s1, s2\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "plv_targets = [0.3, 0.5, 0.7]\n",
    "signal_lengths = [1, 2, 5]  # seconds\n",
    "n_tests = 50\n",
    "n_surrogates_ex4 = 200\n",
    "\n",
    "# Store power results\n",
    "power_results = np.zeros((len(plv_targets), len(signal_lengths)))\n",
    "\n",
    "print(\"Running power analysis (this may take a moment)...\")\n",
    "\n",
    "for i, plv_target in enumerate(plv_targets):\n",
    "    for j, sig_len in enumerate(signal_lengths):\n",
    "        n_samples_ex4 = int(sig_len * fs)\n",
    "        n_significant = 0\n",
    "        \n",
    "        for test in range(n_tests):\n",
    "            # Generate coupled signals\n",
    "            s1, s2 = generate_coupled_signals(plv_target, n_samples_ex4, fs)\n",
    "            \n",
    "            # Observed PLV\n",
    "            plv = compute_plv(s1, s2)\n",
    "            \n",
    "            # Null distribution\n",
    "            null_vals = np.zeros(n_surrogates_ex4)\n",
    "            for k in range(n_surrogates_ex4):\n",
    "                null_vals[k] = compute_plv(s1, time_shift(s2))\n",
    "            \n",
    "            # P-value\n",
    "            pval = compute_pvalue(plv, null_vals)\n",
    "            \n",
    "            if pval < 0.05:\n",
    "                n_significant += 1\n",
    "        \n",
    "        power_results[i, j] = n_significant / n_tests\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(signal_lengths))\n",
    "width = 0.25\n",
    "colors_power = [PRIMARY_BLUE, SECONDARY_ORANGE, PRIMARY_GREEN]\n",
    "\n",
    "for i, (plv_target, color) in enumerate(zip(plv_targets, colors_power)):\n",
    "    offset = (i - 1) * width\n",
    "    bars = ax.bar(x + offset, power_results[i] * 100, width, \n",
    "                  label=f'PLV = {plv_target}', color=color, alpha=0.8)\n",
    "\n",
    "ax.axhline(80, color='red', linestyle='--', alpha=0.5, label='80% power threshold')\n",
    "ax.set_xlabel('Signal Length (seconds)', fontsize=12)\n",
    "ax.set_ylabel('Statistical Power (%)', fontsize=12)\n",
    "ax.set_title('Exercise 4: Power Increases with PLV and Signal Length', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f'{s}s' for s in signal_lengths])\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistical Power (% tests correctly rejecting H‚ÇÄ):\")\n",
    "print(f\"{'PLV':>8}\", end='')\n",
    "for sig_len in signal_lengths:\n",
    "    print(f\"{sig_len}s\".rjust(10), end='')\n",
    "print()\n",
    "for i, plv_target in enumerate(plv_targets):\n",
    "    print(f\"{plv_target:>8.1f}\", end='')\n",
    "    for j in range(len(signal_lengths)):\n",
    "        print(f\"{power_results[i, j] * 100:>9.0f}%\", end='')\n",
    "    print()\n",
    "print(\"\\n‚Üí Higher PLV and longer signals = more statistical power!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eaa41b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 17: Summary\n",
    "\n",
    "### Key Concepts Learned\n",
    "\n",
    "1. **Null Hypothesis Testing**\n",
    "   - H‚ÇÄ: No true connectivity (observed values due to chance)\n",
    "   - Build null distribution using surrogate data\n",
    "   - P-value = probability of observing data this extreme under H‚ÇÄ\n",
    "\n",
    "2. **Surrogate Methods**\n",
    "   - **Phase shuffling**: Preserves spectrum, destroys phase relationships\n",
    "   - **Time shifting**: Faster, simpler, less rigorous\n",
    "   - Use enough surrogates (‚â•1000 for Œ± = 0.05)\n",
    "\n",
    "3. **Multiple Comparisons Correction**\n",
    "   - Testing many pairs inflates false positive rate\n",
    "   - **Bonferroni**: Conservative, controls FWER\n",
    "   - **FDR (Benjamini-Hochberg)**: Less conservative, controls proportion of false discoveries\n",
    "\n",
    "4. **Beyond P-Values**\n",
    "   - **Effect size** (Cohen's d): How big is the effect?\n",
    "   - **Confidence intervals**: Uncertainty quantification\n",
    "   - Always report both p-values AND effect sizes\n",
    "\n",
    "5. **Permutation Testing**\n",
    "   - Non-parametric group comparisons\n",
    "   - No distributional assumptions\n",
    "   - Shuffle group labels to build null distribution\n",
    "\n",
    "### Decision Flowchart\n",
    "\n",
    "```\n",
    "Is connectivity significant?\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚Üí Single pair ‚Üí Surrogate testing (phase shuffle)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚Üí Many pairs ‚Üí Apply correction (FDR for exploratory, Bonferroni for confirmatory)\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚Üí Group comparison ‚Üí Permutation testing\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **C04**, you'll learn about **causality and directionality** ‚Äî determining not just IF signals are connected, but in which DIRECTION information flows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a630e0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 18: Discussion Questions\n",
    "\n",
    "1. **When would you choose Bonferroni over FDR correction?**\n",
    "   - Consider the cost of false positives vs. false negatives in your research context.\n",
    "\n",
    "2. **How many surrogates are \"enough\"?**\n",
    "   - It depends on your desired p-value precision. For Œ± = 0.001, you need at least 1000. For Œ± = 0.0001, at least 10000.\n",
    "\n",
    "3. **What if you test multiple frequency bands AND multiple channel pairs?**\n",
    "   - You should correct for ALL tests. Some researchers use cluster-based permutation testing.\n",
    "\n",
    "4. **Can you trust a significant result without replication?**\n",
    "   - Single studies can find false positives. Replication is the gold standard.\n",
    "\n",
    "5. **How do surrogate methods handle non-stationarity?**\n",
    "   - Standard phase shuffling assumes stationarity. For non-stationary data, consider windowed approaches or AAFT.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** You now understand how to properly test connectivity for statistical significance. This is crucial for making valid scientific claims about brain connectivity.\n",
    "\n",
    "*Remember: A connectivity value without proper statistical testing is just a number ‚Äî it tells you nothing about whether it's real or just noise.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectivity-metrics-tutorials-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
